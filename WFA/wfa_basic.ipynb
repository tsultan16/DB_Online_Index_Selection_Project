{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Online Index Selection using the Work Function Algorithm (WFA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "import uuid\n",
    "\n",
    "import pyodbc\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import re\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import itertools\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import IPython\n",
    "notebook_path = IPython.get_ipython().starting_dir\n",
    "target_subdirectory_path = os.path.abspath(os.path.join(os.path.dirname(notebook_path), 'database'))\n",
    "sys.path.append(target_subdirectory_path)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we will define some helper functions for generating list of all possible configurations subject to constraints (i.e. max number of columns per index, max number of indices per configuration), along with cost estimation (such as transition costs and query execution cost in a hypothetical configuration). We will also precompute estimates of all index creation costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index:\n",
    "    def __init__(self, table_name, index_id, index_columns, size, include_columns=(), value=None):\n",
    "        self.table_name = table_name\n",
    "        self.index_id = index_id\n",
    "        self.index_columns = index_columns\n",
    "        self.size = size\n",
    "        self.include_columns = include_columns\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Index({self.table_name}, {self.index_id}, {self.index_columns}, {self.include_columns}, {self.size}, {self.value})\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Function for generating all possible configurations (i.e. subsets of indixes) and also precomputing index creation cost estimates\n",
    "\"\"\"\n",
    "def generate_all_configurations(connection, MAX_INDICES_PER_CONFIG=2, MAX_COLS=2, verbose=False):\n",
    "    # first, generate all possible indices\n",
    "    tables = get_all_tables(connection)\n",
    "    all_indices = {} \n",
    "    # tqdm bar around table loop\n",
    "    for table_name, table in tqdm(tables.items(), desc=\"Generating all indices for table:\"):\n",
    "        columns = table.get_columns()\n",
    "        if verbose:\n",
    "            print(f\"Table --> {table_name} with columns --> {columns}\")\n",
    "        # get all possible permutations of columns, up to MAX_KEY_COLS columns\n",
    "        for num_columns in range(1, MAX_COLS+1):\n",
    "            col_permutations = list(itertools.permutations(columns, num_columns))\n",
    "            # also generate permutations of columns with include columns\n",
    "            for cp in col_permutations:\n",
    "                # get columns not in cp\n",
    "                include_columns = list(set(columns) - set(cp))\n",
    "                # get all permutations of MAX_INCLUDE_COLS include columns\n",
    "                include_col_permutations = list(itertools.permutations(include_columns, MAX_COLS-num_columns))\n",
    "                for icp in include_col_permutations:\n",
    "                    index_id = get_index_id(cp, table_name, include_columns)\n",
    "                    if index_id not in all_indices:\n",
    "                        index_size = get_estimated_index_size(connection, table_name, list(cp) + include_columns)\n",
    "                        # create index object\n",
    "                        all_indices[index_id] = Index(table_name, index_id, cp, index_size, tuple(icp))\n",
    "\n",
    "    print(f\"Total number of indices generated: {len(all_indices)}, total estimated size: {sum([i.size for i in all_indices.values()]):.2f} Mb\")\n",
    "\n",
    "    # now estimate the creation cost of each index (we this by creating the index and then dropping it, which is potentially \n",
    "    # very expensive, but I don't know a more efficient way)\n",
    "    index_creation_cost = {}\n",
    "    for index_id, index in tqdm(all_indices.items(), desc=\"Estimating index creation cost:\"):\n",
    "        index_creation_cost[index_id] = create_nonclustered_index_object(connection, index)\n",
    "        drop_noncluster_index_object(connection, index)\n",
    "\n",
    "    # now generate all possible configurations with up to MAX_INDICES_PER_CONFIG indices\n",
    "    all_configurations = []\n",
    "    print(f\"Gneretaing all possible configurations with up to {MAX_INDICES_PER_CONFIG} indices.\")\n",
    "    for num_indices in range(1, MAX_INDICES_PER_CONFIG+1):\n",
    "        all_configurations.append(list(itertools.combinations(all_indices.values(), num_indices)))\n",
    "\n",
    "    all_configurations = list(itertools.chain.from_iterable(all_configurations))\n",
    "\n",
    "\n",
    "    return all_indices, index_creation_cost, all_configurations, tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate all possible configurations with up to 2 indices, each index containing up to 2 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All non-clustered indexes --> [('dbo', 'customer', 'IX_customer_c_custkey'), ('dbo', 'lineitem', 'IXN_lineitem_l_shipdate_l_suppkey_l_partkey_l_qu'), ('dbo', 'lineitem', 'IXN_lineitem_l_suppkey_l_partkey_l_orderkey_l_di_l_ex_l_qu'), ('dbo', 'partsupp', 'IX_partsupp_ps_partkey'), ('dbo', 'supplier', 'IX_supplier_s_nationkey'), ('dbo', 'supplier', 'IX_supplier_s_nationkey_s_suppkey'), ('dbo', 'supplier', 'IX_supplier_s_suppkey_s_nationkey')]\n",
      "All nonclustered indexes removed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating all indices for table:: 100%|██████████| 8/8 [00:00<00:00, 1884.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table --> customer with columns --> {'c_custkey': <utils.Column object at 0x7f24ea917d90>, 'c_name': <utils.Column object at 0x7f24ea916cd0>, 'c_address': <utils.Column object at 0x7f24eae2ab10>, 'c_nationkey': <utils.Column object at 0x7f24eae28c10>, 'c_phone': <utils.Column object at 0x7f24eae2acd0>, 'c_acctbal': <utils.Column object at 0x7f24eae2aa10>, 'c_mktsegment': <utils.Column object at 0x7f24eae2ad90>, 'c_comment': <utils.Column object at 0x7f24eae2a150>}\n",
      "Table --> orders with columns --> {'o_orderkey': <utils.Column object at 0x7f24eaf2e590>, 'o_custkey': <utils.Column object at 0x7f24eaf33910>, 'o_orderstatus': <utils.Column object at 0x7f24eaf33610>, 'o_totalprice': <utils.Column object at 0x7f24eaf31990>, 'o_orderdate': <utils.Column object at 0x7f24eaf30c50>, 'o_orderpriority': <utils.Column object at 0x7f24dc09ef90>, 'o_clerk': <utils.Column object at 0x7f24dc09fd10>, 'o_shippriority': <utils.Column object at 0x7f24dc09c910>, 'o_comment': <utils.Column object at 0x7f24dc09e990>}\n",
      "Table --> lineitem with columns --> {'l_orderkey': <utils.Column object at 0x7f24c8d26b10>, 'l_partkey': <utils.Column object at 0x7f24c8d26bd0>, 'l_suppkey': <utils.Column object at 0x7f24c8d268d0>, 'l_linenumber': <utils.Column object at 0x7f24c8d26450>, 'l_quantity': <utils.Column object at 0x7f24c8d279d0>, 'l_extendedprice': <utils.Column object at 0x7f24c8d27dd0>, 'l_discount': <utils.Column object at 0x7f24c8d27e90>, 'l_tax': <utils.Column object at 0x7f24c8d27d10>, 'l_returnflag': <utils.Column object at 0x7f24c8d272d0>, 'l_linestatus': <utils.Column object at 0x7f24c8d26fd0>, 'l_shipdate': <utils.Column object at 0x7f24c8d25f90>, 'l_commitdate': <utils.Column object at 0x7f24c8d2c2d0>, 'l_receiptdate': <utils.Column object at 0x7f24c8d2c6d0>, 'l_shipinstruct': <utils.Column object at 0x7f24c8d2c050>, 'l_shipmode': <utils.Column object at 0x7f24c8d2dc10>, 'l_comment': <utils.Column object at 0x7f24c8d2dad0>}\n",
      "Table --> part with columns --> {'p_partkey': <utils.Column object at 0x7f24c8d2ebd0>, 'p_name': <utils.Column object at 0x7f24c8d2f2d0>, 'p_mfgr': <utils.Column object at 0x7f24c8d2e190>, 'p_brand': <utils.Column object at 0x7f24c8d2e510>, 'p_type': <utils.Column object at 0x7f24c8d2d650>, 'p_size': <utils.Column object at 0x7f24c8cf6ed0>, 'p_container': <utils.Column object at 0x7f24c8cf4090>, 'p_retailprice': <utils.Column object at 0x7f24c8cf7fd0>, 'p_comment': <utils.Column object at 0x7f24c8cf7d10>}\n",
      "Table --> supplier with columns --> {'s_suppkey': <utils.Column object at 0x7f24c8d34f50>, 's_name': <utils.Column object at 0x7f24c8d34e50>, 's_address': <utils.Column object at 0x7f24c8d34950>, 's_nationkey': <utils.Column object at 0x7f24c8d34a10>, 's_phone': <utils.Column object at 0x7f24c8d348d0>, 's_acctbal': <utils.Column object at 0x7f24c8d37490>, 's_comment': <utils.Column object at 0x7f24c8d35190>}\n",
      "Table --> partsupp with columns --> {'ps_partkey': <utils.Column object at 0x7f24e81e2950>, 'ps_suppkey': <utils.Column object at 0x7f25070565d0>, 'ps_availqty': <utils.Column object at 0x7f24c9baa050>, 'ps_supplycost': <utils.Column object at 0x7f24dc156090>, 'ps_comment': <utils.Column object at 0x7f24c8d0f710>}\n",
      "Table --> nation with columns --> {'n_nationkey': <utils.Column object at 0x7f24c8d38d90>, 'n_name': <utils.Column object at 0x7f24c8d38d10>, 'n_regionkey': <utils.Column object at 0x7f24c8d38dd0>, 'n_comment': <utils.Column object at 0x7f24c8d391d0>}\n",
      "Table --> region with columns --> {'r_regionkey': <utils.Column object at 0x7f24c8d395d0>, 'r_name': <utils.Column object at 0x7f24c8d39a90>, 'r_comment': <utils.Column object at 0x7f24c8d39b50>}\n",
      "Total number of indices generated: 581, total estimated size: 220776.07 Mb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimating index creation cost:: 100%|██████████| 581/581 [1:01:03<00:00,  6.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gneretaing all possible configurations with up to 2 indices.\n"
     ]
    }
   ],
   "source": [
    "connection = start_connection() \n",
    "\n",
    "# wipe out all non clustered indices from database\n",
    "remove_all_nonclustered_indexes(connection)\n",
    "\n",
    "all_indices, index_creation_cost, all_configurations, tables = generate_all_configurations(connection, MAX_INDICES_PER_CONFIG=2, MAX_COLS=2, verbose=True)\n",
    "\n",
    "close_connection(connection)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith open('all_indices.pkl', 'rb') as f:\\n    all_indices = pickle.load(f)\\n\\nwith open('index_creation_cost.pkl', 'rb') as f:\\n    index_creation_cost = pickle.load(f)\\n\\nwith open('all_configurations.pkl', 'rb') as f:\\n    all_configurations = pickle.load(f)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save copy of all_indices, index_creation_cost, all_configurations\n",
    "\"\"\"\n",
    "with open('all_indices.pkl', 'wb') as f:\n",
    "    pickle.dump(all_indices, f)\n",
    "\n",
    "with open('index_creation_cost.pkl', 'wb') as f:\n",
    "    pickle.dump(index_creation_cost, f)\n",
    "\n",
    "with open('all_configurations.pkl', 'wb') as f:\n",
    "    pickle.dump(all_configurations, f)\n",
    "\"\"\"\n",
    "\n",
    "# load copy of all_indices, index_creation_cost, all_configurations\n",
    "\"\"\"\n",
    "with open('all_indices.pkl', 'rb') as f:\n",
    "    all_indices = pickle.load(f)\n",
    "\n",
    "with open('index_creation_cost.pkl', 'rb') as f:\n",
    "    index_creation_cost = pickle.load(f)\n",
    "\n",
    "with open('all_configurations.pkl', 'rb') as f:\n",
    "    all_configurations = pickle.load(f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's implement WFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class WFA:\n",
    "    def __init__(self, all_configurations, all_indices, index_creation_cost):\n",
    "        self.all_configurations = all_configurations\n",
    "        self.all_indices = all_indices\n",
    "        self.index_creation_cost = index_creation_cost\n",
    "        #self.best_configurations = []\n",
    "        #self.best_cost = float('inf')\n",
    "        self.current_recommendation= None\n",
    "        self.current_recommendation_id = None\n",
    "        self.w = np.zeros(shape=(len(all_configurations)))\n",
    "\n",
    "\n",
    "    def transition(self, C_old, C_new, connection):       \n",
    "        # find out which indices need to be added and removed\n",
    "        indices_old = set([i.index_id for i in C_old])\n",
    "        indices_new = set([i.index_id for i in C_new])\n",
    "        indices_added = indices_new - indices_old\n",
    "        indices_added = [self.all_indices[i] for i in indices_added]\n",
    "        indices_removed = indices_old - indices_new\n",
    "        indices_removed = [self.all_indices[i] for i in indices_removed]\n",
    "\n",
    "        # implement configuration change\n",
    "        total_index_creation_cost = bulk_create_drop_nonclustered_indexes(connection, indices_added, indices_removed)\n",
    "\n",
    "        return total_index_creation_cost\n",
    "\n",
    "\n",
    "    # estimation of cost of transition from configuration Ci to Cj\n",
    "    def get_transition_cost(self, Ci, Cj):\n",
    "        # find out which indices need to be added and removed\n",
    "        indices_old = set([i.index_id for i in Ci])\n",
    "        indices_new = set([i.index_id for i in Cj])\n",
    "        indices_added = indices_new - indices_old\n",
    "        indices_added = [self.all_indices[i] for i in indices_added]\n",
    "        # compute cost of adding indices (assume that cost of removing indices is negligible)\n",
    "        total_index_creation_cost = sum([self.index_creation_cost[i.index_id] for i in indices_added])\n",
    "        return total_index_creation_cost\n",
    "    \n",
    "\n",
    "    # estimation of cost of executing query/mini-workload q in configuration C_new    \n",
    "    # q needs to be a list of query strings\n",
    "    def get_execution_cost(self, q, C_old, C_new, connection, cost_type='hypothetical'):\n",
    "        if cost_type == 'hypothetical':\n",
    "            # find out which indices need to be added and removed\n",
    "            indices_old = set([i.index_id for i in C_old])\n",
    "            indices_new = set([i.index_id for i in C_new])\n",
    "            indices_added = indices_new - indices_old\n",
    "            indices_added = [self.all_indices[i] for i in indices_added]\n",
    "            indices_removed = indices_old - indices_new\n",
    "            indices_removed = [self.all_indices[i] for i in indices_removed]\n",
    "\n",
    "            _, total_execution_cost, _ = hyp_configuration_cost_estimate(connection, indexes_added, indexes_removed, q, verbose=False)\n",
    "\n",
    "        elif cost_type == 'exact':\n",
    "            # for exact cost estimation, we will first transition to the configuration C, then execute the query/mini-workload q, then transition back to the original configuration\n",
    "            C_original = self.current_configuration\n",
    "            _ = self.transition(C_old, C_new, connection)\n",
    "            total_execution_cost =  bulk_execute_queries(q, connection)\n",
    "            _ = self.transition(C_new, C_old, connection)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Cost type {cost_type} not supported.\")\n",
    "\n",
    "        return total_execution_cost\n",
    "\n",
    "\n",
    "    # work function initialization\n",
    "    def initialize_w(self, i):\n",
    "        # initial configuration\n",
    "        C_init = self.all_configurations[i] \n",
    "        self.current_recommendation = C_init\n",
    "        self.current_recommendation_id = i\n",
    "        for j, X in enumerate(self.all_configurations):\n",
    "            self.w[j] = self.get_transition_cost(C_init, X) \n",
    "\n",
    "    # run 1 step of WFA for given query/mini-workload q to recommend a configuration   \n",
    "    def recommend(self, q):\n",
    "        # evaluate work function for all possible configurations\n",
    "        w_new = np.zeros(shape=(len(self.all_configurations)))\n",
    "        best_configuration = []\n",
    "        for i, C_new in enumerate(self.all_configurations):\n",
    "            # compute all possible transition and service costs\n",
    "            min_value = float('inf')\n",
    "            min_j = None\n",
    "            for j, C_old in enumerate(self.all_configurations):\n",
    "                if j == i:\n",
    "                    transition_cost = 0\n",
    "                else:\n",
    "                    transition_cost = self.get_transition_cost(C_old, C_new)\n",
    "                \n",
    "                service_cost = self.get_execution_cost(q, C_old)\n",
    "                value = self.w[j] + transition_cost + service_cost\n",
    "                if value < min_value:\n",
    "                    min_value = value\n",
    "                    min_j = j\n",
    "\n",
    "            # find the minimum work function value\n",
    "            w_new[i] = min_value\n",
    "            # store the best configuration\n",
    "            best_configuration.append(self.all_configurations[min_j])    \n",
    "\n",
    "        # overwrite the work function values\n",
    "        self.w = w_new    \n",
    "\n",
    "        # compute scores for all configurations\n",
    "        scores = np.zeros(shape=(len(self.all_configurations)))\n",
    "        min_score = float('inf')\n",
    "        best_new_configuration_id = None\n",
    "        for i in range(len(self.all_configurations)):\n",
    "            scores[i] = self.w[i]  + self.get_transition_cost(self.all_configurations[i], self.current_recommendation)\n",
    "            if scores[i] < min_score:\n",
    "                min_score = scores[i]\n",
    "                best_new_configuration_id = i\n",
    "\n",
    "        # update the current recommendation\n",
    "        self.current_recommendation = self.all_configurations[best_new_configuration_id]  \n",
    "        return self.current_recommendation      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read workload queries from JSON file\n",
    "def read_workload(workload_filepath):\n",
    "    workload = []\n",
    "    with open(workload_filepath) as f:\n",
    "        line = f.readline()\n",
    "        # read the queries from each line\n",
    "        while line:\n",
    "            workload.append(json.loads(line))\n",
    "            line = f.readline()\n",
    "\n",
    "    return workload\n",
    "\n",
    "# Base directory containing the generated queries\n",
    "workload_filepath = '../datagen/TPCH_workloads/TPCH_static_100_workload.json'\n",
    "\n",
    "# Read the workload queries from file\n",
    "workload = read_workload(workload_filepath)\n",
    "print(len(workload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
