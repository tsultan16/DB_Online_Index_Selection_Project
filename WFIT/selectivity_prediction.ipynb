{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learned Selectivity Prediction:\n",
    "------------------------------\n",
    "\n",
    "\n",
    "So, essentially what I am trying to do is develop a simple algorithm (similar to the postgres query optimizer) which, given a query, predicts the cheapest access path for each table, assuming that the postgres query optimizer will always select the cheapest access path for each table involved in the query, where cost is proportional to total number of disk accesses. To make that prediction the algorithm uses table statistics such as a postgres stats histogram to estimate the selectivity of each precicate and then use independence assumption to compute cardinality (if multiple predicates are present on a single table). For skewed data distributions, I understand that postgres stats histograms may not be accurate and therefore my selectivity estimates may also be highly inaccurate. Thats why my initial idea was to learn a CDF function online for each table attribute. \n",
    "\n",
    "However, I realize now that this approach may not work very well, and instead maybe I should develop a single regression model on each table which can directly predict the selectivity of a given predicate. I could train this model online using explain analyze results from real time query executions. Beacuse those query plan results can be used to know the exact selectivity of each predicate, and features for predicates are also easy to extract and have a simple form, this type of model could potentially be better suited for my task. \n",
    "\n",
    "To keep things simple in the beginning, I want to start with a model for each table that only predicts selectivity for a single predicate, i.e. a predicate on a single attribute. Maybe once I can get such a model trained and working, I could think of ways to extend it to the multi-attribute case, i.e. if a query contains predicates over multiple attributes of a table, then the model shoudl be able to map a feature vector containing information about all of those predicates and predict a combnied selectivity (this could potentially be better that using individual predicate selectivities and then combining them under the independence assumption).\n",
    "\n",
    "The model can be pre-trained so that its predictions are consistent with uniform data distribution assumption. Then, the model can be refined online using actual query execution results (i.e. the actual selectivities observed in the query plan operators).\n",
    "\n",
    "Feature extraction for a single predicate:\n",
    "-----------------------------------------\n",
    "\n",
    "Even though I am predicting the selectivity for a single attribute, the model should be able to rpedict for any attribute in the table. So given that, how should the predicate be encoded into a fixed length feature vector which would allow the model to also know which particular attribute the selectivity corresponds to? i.e. the feature vector needs to somehow be able to encode the identity of the predicate attribute along with details of the predicate itself, such as predictate type (e.g. equality or range) and predicate value.\n",
    "\n",
    "** Idea for **encoding** the predicate information into a fixed size feature vector:\n",
    "\n",
    "Example: For table with three attributes A, B, C\n",
    "\n",
    "Make a feature vector containing equal sized \"slots\" for each attribute. Then given a predicate on a single attribute, fill the corresponding slot with information about that predicate, e.g. predicate type (such as equality or range) and predicate value. And fill the remaining slots with zero.\n",
    "\n",
    "`[ -- slot A -- | -- slot B -- | -- slot C -- ]`\n",
    "\n",
    "This kind of encoding scheme can also be naturally extended to the case of multi-attribute predicates.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training Phase\n",
    "\n",
    "\n",
    "For the pre-training phase, we will borrow the query selectivity estimator from our simple cost model, which uses postgres internal table statistics and assumes uniform data distributions. We will train the selectivity prediction model to match the predictions of the simple cost model's selectivity estimator.\n",
    "\n",
    "### Fine-tuning Phase\n",
    "\n",
    "After pretraining, we can finetune the model via online updates using observed selectivities from actual query execution plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# auto reload all modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from simple_cost_model import *\n",
    "from ssb_qgen_class import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up query generator\n",
    "qgen = QGEN()\n",
    "\n",
    "# Get the statistics for all tables in the SSB database\n",
    "table_names = [\"customer\", \"dwdate\", \"lineorder\", \"part\", \"supplier\"]\n",
    "stats = {}\n",
    "estimated_rows = {}\n",
    "for table_name in table_names:\n",
    "    stats[table_name], estimated_rows[table_name] = get_table_stats(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selectivity Estimator from Simple Cost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_selectivity_one_sided_range(self, attribute, boundary_value, operator, stats_dict, total_rows):\n",
    "    data_type = self.data_type_dict[attribute]\n",
    "    # Get the column statistics\n",
    "    stats = stats_dict[attribute]\n",
    "    histogram_bounds = stats['histogram_bounds']\n",
    "    n_distinct = stats['n_distinct']\n",
    "    most_common_vals = stats['most_common_vals']\n",
    "    most_common_freqs = stats['most_common_freqs']\n",
    "\n",
    "    # Convert most_common_values string to list of correct data type\n",
    "    if most_common_vals:\n",
    "        if data_type == 'numeric':\n",
    "            most_common_vals = [float(x) for x in most_common_vals.strip('{}').split(',')]\n",
    "        elif data_type == 'char':\n",
    "            most_common_vals = [x for x in most_common_vals.strip('{}').split(',')]\n",
    "        else:\n",
    "            raise ValueError(\"Data type not supported, needs to be either numeric or char\")\n",
    "\n",
    "    # Convert negative n_distinct to an absolute count\n",
    "    if n_distinct < 0:\n",
    "        n_distinct = -n_distinct * total_rows\n",
    "\n",
    "    selectivity = 0.0\n",
    "\n",
    "    # Check for overlap with most common values\n",
    "    if most_common_vals:\n",
    "        for val, freq in zip(most_common_vals, most_common_freqs):\n",
    "            if (operator == '>' and val > boundary_value) or (operator == '<' and val < boundary_value):\n",
    "                selectivity += freq\n",
    "\n",
    "    if histogram_bounds is not None:\n",
    "        if data_type == 'numeric':\n",
    "            histogram_bounds = [float(x) for x in histogram_bounds.strip('{}').split(',')]\n",
    "        elif data_type == 'char':\n",
    "            histogram_bounds = [x for x in histogram_bounds.strip('{}').split(',')]\n",
    "        else:\n",
    "            raise ValueError(\"Data type not supported, needs to be either numeric or char\")\n",
    "\n",
    "        total_bins = len(histogram_bounds) - 1\n",
    "\n",
    "        # Iterate over bins, find overlapping bins\n",
    "        for i in range(total_bins):\n",
    "            bin_lower_bound = histogram_bounds[i]\n",
    "            bin_upper_bound = histogram_bounds[i + 1]\n",
    "\n",
    "            if data_type == 'numeric':\n",
    "                # Check for range overlap\n",
    "                if (operator == '>' and boundary_value < bin_upper_bound) or (operator == '<' and boundary_value > bin_lower_bound):\n",
    "                    # Calculate the overlap fraction within this bin\n",
    "                    if operator == '>':\n",
    "                        overlap_min = max(boundary_value, bin_lower_bound)\n",
    "                        overlap_fraction = (bin_upper_bound - overlap_min) / (bin_upper_bound - bin_lower_bound)\n",
    "                    else:  # operator == '<'\n",
    "                        overlap_max = min(boundary_value, bin_upper_bound)\n",
    "                        overlap_fraction = (overlap_max - bin_lower_bound) / (bin_upper_bound - bin_lower_bound)\n",
    "\n",
    "                    # Accumulate to the total selectivity\n",
    "                    selectivity += overlap_fraction * (1.0 / total_bins)\n",
    "\n",
    "            elif data_type == 'char':\n",
    "                if (operator == '>' and boundary_value < bin_upper_bound) or (operator == '<' and boundary_value > bin_lower_bound):\n",
    "                    # assume the whole bin overlaps\n",
    "                    overlap_fraction = 1.0\n",
    "                    # Accumulate to the total selectivity\n",
    "                    selectivity += overlap_fraction * (1.0 / total_bins)\n",
    "\n",
    "    if selectivity == 0.0:\n",
    "        # If no overlap with most common values or histogram bins, assume uniform distribution and estimate selectivity\n",
    "        selectivity = 1.0 / n_distinct\n",
    "\n",
    "    return selectivity\n",
    "\n",
    "\n",
    "def estimate_selectivity_range(self, attribute, value_range, stats_dict, total_rows):\n",
    "    data_type = self.data_type_dict[attribute]\n",
    "    # get the column statistics\n",
    "    stats = stats_dict[attribute]\n",
    "    # get the histogram bounds\n",
    "    histogram_bounds = stats['histogram_bounds']\n",
    "    n_distinct = stats['n_distinct']\n",
    "    most_common_vals = stats['most_common_vals']\n",
    "    most_common_freqs = stats['most_common_freqs']\n",
    "\n",
    "    #print(f\"Histogram bounds: {histogram_bounds}\")\n",
    "    #print(f\"Most common values: {most_common_vals}\")\n",
    "    #print(f\"Most common frequencies: {most_common_freqs}\")\n",
    "\n",
    "    # convert most_common_values string to list of correct data type\n",
    "    if most_common_vals:\n",
    "        if data_type == 'numeric':\n",
    "            most_common_vals = [float(x) for x in most_common_vals.strip('{}').split(',')]\n",
    "        elif data_type == 'char':\n",
    "            most_common_vals = [x for x in most_common_vals.strip('{}').split(',')]    \n",
    "        else:\n",
    "            raise ValueError(\"Data type not supported, needs ot be either numeric or char\")\n",
    "\n",
    "    # Convert negative n_distinct to an absolute count\n",
    "    if n_distinct < 0:\n",
    "        n_distinct = -n_distinct * total_rows\n",
    "\n",
    "    min_value = value_range[0]\n",
    "    max_value = value_range[1]\n",
    "    selectivity = 0.0\n",
    "\n",
    "    # check for overlap with most common values\n",
    "    if most_common_vals:\n",
    "        for val, freq in zip(most_common_vals, most_common_freqs):\n",
    "            if min_value <= val <= max_value:\n",
    "                selectivity += freq    \n",
    "\n",
    "    if histogram_bounds is not None:\n",
    "        if data_type == 'numeric':\n",
    "            histogram_bounds = [float(x) for x in histogram_bounds.strip('{}').split(',')] # convert to list of integers\n",
    "        elif data_type == 'char':\n",
    "            histogram_bounds = [x for x in histogram_bounds.strip('{}').split(',')]\n",
    "        else:\n",
    "            raise ValueError(\"Data type not supported, needs ot be either numeric or char\")    \n",
    "\n",
    "        total_bins = len(histogram_bounds) - 1\n",
    "\n",
    "        # iterate over bins, find overlapping bins\n",
    "        for i in range(total_bins):\n",
    "            bin_lower_bound = histogram_bounds[i]\n",
    "            bin_upper_bound = histogram_bounds[i+1]\n",
    "\n",
    "            # check for range overlap\n",
    "            if min_value < bin_lower_bound or max_value > bin_upper_bound:\n",
    "                # does not overlap\n",
    "                continue    \n",
    "\n",
    "            if data_type == 'numeric':\n",
    "                # calculate the overlap fraction within this bin\n",
    "                overlap_min = max(min_value, bin_lower_bound)\n",
    "                overlap_max = min(max_value, bin_upper_bound)\n",
    "                overlap_fraction = (overlap_max - overlap_min) / (bin_upper_bound - bin_lower_bound)\n",
    "\n",
    "                #print(f\"Overlap fraction for bin {i}: {overlap_fraction}\")\n",
    "                #print(f\"Bin bounds: {bin_lower_bound}, {bin_upper_bound}\")\n",
    "\n",
    "                # accumulate to the total selectivity\n",
    "                # Assume each bin represents an equal fraction of the total rows\n",
    "                selectivity += overlap_fraction * (1.0 / total_bins)\n",
    "\n",
    "            elif data_type == 'char':\n",
    "                # assume the whole bin overlaps\n",
    "                overlap_fraction = 1.0\n",
    "                # accumulate to the total selectivity\n",
    "                # Assume each bin represents an equal fraction of the total rows\n",
    "                selectivity += overlap_fraction * (1.0 / total_bins)\n",
    "\n",
    "    if selectivity == 0.0:\n",
    "        # if no overlap with most common values or histogram bins, assume uniform distribution and estimate selectivity\n",
    "        selectivity = 1.0 / n_distinct       \n",
    "\n",
    "    return selectivity\n",
    "\n",
    "\n",
    "def estimate_selectivity_eq(self, attribute, value, stats_dict):\n",
    "    data_type = self.data_type_dict[attribute]\n",
    "    # get the column statistics\n",
    "    stats = stats_dict[attribute]\n",
    "    # get the histogram bounds\n",
    "    histogram_bounds = stats['histogram_bounds']\n",
    "    n_distinct = stats['n_distinct']\n",
    "    most_common_vals = stats['most_common_vals']\n",
    "    most_common_freqs = stats['most_common_freqs']\n",
    "\n",
    "    # convert most_common_values string to list of correct data type\n",
    "    if most_common_vals:\n",
    "        if data_type == 'numeric':\n",
    "            most_common_vals = [float(x) for x in most_common_vals.strip('{}').split(',')]\n",
    "        elif data_type == 'char':\n",
    "            most_common_vals = [x for x in most_common_vals.strip('{}').split(',')]    \n",
    "        else:\n",
    "            raise ValueError(\"Data type not supported, needs ot be either numeric or char\")\n",
    "\n",
    "    # first check if the value is in the most common values\n",
    "    if most_common_vals and value in most_common_vals:\n",
    "        selectivity = most_common_freqs[most_common_vals.index(value)] \n",
    "        return selectivity\n",
    "\n",
    "    # if not a common value, estimate using n_distinct\n",
    "    if n_distinct < 0:\n",
    "        n_distinct = -n_distinct\n",
    "\n",
    "    selectivity = 1.0 / n_distinct    \n",
    "\n",
    "    if histogram_bounds is not None:\n",
    "        if data_type == 'numeric':\n",
    "            histogram_bounds = [float(x) for x in histogram_bounds.strip('{}').split(',')] # convert to list of integers\n",
    "        elif data_type == 'char':\n",
    "            histogram_bounds = [x for x in histogram_bounds.strip('{}').split(',')]\n",
    "        else:\n",
    "            raise ValueError(\"Data type not supported, needs ot be either numeric or char\")    \n",
    "\n",
    "        total_bins = len(histogram_bounds) - 1\n",
    "\n",
    "        # iterate over bins, find bin that contains the value\n",
    "        for i in range(total_bins):\n",
    "            bin_lower_bound = histogram_bounds[i]\n",
    "            bin_upper_bound = histogram_bounds[i+1]\n",
    "\n",
    "            if data_type == 'numeric':\n",
    "                # check for range overlap\n",
    "                if bin_lower_bound <= value <= bin_upper_bound:\n",
    "                    bin_width = bin_upper_bound - bin_lower_bound\n",
    "                    if bin_width > 0:\n",
    "                        # assume uniform distribution within this bin and calculate selectivity\n",
    "                        uniform_selectivity = 1.0 / (bin_width*total_bins)\n",
    "                        selectivity = min(selectivity, uniform_selectivity)\n",
    "                    break    \n",
    "\n",
    "            elif data_type == 'char':\n",
    "                # check for range overlap\n",
    "                if bin_lower_bound <= value <= bin_upper_bound:\n",
    "                    # assume the whole bin overlaps\n",
    "                    selectivity = 1.0 / total_bins\n",
    "                    break        \n",
    "\n",
    "    return selectivity\n",
    "\n",
    "\n",
    "def estimate_selectivity_or(self, attribute, value, stats_dict):\n",
    "    combined_selectivity = 0.0\n",
    "    individual_selectivities = []\n",
    "\n",
    "    # for each value in the IN list, estimate the selectivity separately\n",
    "    for val in value:\n",
    "        individual_selectivities.append(self.estimate_selectivity_eq(attribute, val, stats_dict))\n",
    "\n",
    "    # compute combined selectivities using inclusion-exclusion principle and assuming independence\n",
    "    for selectivity in individual_selectivities:\n",
    "        combined_selectivity += selectivity \n",
    "\n",
    "    overlap_adjustment = 1.0\n",
    "    for selectivity in individual_selectivities:\n",
    "        overlap_adjustment *= (1.0 - selectivity)\n",
    "\n",
    "    combined_selectivity -= overlap_adjustment   \n",
    "\n",
    "    # make sure the combined selectivity is between 0 and 1\n",
    "    combined_selectivity = max(0.0, min(combined_selectivity, 1.0))\n",
    "\n",
    "    return combined_selectivity \n",
    "\n",
    "\n",
    "def estimate_selectivity(self, attribute, operator, value, stats_dict, total_rows):\n",
    "    if operator == 'eq':\n",
    "        return self.estimate_selectivity_eq(attribute, value, stats_dict)\n",
    "    elif operator == 'range':\n",
    "        return self.estimate_selectivity_range(attribute, value, stats_dict, total_rows)\n",
    "    elif operator == '<' or operator == '>':\n",
    "        return self.estimate_selectivity_one_sided_range(attribute, value, operator, stats_dict, total_rows)\n",
    "    elif operator == 'or':\n",
    "        return self.estimate_selectivity_or(attribute, value, stats_dict)    \n",
    "    else:\n",
    "        raise ValueError(f\"Operator '{operator}' not supported, needs to be either 'eq', 'range', or 'or'\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicates:\n",
      "  Table: lineorder\n",
      "    {'column': 'lo_orderdate', 'operator': 'eq', 'value': 'd_datekey', 'join': True}\n",
      "    {'column': 'lo_discount', 'operator': 'range', 'value': (2, 4), 'join': False}\n",
      "    {'column': 'lo_quantity', 'operator': '<', 'value': 25, 'join': False}\n",
      "  Table: dwdate\n",
      "    {'column': 'd_year', 'operator': 'eq', 'value': 1998, 'join': False}\n"
     ]
    }
   ],
   "source": [
    "# generate example query\n",
    "example_query = qgen.generate_query(1)\n",
    "# extract the predicates from the query\n",
    "predicate_dict = example_query.predicate_dict\n",
    "\n",
    "print(f\"Predicates:\")\n",
    "for table_name, predicates in predicate_dict.items():\n",
    "    print(f\"  Table: {table_name}\")\n",
    "    for predicate in predicates:\n",
    "        print(f\"    {predicate}\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
