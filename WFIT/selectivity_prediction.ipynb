{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learned Selectivity Prediction:\n",
    "------------------------------\n",
    "\n",
    "\n",
    "So, essentially what I am trying to do is develop a simple algorithm (similar to the postgres query optimizer) which, given a query, predicts the cheapest access path for each table, assuming that the postgres query optimizer will always select the cheapest access path for each table involved in the query, where cost is proportional to total number of disk accesses. To make that prediction the algorithm uses table statistics such as a postgres stats histogram to estimate the selectivity of each precicate and then use independence assumption to compute cardinality (if multiple predicates are present on a single table). For skewed data distributions, I understand that postgres stats histograms may not be accurate and therefore my selectivity estimates may also be highly inaccurate. Thats why my initial idea was to learn a CDF function online for each table attribute. \n",
    "\n",
    "However, I realize now that this approach may not work very well, and instead maybe I should develop a single regression model on each table which can directly predict the selectivity of a given predicate. I could train this model online using explain analyze results from real time query executions. Beacuse those query plan results can be used to know the exact selectivity of each predicate, and features for predicates are also easy to extract and have a simple form, this type of model could potentially be better suited for my task. \n",
    "\n",
    "To keep things simple in the beginning, I want to start with a model for each table that only predicts selectivity for a single predicate, i.e. a predicate on a single attribute. Maybe once I can get such a model trained and working, I could think of ways to extend it to the multi-attribute case, i.e. if a query contains predicates over multiple attributes of a table, then the model shoudl be able to map a feature vector containing information about all of those predicates and predict a combnied selectivity (this could potentially be better that using individual predicate selectivities and then combining them under the independence assumption).\n",
    "\n",
    "The model can be pre-trained so that its predictions are consistent with uniform data distribution assumption. Then, the model can be refined online using actual query execution results (i.e. the actual selectivities observed in the query plan operators).\n",
    "\n",
    "Feature extraction for a single predicate:\n",
    "-----------------------------------------\n",
    "\n",
    "Even though I am predicting the selectivity for a single attribute, the model should be able to predict for any attribute in the table. So given that, how should the predicate be encoded into a fixed length feature vector which would allow the model to also know which particular attribute the selectivity corresponds to? i.e. the feature vector needs to somehow be able to encode the identity of the predicate attribute along with details of the predicate itself, such as predictate type (e.g. equality or range) and predicate value.\n",
    "\n",
    "** Idea for **encoding** the predicate information into a fixed size feature vector:\n",
    "\n",
    "Example: For table with three attributes A, B, C\n",
    "\n",
    "Make a feature vector containing equal sized \"slots\" for each attribute. Then given a predicate on a single attribute, fill the corresponding slot with information about that predicate, e.g. predicate type (such as equality or range) and predicate value. And fill the remaining slots with zero.\n",
    "\n",
    "`[ -- slot A -- | -- slot B -- | -- slot C -- ]`\n",
    "\n",
    "This kind of encoding scheme can also be naturally extended to the case of multi-attribute predicates.\n",
    "\n",
    "In addition to predicate type and predicate value, each slot should also contain other relavant information that might directly help the model to learn the underlying distribution of each attribute.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training Phase\n",
    "\n",
    "\n",
    "For the pre-training phase, we will borrow the query selectivity estimator from our simple cost model, which uses postgres internal table statistics and assumes uniform data distributions. We will train the selectivity prediction model to match the predictions of the simple cost model's selectivity estimator.\n",
    "\n",
    "### Fine-tuning Phase\n",
    "\n",
    "After pretraining, we can finetune the model via online updates using observed selectivities from actual query execution plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# auto reload all modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from simple_cost_model import *\n",
    "from ssb_qgen_class import *\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up query generator\n",
    "qgen = QGEN()\n",
    "\n",
    "# Get the statistics for all tables in the SSB database\n",
    "table_names = [\"customer\", \"dwdate\", \"lineorder\", \"part\", \"supplier\"]\n",
    "pg_stats = {}\n",
    "estimated_rows = {}\n",
    "for table_name in table_names:\n",
    "    pg_stats[table_name], estimated_rows[table_name] = get_table_stats(table_name)\n",
    "\n",
    "ssb_tables, pk_columns = get_ssb_schema()\n",
    "# create a dictionary and specify whether each attribute in each table is numeric or char\n",
    "data_type_dict = {}\n",
    "for table_name in [\"customer\", \"dwdate\", \"lineorder\", \"part\", \"supplier\"]:\n",
    "    for column_name, column_type in ssb_tables[table_name]:\n",
    "        if (\"INT\" in column_type) or (\"DECIMAL\" in column_type) or (\"BIT\" in column_type):\n",
    "            data_type_dict[column_name] = \"numeric\"\n",
    "        else:\n",
    "            data_type_dict[column_name] = \"char\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selectivity Estimator - Simple Cost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_selectivity_one_sided_range(attribute, boundary_value, operator, stats_dict, total_rows):\n",
    "    data_type = data_type_dict[attribute]\n",
    "    # Get the column statistics\n",
    "    stats = stats_dict[attribute]\n",
    "    histogram_bounds = stats['histogram_bounds']\n",
    "    n_distinct = stats['n_distinct']\n",
    "    most_common_vals = stats['most_common_vals']\n",
    "    most_common_freqs = stats['most_common_freqs']\n",
    "\n",
    "    # Convert most_common_values string to list of correct data type\n",
    "    if most_common_vals:\n",
    "        if data_type == 'numeric':\n",
    "            most_common_vals = [float(x) for x in most_common_vals.strip('{}').split(',')]\n",
    "        elif data_type == 'char':\n",
    "            most_common_vals = [x for x in most_common_vals.strip('{}').split(',')]\n",
    "        else:\n",
    "            raise ValueError(\"Data type not supported, needs to be either numeric or char\")\n",
    "\n",
    "    # Convert negative n_distinct to an absolute count\n",
    "    if n_distinct < 0:\n",
    "        n_distinct = -n_distinct * total_rows\n",
    "\n",
    "    selectivity = 0.0\n",
    "\n",
    "    # Check for overlap with most common values\n",
    "    if most_common_vals:\n",
    "        for val, freq in zip(most_common_vals, most_common_freqs):\n",
    "            if (operator == '>' and val > boundary_value) or (operator == '<' and val < boundary_value):\n",
    "                selectivity += freq\n",
    "\n",
    "    if histogram_bounds is not None:\n",
    "        if data_type == 'numeric':\n",
    "            histogram_bounds = [float(x) for x in histogram_bounds.strip('{}').split(',')]\n",
    "        elif data_type == 'char':\n",
    "            histogram_bounds = [x for x in histogram_bounds.strip('{}').split(',')]\n",
    "        else:\n",
    "            raise ValueError(\"Data type not supported, needs to be either numeric or char\")\n",
    "\n",
    "        total_bins = len(histogram_bounds) - 1\n",
    "\n",
    "        # Iterate over bins, find overlapping bins\n",
    "        for i in range(total_bins):\n",
    "            bin_lower_bound = histogram_bounds[i]\n",
    "            bin_upper_bound = histogram_bounds[i + 1]\n",
    "\n",
    "            if data_type == 'numeric':\n",
    "                # Check for range overlap\n",
    "                if (operator == '>' and boundary_value < bin_upper_bound) or (operator == '<' and boundary_value > bin_lower_bound):\n",
    "                    # Calculate the overlap fraction within this bin\n",
    "                    if operator == '>':\n",
    "                        overlap_min = max(boundary_value, bin_lower_bound)\n",
    "                        overlap_fraction = (bin_upper_bound - overlap_min) / (bin_upper_bound - bin_lower_bound)\n",
    "                    else:  # operator == '<'\n",
    "                        overlap_max = min(boundary_value, bin_upper_bound)\n",
    "                        overlap_fraction = (overlap_max - bin_lower_bound) / (bin_upper_bound - bin_lower_bound)\n",
    "\n",
    "                    # Accumulate to the total selectivity\n",
    "                    selectivity += overlap_fraction * (1.0 / total_bins)\n",
    "\n",
    "            elif data_type == 'char':\n",
    "                if (operator == '>' and boundary_value < bin_upper_bound) or (operator == '<' and boundary_value > bin_lower_bound):\n",
    "                    # assume the whole bin overlaps\n",
    "                    overlap_fraction = 1.0\n",
    "                    # Accumulate to the total selectivity\n",
    "                    selectivity += overlap_fraction * (1.0 / total_bins)\n",
    "\n",
    "    if selectivity == 0.0:\n",
    "        # If no overlap with most common values or histogram bins, assume uniform distribution and estimate selectivity\n",
    "        selectivity = 1.0 / n_distinct\n",
    "\n",
    "    return selectivity\n",
    "\n",
    "\n",
    "def estimate_selectivity_range(attribute, value_range, stats_dict, total_rows):\n",
    "    data_type = data_type_dict[attribute]\n",
    "    # get the column statistics\n",
    "    stats = stats_dict[attribute]\n",
    "    # get the histogram bounds\n",
    "    histogram_bounds = stats['histogram_bounds']\n",
    "    n_distinct = stats['n_distinct']\n",
    "    most_common_vals = stats['most_common_vals']\n",
    "    most_common_freqs = stats['most_common_freqs']\n",
    "\n",
    "    #print(f\"Histogram bounds: {histogram_bounds}\")\n",
    "    #print(f\"Most common values: {most_common_vals}\")\n",
    "    #print(f\"Most common frequencies: {most_common_freqs}\")\n",
    "\n",
    "    # convert most_common_values string to list of correct data type\n",
    "    if most_common_vals:\n",
    "        if data_type == 'numeric':\n",
    "            most_common_vals = [float(x) for x in most_common_vals.strip('{}').split(',')]\n",
    "        elif data_type == 'char':\n",
    "            most_common_vals = [x for x in most_common_vals.strip('{}').split(',')]    \n",
    "        else:\n",
    "            raise ValueError(\"Data type not supported, needs ot be either numeric or char\")\n",
    "\n",
    "    # Convert negative n_distinct to an absolute count\n",
    "    if n_distinct < 0:\n",
    "        n_distinct = -n_distinct * total_rows\n",
    "\n",
    "    min_value = value_range[0]\n",
    "    max_value = value_range[1]\n",
    "    selectivity = 0.0\n",
    "\n",
    "    # check for overlap with most common values\n",
    "    if most_common_vals:\n",
    "        for val, freq in zip(most_common_vals, most_common_freqs):\n",
    "            if min_value <= val <= max_value:\n",
    "                selectivity += freq    \n",
    "\n",
    "    if histogram_bounds is not None:\n",
    "        if data_type == 'numeric':\n",
    "            histogram_bounds = [float(x) for x in histogram_bounds.strip('{}').split(',')] # convert to list of integers\n",
    "        elif data_type == 'char':\n",
    "            histogram_bounds = [x for x in histogram_bounds.strip('{}').split(',')]\n",
    "        else:\n",
    "            raise ValueError(\"Data type not supported, needs ot be either numeric or char\")    \n",
    "\n",
    "        total_bins = len(histogram_bounds) - 1\n",
    "\n",
    "        # iterate over bins, find overlapping bins\n",
    "        for i in range(total_bins):\n",
    "            bin_lower_bound = histogram_bounds[i]\n",
    "            bin_upper_bound = histogram_bounds[i+1]\n",
    "\n",
    "            # check for range overlap\n",
    "            if min_value < bin_lower_bound or max_value > bin_upper_bound:\n",
    "                # does not overlap\n",
    "                continue    \n",
    "\n",
    "            if data_type == 'numeric':\n",
    "                # calculate the overlap fraction within this bin\n",
    "                overlap_min = max(min_value, bin_lower_bound)\n",
    "                overlap_max = min(max_value, bin_upper_bound)\n",
    "                overlap_fraction = (overlap_max - overlap_min) / (bin_upper_bound - bin_lower_bound)\n",
    "\n",
    "                #print(f\"Overlap fraction for bin {i}: {overlap_fraction}\")\n",
    "                #print(f\"Bin bounds: {bin_lower_bound}, {bin_upper_bound}\")\n",
    "\n",
    "                # accumulate to the total selectivity\n",
    "                # Assume each bin represents an equal fraction of the total rows\n",
    "                selectivity += overlap_fraction * (1.0 / total_bins)\n",
    "\n",
    "            elif data_type == 'char':\n",
    "                # assume the whole bin overlaps\n",
    "                overlap_fraction = 1.0\n",
    "                # accumulate to the total selectivity\n",
    "                # Assume each bin represents an equal fraction of the total rows\n",
    "                selectivity += overlap_fraction * (1.0 / total_bins)\n",
    "\n",
    "    if selectivity == 0.0:\n",
    "        # if no overlap with most common values or histogram bins, assume uniform distribution and estimate selectivity\n",
    "        selectivity = 1.0 / n_distinct       \n",
    "\n",
    "    return selectivity\n",
    "\n",
    "\n",
    "def estimate_selectivity_eq(attribute, value, stats_dict):\n",
    "    data_type = data_type_dict[attribute]\n",
    "    # get the column statistics\n",
    "    stats = stats_dict[attribute]\n",
    "    # get the histogram bounds\n",
    "    histogram_bounds = stats['histogram_bounds']\n",
    "    n_distinct = stats['n_distinct']\n",
    "    most_common_vals = stats['most_common_vals']\n",
    "    most_common_freqs = stats['most_common_freqs']\n",
    "\n",
    "    # convert most_common_values string to list of correct data type\n",
    "    if most_common_vals:\n",
    "        if data_type == 'numeric':\n",
    "            most_common_vals = [float(x) for x in most_common_vals.strip('{}').split(',')]\n",
    "        elif data_type == 'char':\n",
    "            most_common_vals = [x for x in most_common_vals.strip('{}').split(',')]    \n",
    "        else:\n",
    "            raise ValueError(\"Data type not supported, needs ot be either numeric or char\")\n",
    "\n",
    "    # first check if the value is in the most common values\n",
    "    if most_common_vals and value in most_common_vals:\n",
    "        selectivity = most_common_freqs[most_common_vals.index(value)] \n",
    "        return selectivity\n",
    "\n",
    "    # if not a common value, estimate using n_distinct\n",
    "    if n_distinct < 0:\n",
    "        n_distinct = -n_distinct\n",
    "\n",
    "    selectivity = 1.0 / n_distinct    \n",
    "\n",
    "    if histogram_bounds is not None:\n",
    "        if data_type == 'numeric':\n",
    "            histogram_bounds = [float(x) for x in histogram_bounds.strip('{}').split(',')] # convert to list of integers\n",
    "        elif data_type == 'char':\n",
    "            histogram_bounds = [x for x in histogram_bounds.strip('{}').split(',')]\n",
    "        else:\n",
    "            raise ValueError(\"Data type not supported, needs ot be either numeric or char\")    \n",
    "\n",
    "        total_bins = len(histogram_bounds) - 1\n",
    "\n",
    "        # iterate over bins, find bin that contains the value\n",
    "        for i in range(total_bins):\n",
    "            bin_lower_bound = histogram_bounds[i]\n",
    "            bin_upper_bound = histogram_bounds[i+1]\n",
    "\n",
    "            if data_type == 'numeric':\n",
    "                # check for range overlap\n",
    "                if bin_lower_bound <= value <= bin_upper_bound:\n",
    "                    bin_width = bin_upper_bound - bin_lower_bound\n",
    "                    if bin_width > 0:\n",
    "                        # assume uniform distribution within this bin and calculate selectivity\n",
    "                        uniform_selectivity = 1.0 / (bin_width*total_bins)\n",
    "                        selectivity = min(selectivity, uniform_selectivity)\n",
    "                    break    \n",
    "\n",
    "            elif data_type == 'char':\n",
    "                # check for range overlap\n",
    "                if bin_lower_bound <= value <= bin_upper_bound:\n",
    "                    # assume the whole bin overlaps\n",
    "                    selectivity = 1.0 / total_bins\n",
    "                    break        \n",
    "\n",
    "    return selectivity\n",
    "\n",
    "\n",
    "def estimate_selectivity_or(attribute, value, stats_dict):\n",
    "    combined_selectivity = 0.0\n",
    "    individual_selectivities = []\n",
    "\n",
    "    # for each value in the IN list, estimate the selectivity separately\n",
    "    for val in value:\n",
    "        individual_selectivities.append(estimate_selectivity_eq(attribute, val, stats_dict))\n",
    "\n",
    "    # compute combined selectivities using inclusion-exclusion principle and assuming independence\n",
    "    for selectivity in individual_selectivities:\n",
    "        combined_selectivity += selectivity \n",
    "\n",
    "    overlap_adjustment = 1.0\n",
    "    for selectivity in individual_selectivities:\n",
    "        overlap_adjustment *= (1.0 - selectivity)\n",
    "\n",
    "    combined_selectivity -= overlap_adjustment   \n",
    "\n",
    "    # make sure the combined selectivity is between 0 and 1\n",
    "    combined_selectivity = max(0.0, min(combined_selectivity, 1.0))\n",
    "\n",
    "    return combined_selectivity \n",
    "\n",
    "\n",
    "def estimate_selectivity(attribute, operator, value, stats_dict, total_rows):\n",
    "    if operator == 'eq':\n",
    "        return estimate_selectivity_eq(attribute, value, stats_dict)\n",
    "    elif operator == 'range':\n",
    "        return estimate_selectivity_range(attribute, value, stats_dict, total_rows)\n",
    "    elif operator == '<' or operator == '>':\n",
    "        return estimate_selectivity_one_sided_range(attribute, value, operator, stats_dict, total_rows)\n",
    "    elif operator == 'or':\n",
    "        return estimate_selectivity_or(attribute, value, stats_dict)    \n",
    "    else:\n",
    "        raise ValueError(f\"Operator '{operator}' not supported, needs to be either 'eq', 'range', or 'or'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicates:\n",
      "  Table: lineorder\n",
      "    {'column': 'lo_orderdate', 'operator': 'eq', 'value': 'd_datekey', 'join': True}\n",
      "    {'column': 'lo_discount', 'operator': 'range', 'value': (3, 5), 'join': False}\n",
      "    {'column': 'lo_quantity', 'operator': '<', 'value': 25, 'join': False}\n",
      "  Table: dwdate\n",
      "    {'column': 'd_year', 'operator': 'eq', 'value': 1998, 'join': False}\n"
     ]
    }
   ],
   "source": [
    "# generate example query\n",
    "example_query = qgen.generate_query(1)\n",
    "# extract the predicates from the query\n",
    "predicate_dict = example_query.predicate_dict\n",
    "\n",
    "print(f\"Predicates:\")\n",
    "for table_name, predicates in predicate_dict.items():\n",
    "    print(f\"  Table: {table_name}\")\n",
    "    for predicate in predicates:\n",
    "        print(f\"    {predicate}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: lineorder\n",
      "\n",
      "  Predicate: {'column': 'lo_discount', 'operator': 'range', 'value': (3, 5), 'join': False}\n",
      "  Selectivity: 0.273600006\n",
      "\n",
      "  Predicate: {'column': 'lo_quantity', 'operator': '<', 'value': 25, 'join': False}\n",
      "  Selectivity: 0.4762666669999999\n",
      "\n",
      "Table: dwdate\n",
      "\n",
      "  Predicate: {'column': 'd_year', 'operator': 'eq', 'value': 1998, 'join': False}\n",
      "  Selectivity: 0.14241001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test out the selectivity estimation functions using the example query predicates\n",
    "total_rows = estimated_rows[\"lineorder\"]\n",
    "for table_name, predicates in predicate_dict.items():\n",
    "    print(f\"Table: {table_name}\\n\")\n",
    "    for predicate in predicates:\n",
    "        if predicate['join'] == False:\n",
    "            attribute = predicate['column']\n",
    "            operator = predicate['operator']\n",
    "            value = predicate['value']\n",
    "            selectivity = estimate_selectivity(attribute, operator, value, pg_stats[table_name], total_rows)\n",
    "            print(f\"  Predicate: {predicate}\")\n",
    "            print(f\"  Selectivity: {selectivity}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selectivity Estimator - Exact\n",
    "\n",
    "For exact selectivity estimation, we will use the actual uniform distributions of attributes in the tables.\n",
    "\n",
    "For sanity check, we can compare the simple cost model's selectivity estimates with the exact selectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ssb10_stats.pkl\n",
    "with open('ssb10_stats.pkl', 'rb') as f:\n",
    "    actual_stats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(f\"Supplier table actual statistics.\\n\")\\nfor attribute, stats in actual_stats[\\'supplier\\'].items():\\n    print(attribute)\\n    print(f\"  {stats}\\n\")'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"print(f\"Supplier table actual statistics.\\n\")\n",
    "for attribute, stats in actual_stats['supplier'].items():\n",
    "    print(attribute)\n",
    "    print(f\"  {stats}\\n\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_selectivity_eq_exact(attribute, value, stats_dict):\n",
    "    data_type = data_type_dict[attribute]\n",
    "    # get the column statistics\n",
    "    stats = stats_dict[attribute]\n",
    "    #print(f\"Stats: {stats}\")\n",
    "    min = stats['min'] \n",
    "    max = stats['max']\n",
    "    total_count = stats['total_count']\n",
    "    distinct_count = stats['distinct_count']\n",
    "    histogram = stats['histogram']\n",
    "\n",
    "    # check if historgam is available\n",
    "    if histogram is not None:\n",
    "        # check if the value is in the histogram\n",
    "        if value in histogram:\n",
    "            selectivity = histogram[value] / total_count\n",
    "        else:\n",
    "            selectivity = 0.0\n",
    "\n",
    "    else:\n",
    "        # assume uniform distribution\n",
    "        if min <= value <= max:\n",
    "            selectivity = 1.0 / distinct_count\n",
    "        else:\n",
    "            selectivity = 0.0    \n",
    "\n",
    "    return selectivity\n",
    "\n",
    "\n",
    "def estimate_selectivity_or_exact(attribute, value, stats_dict):\n",
    "    combined_selectivity = 0.0\n",
    "    individual_selectivities = []\n",
    "\n",
    "    # for each value in the IN list, estimate the selectivity separately\n",
    "    for val in value:\n",
    "        individual_selectivities.append(estimate_selectivity_eq_exact(attribute, val, stats_dict))\n",
    "\n",
    "    # compute combined selectivities using inclusion-exclusion principle and assuming independence\n",
    "    for selectivity in individual_selectivities:\n",
    "        combined_selectivity += selectivity \n",
    "\n",
    "    overlap_adjustment = 1.0\n",
    "    for selectivity in individual_selectivities:\n",
    "        overlap_adjustment *= (1.0 - selectivity)\n",
    "\n",
    "    combined_selectivity -= overlap_adjustment   \n",
    "\n",
    "    # make sure the combined selectivity is between 0 and 1\n",
    "    combined_selectivity = max(0.0, min(combined_selectivity, 1.0))\n",
    "\n",
    "    return combined_selectivity \n",
    "\n",
    "\n",
    "def estimate_selectivity_range_exact(attribute, value_range, stats_dict, total_rows):\n",
    "    data_type = data_type_dict[attribute]\n",
    "    # get the column statistics\n",
    "    stats = stats_dict[attribute]\n",
    "    #print(f\"Stats: {stats}\")\n",
    "    min = stats['min'] \n",
    "    max = stats['max']  \n",
    "    total_count = stats['total_count']\n",
    "    distinct_count = stats['distinct_count']\n",
    "    histogram = stats['histogram']\n",
    "    #print(f\"Data type: {data_type}, Min: {min}, Max: {max}, Total count: {total_count}, Distinct count: {distinct_count}\")\n",
    "\n",
    "    selectivity = 0.0\n",
    "    if min <= value_range[0] <= value_range[1] <= max:\n",
    "        # use histogram if available\n",
    "        if histogram is not None:\n",
    "            # iterate over historgram values and counts\n",
    "            for val, count in histogram.items():\n",
    "                #print(f\"Historgram, value: {val}, count: {count}\")\n",
    "                if value_range[0] <= val <= value_range[1]:\n",
    "                    selectivity += count\n",
    "\n",
    "            selectivity /= total_count\n",
    "\n",
    "        # otherwise use min and max values and assume uniform distribution\n",
    "        # Note: this won't work for char type columns\n",
    "        else:\n",
    "            selectivity = (value_range[1] - value_range[0]) / (max - min)     \n",
    "\n",
    "    return selectivity           \n",
    "\n",
    "\n",
    "def estimate_selectivity_one_sided_range_exact(attribute, boundary_value, operator, stats_dict, total_rows):\n",
    "    data_type = data_type_dict[attribute]\n",
    "    # get the column statistics\n",
    "    stats = stats_dict[attribute]\n",
    "    #print(f\"Stats: {stats}\")\n",
    "    min = stats['min'] \n",
    "    max = stats['max']\n",
    "    total_count = stats['total_count']\n",
    "    distinct_count = stats['distinct_count']\n",
    "    histogram = stats['histogram']\n",
    "\n",
    "    selectivity = 0.0\n",
    "    if operator == '<':\n",
    "        # check if within bounds\n",
    "        if min <= boundary_value <= max:\n",
    "            # use histogram if available\n",
    "            if histogram is not None:\n",
    "                # iterate over historgram values and counts\n",
    "                for val, count in histogram.items():\n",
    "                    if val < boundary_value:\n",
    "                        selectivity += count\n",
    "\n",
    "                selectivity /= total_count        \n",
    "                \n",
    "            # otherwise use min and max values and assume uniform distribution\n",
    "            # Note: this won't work for char type columns\n",
    "            else:\n",
    "                selectivity = (boundary_value - min) / (max - min)\n",
    "\n",
    "    elif operator == '>':\n",
    "        # check if within bounds\n",
    "        if min <= boundary_value <= max:\n",
    "            # use histogram if available\n",
    "            if histogram is not None:\n",
    "               # iterate over historgram values and counts\n",
    "                for val, count in histogram.items():\n",
    "                    if val > boundary_value:\n",
    "                        selectivity += count\n",
    "\n",
    "                selectivity /= total_count\n",
    "               \n",
    "            # otherwise use min and max values and assume uniform distribution\n",
    "            # Note: this won't work for char type columns\n",
    "            else:\n",
    "                selectivity = (max - boundary_value) / (max - min)\n",
    "\n",
    "    return selectivity\n",
    "\n",
    "\n",
    "def estimate_selectivity_exact(attribute, operator, value, stats_dict, total_rows):\n",
    "    if operator == 'eq':\n",
    "        return estimate_selectivity_eq_exact(attribute, value, stats_dict)\n",
    "    elif operator == 'range':\n",
    "        return estimate_selectivity_range_exact(attribute, value, stats_dict, total_rows)\n",
    "    elif operator == '<' or operator == '>':\n",
    "        return estimate_selectivity_one_sided_range_exact(attribute, value, operator, stats_dict, total_rows)\n",
    "    elif operator == 'or':\n",
    "        return estimate_selectivity_or_exact(attribute, value, stats_dict)    \n",
    "    else:\n",
    "        raise ValueError(f\"Operator '{operator}' not supported, needs to be either 'eq', 'range', or 'or'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: lineorder\n",
      "\n",
      "  Predicate: {'column': 'lo_discount', 'operator': 'range', 'value': (3, 5), 'join': False}\n",
      "  Selectivity: 0.27278847769922604\n",
      "\n",
      "  Predicate: {'column': 'lo_quantity', 'operator': '<', 'value': 25, 'join': False}\n",
      "  Selectivity: 0.47999450340373206\n",
      "\n",
      "Table: dwdate\n",
      "\n",
      "  Predicate: {'column': 'd_year', 'operator': 'eq', 'value': 1998, 'join': False}\n",
      "  Selectivity: 0.14241001564945227\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test out the exact selectivity estimation functions using the example query predicates\n",
    "total_rows = estimated_rows[\"lineorder\"]\n",
    "for table_name, predicates in predicate_dict.items():\n",
    "    print(f\"Table: {table_name}\\n\")\n",
    "    for predicate in predicates:\n",
    "        if predicate['join'] == False:\n",
    "            attribute = predicate['column']\n",
    "            operator = predicate['operator']\n",
    "            value = predicate['value']\n",
    "            selectivity = estimate_selectivity_exact(attribute, operator, value, actual_stats[table_name], total_rows)\n",
    "            print(f\"  Predicate: {predicate}\")\n",
    "            print(f\"  Selectivity: {selectivity}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Query Selectivity Model\n",
    "\n",
    "\n",
    "Feature Engineering:\n",
    "\n",
    "The feature vector contains equal sized slots for each attribute on the table. Each slot will contain the following information:\n",
    "\n",
    "1) A binary indicator (0 - no, 1 - yes) for whether the predicate is on the corresponding slot\n",
    "2) A binary indicator for predicate type (0 - equality, 1 - range) to indicate whether the predicate is an equality or range\n",
    "3) Two scalar predicate range values (i.e. upper and lower bounds), for equality set both values to the equality predicate value\n",
    "\n",
    "\n",
    "For now, we will only predict selectivity for equality and range predicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectivityModel:\n",
    "\n",
    "    def __init__(self, table_name, table_attributes, lambda_reg=0.1, epsilon=1e-8):\n",
    "        self.table_name = table_name\n",
    "        self.table_attributes = table_attributes\n",
    "        # mapping from attribute name to id\n",
    "        self.attr2id = {attr: i for i, attr in enumerate(table_attributes)}\n",
    "        # mapping from id to attribute name\n",
    "        self.id2attr = {i: attr for i, attr in enumerate(table_attributes)}\n",
    "        \n",
    "        # set feature dimensions\n",
    "        self.features_per_attribute = 4\n",
    "        self.feature_dims = 4 * len(table_attributes)   # 4 features per attribute\n",
    "            \n",
    "        self.V = lambda_reg * np.eye(self.feature_dims)\n",
    "        self.b = np.zeros(self.feature_dims)\n",
    "        self.theta = np.zeros(self.feature_dims)\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.epsilon = epsilon\n",
    "        self.loss_history = []\n",
    "        #self.scaler = StandardScaler()\n",
    "        #self.normalize_features()\n",
    "\n",
    "\n",
    "    #def normalize_features(self):\n",
    "    #    all_features = np.array(list(self.feature_vectors.values()))\n",
    "    #    self.scaler.fit(all_features)\n",
    "    #    for key in self.feature_vectors:\n",
    "    #        self.feature_vectors[key] = self.scaler.transform([self.feature_vectors[key]])[0]\n",
    "\n",
    "    def update(self, predicate, y_actual, verbose=False):\n",
    "        x = self.predicate_to_feature_vector(predicate)\n",
    "        self.V += np.outer(x, x)\n",
    "        self.b += y_actual * x\n",
    "        # add small epsilon to diagonal of V for conditioning\n",
    "        #self.theta = np.linalg.solve(self.V + self.epsilon*np.eye(self.feature_dims), self.b)\n",
    "        self.theta = np.linalg.solve(self.V, self.b)\n",
    "        loss, y_pred = self.compute_loss(predicate, y_actual)\n",
    "        if verbose:\n",
    "            print(f\"Actual selectivity: {y_actual}, predicted selectivity: {y_pred:.3f}, loss incurred: {loss}\")\n",
    "\n",
    "    \n",
    "    def predict(self, predicate):\n",
    "        x = self.predicate_to_feature_vector(predicate)\n",
    "        # need to constrain the predicted selectivity to be between 0 and 1\n",
    "        y_pred = np.dot(self.theta, x)\n",
    "        y_pred = min(max(0, y_pred), 1)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "    def compute_loss(self, predicate, y_actual):\n",
    "        y_pred = self.predict(predicate)\n",
    "        mse = (y_actual - y_pred)**2\n",
    "        reg = self.lambda_reg * np.dot(self.theta, self.theta)\n",
    "        loss = mse + reg\n",
    "        self.loss_history.append(loss)\n",
    "        return loss, y_pred    \n",
    "        \n",
    "\n",
    "    def predicate_to_feature_vector(self, predicate):\n",
    "        # prepare feature vector\n",
    "        attribute = predicate['column']\n",
    "        operator = predicate['operator']\n",
    "        value = predicate['value']\n",
    "        if operator == 'eq':\n",
    "            type_indicator = 0\n",
    "            lower = value\n",
    "            upper = value\n",
    "        elif operator == 'range':\n",
    "            type_indicator = 1\n",
    "            lower = value[0]\n",
    "            upper = value[1]\n",
    "        elif operator == '<':\n",
    "            type_indicator = 1\n",
    "            lower = 0\n",
    "            upper = value\n",
    "        elif operator == '>':\n",
    "            type_indicator = 1\n",
    "            lower = value\n",
    "            upper = 0\n",
    "\n",
    "        predicate_features = [1, type_indicator, lower, upper]\n",
    "        slot_start = self.attr2id[attribute] * self.features_per_attribute\n",
    "        x = np.zeros(self.feature_dims)\n",
    "        x[slot_start:slot_start+self.features_per_attribute] = predicate_features\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Generator for Pre-Training of Query Selectivity Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a data generator for training the selectivity prediction model, generates predicates and estimated selectivities using simple_cost_model\n",
    "def generate_predicate(): \n",
    "    done = False\n",
    "    while not done:\n",
    "        # pick a random table\n",
    "        table_name = np.random.choice(table_names)\n",
    "        # get the table attributes\n",
    "        table_attributes = [column[0] for column in ssb_tables[table_name]]\n",
    "        # pick a random attribute\n",
    "        attribute = np.random.choice(table_attributes)\n",
    "        data_type = data_type_dict[attribute]\n",
    "        # pick a random operator\n",
    "        operator = np.random.choice(['eq', 'range', '<', '>'])\n",
    "        # pick a random value\n",
    "        if operator == 'eq':\n",
    "            # use histogram if available\n",
    "            if actual_stats[table_name][attribute]['histogram'] is not None:\n",
    "                value = np.random.choice(list(actual_stats[table_name][attribute]['histogram'].keys()))\n",
    "                done = True\n",
    "            else:\n",
    "                if data_type == 'numeric':\n",
    "                    # pick uniformly from the min and max values if data type is numeric\n",
    "                    value = np.random.uniform(actual_stats[table_name][attribute]['min'], actual_stats[table_name][attribute]['max'])      \n",
    "                    done = True\n",
    "                else:\n",
    "                    # for char type, if historgram is not available, then skip\n",
    "                    continue\n",
    "\n",
    "        elif operator == 'range':\n",
    "            # use histogram if available\n",
    "            if actual_stats[table_name][attribute]['histogram'] is not None:\n",
    "                # pick a random range from the histogram\n",
    "                value = np.sort(np.random.choice(list(actual_stats[table_name][attribute]['histogram'].keys()), 2))\n",
    "                done = True\n",
    "            else:\n",
    "                if data_type == 'numeric':\n",
    "                    # pick a random range from the min and max values if data type is numeric\n",
    "                    value = np.sort(np.random.uniform(actual_stats[table_name][attribute]['min'], actual_stats[table_name][attribute]['max'], 2))\n",
    "                    done = True\n",
    "                else:\n",
    "                    # for char type, if historgram is not available, then skip\n",
    "                    continue\n",
    "\n",
    "        elif operator == '<' or operator == '>':\n",
    "            # use histogram if available\n",
    "            if actual_stats[table_name][attribute]['histogram'] is not None:\n",
    "                value = np.random.choice(list(actual_stats[table_name][attribute]['histogram'].keys()))\n",
    "                done = True\n",
    "            else:\n",
    "                if data_type == 'numeric':\n",
    "                    # pick uniformly from the min and max values if data type is numeric\n",
    "                    value = np.random.uniform(actual_stats[table_name][attribute]['min'], actual_stats[table_name][attribute]['max'])      \n",
    "                    done = True\n",
    "                else:\n",
    "                    # for char type, if historgram is not available, then skip\n",
    "                    continue\n",
    "\n",
    "    predicate = {'column': attribute, 'operator': operator, 'value': value, 'join': False}\n",
    "\n",
    "    # get simple cost model estimated selectivity\n",
    "    total_rows = estimated_rows[table_name]\n",
    "    selectivity = estimate_selectivity_exact(attribute, operator, value, actual_stats[table_name], total_rows)    \n",
    "\n",
    "    return table_name, predicate, selectivity                \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: lineorder\n",
      "Predicate: {'column': 'lo_quantity', 'operator': 'range', 'value': array([29, 38]), 'join': False}\n",
      "Simplified cost model estimated selectivity: 0.200\n"
     ]
    }
   ],
   "source": [
    "table_name, predicate, selectivity = generate_predicate()\n",
    "\n",
    "print(f\"Table: {table_name}\")\n",
    "print(f\"Predicate: {predicate}\")\n",
    "print(f\"Simplified cost model estimated selectivity: {selectivity:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
