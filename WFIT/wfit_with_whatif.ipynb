{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WFIT Algorithm Implementation (Schnaitter 2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "import IPython\n",
    "notebook_path = IPython.get_ipython().starting_dir\n",
    "target_subdirectory_path = os.path.abspath(os.path.join(os.path.dirname(notebook_path), 'PostgreSQL'))\n",
    "sys.path.append(target_subdirectory_path)\n",
    "\n",
    "from pg_utils import *\n",
    "from ssb_qgen_class import *\n",
    "\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "import time\n",
    "import random\n",
    "from more_itertools import powerset\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index Benefit Graph (IBG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, id, indexes):\n",
    "        self.id = id\n",
    "        self.indexes = indexes\n",
    "        self.children = []\n",
    "        self.parents = []\n",
    "        self.built = False\n",
    "        self.cost = None\n",
    "        self.used = None\n",
    "\n",
    "\n",
    "# class for creating and storing the IBG\n",
    "class IBG:\n",
    "    # Class-level cache\n",
    "    #_class_cache = {}\n",
    "\n",
    "    def __init__(self, query_object, C):\n",
    "        self.q = query_object\n",
    "        self.C = C\n",
    "        print(f\"Number of candidate indexes: {len(self.C)}\")\n",
    "        #print(f\"Candidate indexes: {self.C}\")\n",
    "        \n",
    "        # map index_id to integer\n",
    "        self.idx2id = {index.index_id:i for i, index in enumerate(self.C)}\n",
    "        self.idx2index = {index.index_id:index for index in self.C}\n",
    "        print(f\"Index id to integer mapping: {self.idx2id}\")\n",
    "        \n",
    "        # create a hash table for keeping track of all created nodes\n",
    "        self.nodes = {}\n",
    "        # create a root node\n",
    "        self.root = Node(self.get_configuration_id(self.C), self.C)\n",
    "        self.nodes[self.root.id] = self.root\n",
    "        print(f\"Created root node with id: {self.root.id}\")\n",
    "        \n",
    "        self.total_whatif_calls = 0\n",
    "        self.total_whatif_time = 0\n",
    "        self.node_count = 0\n",
    "\n",
    "        # start the IBG construction\n",
    "        print(\"Constructing IBG...\")\n",
    "        self.construct_ibg(self.root)\n",
    "        print(f\"Number of nodes in IBG: {len(self.nodes)}, Total number of what-if calls: {self.total_whatif_calls}, Time spent on what-if calls: {self.total_whatif_time}\")\n",
    "        # compute all pair degree of interaction\n",
    "        print(f\"Computing all pair degree of interaction...\")\n",
    "        self.doi = self.compute_all_pair_doi()\n",
    "\n",
    "\n",
    "    # assign unique string id to a configuration\n",
    "    def get_configuration_id(self, indexes):\n",
    "        # get sorted list of integer ids\n",
    "        ids = sorted([self.idx2id[idx.index_id] for idx in indexes])\n",
    "        return \"_\".join([str(i) for i in ids])\n",
    "    \n",
    "    def _get_cost_used(self, indexes):\n",
    "        # Convert indexes to a tuple to make it hashable\n",
    "        #indexes_tuple = tuple(sorted(indexes, key=lambda x: x.index_id))\n",
    "        # Check if the result is already in the class-level cache\n",
    "        #if indexes_tuple in self._class_cache:\n",
    "        #    return self._class_cache[indexes_tuple]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        conn = create_connection()\n",
    "        # create hypothetical indexes\n",
    "        hypo_indexes = bulk_create_hypothetical_indexes(conn, indexes)\n",
    "        # map oid to index object\n",
    "        oid2index = {}\n",
    "        for i in range(len(hypo_indexes)):\n",
    "            oid2index[hypo_indexes[i]] = indexes[i]\n",
    "        # get cost and used indexes\n",
    "        cost, indexes_used = get_query_cost_estimate_hypo_indexes(conn, self.q.query_string, show_plan=False)\n",
    "        # map used index oids to index objects\n",
    "        used = [oid2index[oid] for oid, scan_type, scan_cost in indexes_used]\n",
    "        # drop hypothetical indexes\n",
    "        bulk_drop_hypothetical_indexes(conn)\n",
    "        close_connection(conn)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Store the result in the class-level cache\n",
    "        #self._class_cache[indexes_tuple] = (cost, used)\n",
    "        self.total_whatif_calls += 1\n",
    "        self.total_whatif_time += end_time - start_time\n",
    "\n",
    "        return cost, used\n",
    "\n",
    "    # Ensure the indexes parameter is hashable\n",
    "    def _cached_get_cost_used(self, indexes):\n",
    "        return self._get_cost_used(tuple(indexes))\n",
    "\n",
    "    # recursive IBG construction algorithm\n",
    "    def construct_ibg(self, Y):\n",
    "        if Y.built:\n",
    "            return \n",
    "        \n",
    "        # obtain query optimizers cost and used indexes\n",
    "        #print(f\"Creating node for configuration: {[idx.index_id for idx in Y.indexes]}\")\n",
    "        self.node_count += 1\n",
    "        print(f\"Creating node # {self.node_count}\", end=\"\\r\")\n",
    "\n",
    "        cost, used = self._cached_get_cost_used(Y.indexes)\n",
    "        Y.cost = cost\n",
    "        Y.used = used\n",
    "        Y.built = True\n",
    "        \n",
    "        #print(f\"Cost: {cost}, Used indexes: {[idx.index_id for idx in used]}\")\n",
    "        #for idx in used:\n",
    "        #    print(f\"{idx}\")\n",
    "\n",
    "        # create children\n",
    "        for a in Y.used:\n",
    "            # create a new configuration with index a removed from Y\n",
    "            X_indexes = [index for index in Y.indexes if index != a]\n",
    "            X_id = self.get_configuration_id(X_indexes)\n",
    "            \n",
    "            # if X is not in the hash table, create a new node and recursively build it\n",
    "            if X_id not in self.nodes:\n",
    "                X = Node(X_id, X_indexes)\n",
    "                X.parents.append(Y)\n",
    "                self.nodes[X_id] = X\n",
    "                Y.children.append(X)\n",
    "                self.construct_ibg(X)\n",
    "\n",
    "            else:\n",
    "                X = self.nodes[X_id]\n",
    "                Y.children.append(X)\n",
    "                X.parents.append(Y)\n",
    "\n",
    "\n",
    "    # use IBG to obtain estimated cost and used indexes for arbitrary subset of C\n",
    "    def get_cost_used(self, X):\n",
    "        # get id of the configuration\n",
    "        id = self.get_configuration_id(X)\n",
    "        # check if the configuration is in the IBG\n",
    "        if id in self.nodes:\n",
    "            cost, used = self.nodes[id].cost, self.nodes[id].used\n",
    "        \n",
    "        # if not in the IBG, traverse the IBG to find a covering node\n",
    "        else:\n",
    "            Y = self.find_covering_node(X)              \n",
    "            cost, used = Y.cost, Y.used\n",
    "\n",
    "        return cost, used    \n",
    "\n",
    "\n",
    "    # traverses the IBG to find a node that removes indexes not in X (i.e. a covering node for X)\n",
    "    def find_covering_node(self, X):\n",
    "        X_indexes = set([index.index_id for index in X])\n",
    "        Y = self.root\n",
    "        Y_indexes = set([index.index_id for index in Y.indexes])\n",
    "        # traverse IBG to find covering node\n",
    "        while (len(Y_indexes - X_indexes) != 0) or (len(Y.children) > 0):               \n",
    "            # traverse down to the child node that removes an index not in X\n",
    "            child_found = False\n",
    "            for child in Y.children:\n",
    "                child_indexes = set([index.index_id for index in child.indexes])\n",
    "                child_indexes_removed = Y_indexes - child_indexes\n",
    "                child_indexes_removed_not_in_X = child_indexes_removed - X_indexes\n",
    "        \n",
    "                # check if child removes an index not in X\n",
    "                if len(child_indexes_removed_not_in_X) > 0:\n",
    "                    Y = child\n",
    "                    Y_indexes = child_indexes\n",
    "                    child_found = True\n",
    "                    break\n",
    "\n",
    "            # if no children remove indexes not in X    \n",
    "            if not child_found:\n",
    "                break    \n",
    "    \n",
    "        return Y        \n",
    "\n",
    "    # compute benefit of an index for a given configuration \n",
    "    # input X is a list of index objects and 'a' is a single index object\n",
    "    # X must not contain 'a'\n",
    "    def compute_benefit(self, a, X):\n",
    "        if a in X:\n",
    "            # zero benefit if 'a' is already in X\n",
    "            #raise ValueError(\"Index 'a' is already in X\")\n",
    "            return 0\n",
    "        \n",
    "        # get cost  for X\n",
    "        cost_X = self.get_cost_used(X)[0]\n",
    "        # create a new configuration with index a added to X\n",
    "        X_a = X + [a]\n",
    "        # get cost for X + {a}\n",
    "        cost_X_a = self.get_cost_used(X_a)[0]\n",
    "        # compute benefit\n",
    "        benefit = cost_X - cost_X_a\n",
    "        return benefit \n",
    "\n",
    "\n",
    "    # compute maximum benefit of adding an index to any possibe configuration\n",
    "    def compute_max_benefit(self, a):\n",
    "        max_benefit = float('-inf')\n",
    "        for id, node in self.nodes.items():\n",
    "            #print(f\"Computing benefit for node: {[index.index_id for index in node.indexes]}\")\n",
    "            benefit = self.compute_benefit(a, node.indexes)\n",
    "            if benefit > max_benefit:\n",
    "                max_benefit = benefit\n",
    "\n",
    "        return max_benefit\n",
    "    \n",
    "    # compute the degree of interaction between two indexes a,b in configuration X \n",
    "    def compute_doi_configuration(self, a, b, X, normalize=True):\n",
    "        # X must not contain a or b\n",
    "        if a in X or b in X:\n",
    "            raise ValueError(\"a or b is already in X\")\n",
    "\n",
    "        doi = abs(self.compute_benefit(a, X) - self.compute_benefit(a, X + [b]))\n",
    "        if normalize:\n",
    "            doi /= self.get_cost_used(X + [a,b])[0]   \n",
    "        return doi\n",
    "   \n",
    "    \n",
    "    # Cache the results of find_covering_node and get_cost_used to avoid redundant calculations\n",
    "    @lru_cache(maxsize=None)\n",
    "    def cached_find_covering_node(self, indexes):\n",
    "        return self.find_covering_node(tuple(indexes))\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def cached_get_cost_used(self, indexes):\n",
    "        return self.get_cost_used(tuple(indexes))\n",
    "\n",
    "\n",
    "    # computes the degree of interaction between all pairs of indexes (a,b) in candidate set C\n",
    "    # Note: doi is symmetric, i.e. doi(a,b) = doi(b,a)\n",
    "    def compute_all_pair_doi(self):\n",
    "        # hash table for storing doi values\n",
    "        doi = {}\n",
    "        # intialize doi values to zero\n",
    "        for i in range(len(self.C)):\n",
    "            for j in range(i+1, len(self.C)):\n",
    "                doi[(self.C[i].index_id, self.C[j].index_id)] = 0\n",
    "                doi[(self.C[j].index_id, self.C[i].index_id)] = 0\n",
    "\n",
    "        S_idxs = set([index.index_id for index in self.C])\n",
    "\n",
    "        # iterate over each IBG node\n",
    "        for Y in tqdm(self.nodes.values(), desc=\"Processing nodes\"):\n",
    "            # remove Y.used from S\n",
    "            Y_idxs = set([index.index_id for index in Y.indexes])\n",
    "            used_Y = Y.used\n",
    "            Y_used_idxs = set([index.index_id for index in used_Y])\n",
    "            S_Y = list(S_idxs - Y_used_idxs)\n",
    "            # iterate over all pairs of indexes in S_Y\n",
    "            for i in range(len(S_Y)):\n",
    "                for j in range(i+1, len(S_Y)):\n",
    "                    a_idx = S_Y[i]\n",
    "                    b_idx = S_Y[j]\n",
    "                     \n",
    "                    # find Ya covering node in IBG\n",
    "                    Ya = (Y_idxs - {a_idx, b_idx}) | {a_idx}\n",
    "                    Ya = [self.idx2index[idx] for idx in Ya]\n",
    "                    Ya = self.cached_find_covering_node(tuple(Ya))\n",
    "                    # find Yab covering node in IBG\n",
    "                    Yab = (Y_idxs - {a_idx, b_idx}) | {a_idx, b_idx}\n",
    "                    Yab = [self.idx2index[idx] for idx in Yab]\n",
    "                    Yab = self.cached_find_covering_node(tuple(Yab))\n",
    "\n",
    "                    #used_Y = self.cached_get_cost_used(tuple(Y.indexes))[1]\n",
    "                    #used_Ya = self.cached_get_cost_used(tuple(Ya))[1]\n",
    "                    #used_Yab = self.cached_get_cost_used(tuple(Yab))[1]\n",
    "                    used_Ya = Ya.used\n",
    "                    used_Yab = Yab.used\n",
    "\n",
    "                    Uab = set([index.index_id for index in used_Y]) | set([index.index_id for index in used_Ya]) | set([index.index_id for index in used_Yab]) \n",
    "                    # find Yb_minus covering node in IBG \n",
    "                    Yb_minus = list((Uab - {a_idx, b_idx}) | {b_idx})\n",
    "                    Yb_minus = [self.idx2index[idx] for idx in Yb_minus]\n",
    "                    Yb_minus = self.cached_find_covering_node(tuple(Yb_minus))\n",
    "                    # find Yb_plus covering node in IBG\n",
    "                    Yb_plus = list((Y_idxs - {a_idx, b_idx}) | {b_idx})\n",
    "                    Yb_plus = [self.idx2index[idx] for idx in Yb_plus]\n",
    "                    Yb_plus = self.cached_find_covering_node(tuple(Yb_plus))\n",
    "\n",
    "                    # generate quadruples\n",
    "                    quadruples = [(Y.indexes, Ya.indexes, Yb_minus.indexes, Yab.indexes), (Y.indexes, Ya.indexes, Yb_plus.indexes, Yab.indexes)]\n",
    "\n",
    "                    # compute doi using the quadruples\n",
    "                    for Y_indexes, Ya_indexes, Yb_indexes, Yab_indexes in quadruples:\n",
    "                        cost_Y = self.cached_get_cost_used(tuple(Y_indexes))[0]\n",
    "                        cost_Ya = self.cached_get_cost_used(tuple(Ya_indexes))[0]\n",
    "                        cost_Yb = self.cached_get_cost_used(tuple(Yb_indexes))[0]\n",
    "                        cost_Yab = self.cached_get_cost_used(tuple(Yab_indexes))[0]\n",
    "                        # can ignore the normalization terms in denominator to get an absolute measure of doi\n",
    "                        d = abs(cost_Y - cost_Ya - cost_Yb + cost_Yab) / cost_Yab\n",
    "                        \"\"\"\n",
    "                        if (a_idx, b_idx) in doi:\n",
    "                            doi[(a_idx,b_idx)] = max(doi[(a_idx,b_idx)], d)\n",
    "                        elif (b_idx, a_idx) in doi:\n",
    "                            doi[(b_idx,a_idx)] = max(doi[(b_idx,a_idx)], d)\n",
    "                        else:\n",
    "                            raise ValueError(\"Invalid pair of indexes\")    \n",
    "                        \"\"\"\n",
    "                        doi[(a_idx,b_idx)] = max(doi[(a_idx,b_idx)], d)\n",
    "                        # save doi value for the symmetric pair\n",
    "                        doi[(b_idx,a_idx)] = max(doi[(b_idx,a_idx)], d)     \n",
    "                            \n",
    "        return doi\n",
    "\n",
    "\n",
    "    # get precomputed degree of interaction between a pair of indexes\n",
    "    def get_doi_pair(self, a, b):\n",
    "            return self.doi[(a.index_id, b.index_id)]\n",
    "\n",
    "\n",
    "    # function for printing the IBG, using BFS level order traversal\n",
    "    def print_ibg(self):\n",
    "        q = [self.root]\n",
    "        # traverse level by level, print all node ids in a level in a single line before moving to the next level\n",
    "        while len(q) > 0:\n",
    "            next_q = []\n",
    "            for node in q:\n",
    "                print(f\"{node.id} -> \", end=\"\")\n",
    "                for child in node.children:\n",
    "                    next_q.append(child)\n",
    "            print()\n",
    "            q = next_q  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an SSB query generator object\n",
    "qg = QGEN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "template id: 1, query: \n",
      "                SELECT SUM(lo_extendedprice * lo_discount) AS revenue\n",
      "                FROM lineorder, dwdate\n",
      "                WHERE lo_orderdate = d_datekey\n",
      "                AND d_year = 1994\n",
      "                AND lo_discount BETWEEN 1 AND 3 \n",
      "                AND lo_quantity < 23;\n",
      "            , payload: {'lineorder': ['lo_extendedprice', 'lo_discount']}, predicates: {'lineorder': ['lo_orderdate', 'lo_discount', 'lo_quantity'], 'dwdate': ['d_datekey', 'd_year']}, order by: {}, group by: {}\n",
      "Number of candidate indexes: 19\n",
      "Index id to integer mapping: {'IX_lineorder_lo_orderdate': 0, 'IX_lineorder_lo_discount': 1, 'IX_lineorder_lo_quantity': 2, 'IX_lineorder_lo_orderdate_lo_discount': 3, 'IX_lineorder_lo_orderdate_lo_quantity': 4, 'IX_lineorder_lo_discount_lo_orderdate': 5, 'IX_lineorder_lo_discount_lo_quantity': 6, 'IX_lineorder_lo_quantity_lo_orderdate': 7, 'IX_lineorder_lo_quantity_lo_discount': 8, 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity': 9, 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount': 10, 'IX_lineorder_lo_discount_lo_orderdate_lo_quantity': 11, 'IX_lineorder_lo_discount_lo_quantity_lo_orderdate': 12, 'IX_lineorder_lo_quantity_lo_orderdate_lo_discount': 13, 'IX_lineorder_lo_quantity_lo_discount_lo_orderdate': 14, 'IX_dwdate_d_datekey': 15, 'IX_dwdate_d_year': 16, 'IX_dwdate_d_datekey_d_year': 17, 'IX_dwdate_d_year_d_datekey': 18}\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of nodes in IBG: 4, Total number of what-if calls: 4, Time spent on what-if calls: 0.09815192222595215\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes: 100%|██████████| 4/4 [00:00<00:00, 828.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18 -> \n",
      "0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17 -> \n",
      "0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_17 -> \n",
      "0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15 -> \n",
      "IBG     --> Cost: 1431969.81, Used indexes: ['IX_dwdate_d_year']\n",
      "What-if --> Cost: 1431969.81, Used indexes: ['IX_dwdate_d_year']\n",
      "\n",
      "Maximum benefit of adding index IX_lineorder_lo_orderdate: 0\n",
      "\n",
      "DOI between indexes IX_lineorder_lo_orderdate and IX_lineorder_lo_orderdate_lo_quantity : 0.0\n",
      "in configuration ['IX_lineorder_lo_discount', 'IX_lineorder_lo_quantity', 'IX_lineorder_lo_discount_lo_orderdate', 'IX_lineorder_lo_discount_lo_quantity', 'IX_lineorder_lo_quantity_lo_discount']\n",
      "\n",
      "DOI between indexes IX_lineorder_lo_orderdate and IX_lineorder_lo_orderdate_lo_quantity : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test IBG \n",
    "\n",
    "query = qg.generate_query(1)\n",
    "print(query)\n",
    "\n",
    "C = extract_query_indexes(qg.generate_query(1), include_cols=False)  \n",
    "\n",
    "ibg = IBG(query, C)\n",
    "\n",
    "ibg.print_ibg()\n",
    "\n",
    "# pick random subset of candidate indexes\n",
    "X = random.sample(ibg.C, 8)\n",
    "cost, used = ibg.get_cost_used(X)\n",
    "print(f\"IBG     --> Cost: {cost}, Used indexes: {[idx.index_id for idx in used]}\")\n",
    "\n",
    "cost, used = ibg._cached_get_cost_used(X)\n",
    "print(f\"What-if --> Cost: {cost}, Used indexes: {[idx.index_id for idx in used]}\")\n",
    "\n",
    "# pick two indexes and a configuration\n",
    "a = ibg.C[0]\n",
    "b = ibg.C[4] \n",
    "X = [ibg.C[1], ibg.C[2], ibg.C[5], ibg.C[6], ibg.C[8]]\n",
    "\n",
    "# compute maximum benefit of adding index 'a' \n",
    "max_benefit = ibg.compute_max_benefit(a)\n",
    "print(f\"\\nMaximum benefit of adding index {a.index_id}: {max_benefit}\")\n",
    "\n",
    "# compute degree of interaction between indexes 'a' and 'b' in configuration X\n",
    "doi = ibg.compute_doi_configuration(a, b, X)\n",
    "print(f\"\\nDOI between indexes {a.index_id} and {b.index_id} : {doi}\")\n",
    "print(f\"in configuration {[idx.index_id for idx in X]}\")\n",
    "\n",
    "# compute configuration independent degree of interaction between indexes 'a' and 'b'\n",
    "doi = ibg.get_doi_pair(a, b)\n",
    "print(f\"\\nDOI between indexes {a.index_id} and {b.index_id} : {doi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for key, value in ibg.doi.items():\n",
    "#    print(f\"doi({key[0]},   {key[1]}) = {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WFIT class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WFIT:\n",
    "\n",
    "    def __init__(self, S_0=[], max_key_columns=None, max_U=150, idxCnt=50, stateCnt=200, histSize=100, rand_cnt=1):\n",
    "        # initial set of materialzed indexes\n",
    "        self.S_0 = S_0\n",
    "        # maximum number of key columns in an index\n",
    "        self.max_key_columns = max_key_columns\n",
    "        # maximum number of candidate indexes for IBG \n",
    "        self.max_U = max_U\n",
    "        # parameter for maximum number of candidate indexes tracked \n",
    "        self.idxCnt = idxCnt\n",
    "        # parameter for maximum number of MTS states/configurations\n",
    "        self.stateCnt = stateCnt\n",
    "        # parameter for maximum number of historical index statistics kept\n",
    "        self.histSize = histSize\n",
    "        # parameter for number of randomized clustering iterations\n",
    "        self.rand_cnt = rand_cnt\n",
    "        # growing list of candidate indexes (initially contains S_0)\n",
    "        self.U = {index.index_id:index for index in S_0}\n",
    "        # index benefit and interaction statistics\n",
    "        self.idxStats = defaultdict(list)\n",
    "        self.intStats = defaultdict(list)\n",
    "        # list of currently monitored indexes\n",
    "        self.C = {index.index_id:index for index in S_0} \n",
    "        # list of currently materialized indexes\n",
    "        self.M = {index.index_id:index for index in S_0}  \n",
    "        # initialize stable partitions (each partition is a singleton set of indexes from S_0)\n",
    "        self.stable_partitions = [[index] for index in S_0]\n",
    "        self.n_pos = 0\n",
    "\n",
    "        print(f\"##################################################################\")\n",
    "        # initialize work function instance for each stable partition\n",
    "        self.W = self.initilize_WFA(self.stable_partitions)\n",
    "        # initialize current recommendations for each stable partition\n",
    "        self.current_recommendations = {i:indexes for i, indexes in enumerate(self.stable_partitions)}\n",
    "\n",
    "\n",
    "        print(f\"Initial set of materialized indexes: {[index.index_id for index in S_0]}\")\n",
    "        print(f\"Stable partitions: {[[index.index_id for index in P] for P in self.stable_partitions]}\")\n",
    "        print(f\"Initial work function instances: \")\n",
    "        for i, wf in self.W.items():\n",
    "            print(f\"\\tWFA Instance #{i}: {wf}\")\n",
    "\n",
    "        print(f\"\\nMaximum number of candidate indexes tracked: {idxCnt}\")\n",
    "        print(f\"Maximum number of MTS states/configurations: {stateCnt}\")\n",
    "        print(f\"Maximum number of historical index statistics kept: {histSize}\")\n",
    "        print(f\"Number of randomized clustering iterations: {rand_cnt}\")\n",
    "        print(f\"##################################################################\\n\")\n",
    "\n",
    "        # set random seed\n",
    "        random.seed(1234)\n",
    "\n",
    "\n",
    "    # initialize a WFA instance for each stable partition\n",
    "    def initilize_WFA(self, stable_partitions):\n",
    "        print(f\"Initializing WFA instances for {len(stable_partitions)} stable partitions...\")\n",
    "        W = {}\n",
    "        for i, P in enumerate(stable_partitions):\n",
    "            # initialize all MTS states, i.e. power set of indexes in the partition\n",
    "            states = [tuple(sorted(state, key=lambda x: x.index_id)) for state in powerset(P)]\n",
    "            # initialize work function instance for the partition\n",
    "            W[i] = {tuple(X):self.compute_transition_cost(self.S_0, X) for X in states}    \n",
    "            \n",
    "        return W\n",
    "\n",
    "\n",
    "    # update WFIT step for next query in workload (this is the MAIN INTERFACE for generating an index configuration recommendation)\n",
    "    def process_WFIT(self, query_object, verbose=False):\n",
    "        self.n_pos += 1\n",
    "        \n",
    "        # generate new partitions \n",
    "        if verbose: print(f\"Generating new partitions for query #{self.n_pos}\")\n",
    "        start_time_1 = time.time()\n",
    "        new_partitions, need_to_repartition, ibg = self.choose_candidates(self.n_pos, query_object, verbose)\n",
    "        end_time_1 = time.time()\n",
    "\n",
    "        # repartition if necessary\n",
    "        start_time_2 = time.time()\n",
    "        if need_to_repartition:\n",
    "            if verbose: print(f\"Repartitioning...\")\n",
    "            self.repartition(new_partitions, verbose)\n",
    "        end_time_2 = time.time()\n",
    "        \n",
    "        # analyze the query\n",
    "        if verbose: print(f\"Analyzing query...\")\n",
    "        start_time_3 = time.time()\n",
    "        self.analyze_query(query_object, ibg, verbose)\n",
    "        end_time_3 = time.time()    \n",
    "\n",
    "        if verbose: print(f\"Currently materialized indexes: {[index.index_id for index in self.M.values()]}\") \n",
    "\n",
    "        # remove stale indexes from U\n",
    "        if verbose: print(f\"Removing stale indexes from U...\")\n",
    "        self.remove_stale_indexes_U(verbose)\n",
    "\n",
    "        # simple recommendation, just the used indexes in the IBG root node\n",
    "        self.get_simple_recommendation_ibg(ibg)\n",
    "\n",
    "\n",
    "        print(f\"Total time taken for processing query #{self.n_pos}: {end_time_3 - start_time_1} seconds\")\n",
    "        print(f\"(Partitioning: {end_time_1 - start_time_1} seconds, Repartitioning: {end_time_2 - start_time_2} seconds, Analyzing: {end_time_3 - start_time_3} seconds)\")\n",
    "\n",
    "\n",
    "    # Simple baseline recommendation: just the used indexes in the IBG root node, i.e. these are the indexes from \n",
    "    # the full set of candidate indexes which are used in the query plan\n",
    "    def get_simple_recommendation_ibg(self, ibg):\n",
    "        simple_recommendation = ibg.root.used  \n",
    "        wfit_recommendation = [index.index_id for i in self.current_recommendations for index in self.current_recommendations[i]]\n",
    "        print(f\"*** WFIT recommendation: {sorted(wfit_recommendation)}\")\n",
    "        print(f\"*** Simple recommendation: {sorted([index.index_id for index in simple_recommendation])}\") \n",
    "\n",
    "\n",
    "    # check for stale indexes in U and remove them\n",
    "    def remove_stale_indexes_U(self, verbose, min_overlapping_columns=0):\n",
    "        # find out which indexes have loweest benefit statistics\n",
    "        avg_benefit = {}\n",
    "        for index_id in self.U:\n",
    "            # compute average benefit of the index from all stats\n",
    "            avg_benefit[index_id] = sum([stat[1] for stat in self.idxStats[index_id]]) / len(self.idxStats[index_id])\n",
    "\n",
    "            \n",
    "        # sort indexes by average benefit\n",
    "        sorted_indexes = sorted(avg_benefit, key=avg_benefit.get, reverse=True)\n",
    "\n",
    "        # mark all indexes with zero benefit and not in M and S_0 as stale\n",
    "        stale_indexes = set()\n",
    "        for index_id in sorted_indexes:\n",
    "            if avg_benefit[index_id] == 0 and index_id not in self.M and index_id not in self.S_0:\n",
    "                stale_indexes.add(index_id)\n",
    "\n",
    "        # remove stale indexes from U\n",
    "        print(f\"Number of indexes in U: {len(self.U)}\")\n",
    "        num_removed = 0\n",
    "        for index_id in stale_indexes:\n",
    "            #if verbose: print(f\"Removing stale index: {index_id}\")\n",
    "            del self.U[index_id]\n",
    "            #if verbose: print(f\"Number of indexes in U after removal: {len(self.U)}\")\n",
    "            num_removed += 1\n",
    "\n",
    "        if verbose:\n",
    "            #print(f\"Average benefit of indexes:\")\n",
    "            #for index_id in sorted_indexes:\n",
    "            #    print(f\"\\tIndex {index_id}: {avg_benefit[index_id]}, Stale: {index_id in stale_indexes}\")\n",
    "                \n",
    "            print(f\"Number of indexes removed: {num_removed}, Number of indexes remaining: {len(self.U)}\")\n",
    "            #print(f\"Indexes in U: {self.U.keys()}\")\n",
    "                \n",
    "\n",
    "    # repartition the stable partitions based on the new partitions\n",
    "    def repartition(self, new_partitions, verbose):\n",
    "        # all indexes recommmendations across the WFA instances from previous round\n",
    "        S_curr = set(chain(*self.current_recommendations.values()))\n",
    "        C = set(self.C.values()) \n",
    "        S_0 = set(self.S_0)\n",
    "\n",
    "        # re-initizlize WFA instances and recommendations for each new partition\n",
    "        if verbose: print(f\"Reinitializing WFA instances...\")\n",
    "        W = {}\n",
    "        recommendations = {}\n",
    "        for i, P in enumerate(new_partitions):\n",
    "            partition_all_configs = [tuple(sorted(state, key=lambda x: x.index_id)) for state in powerset(P)]\n",
    "            wf = {}\n",
    "            # initialize work function values for each state\n",
    "            for X in partition_all_configs:\n",
    "                wf_x = 0\n",
    "                for j, wf_prev in self.W.items(): \n",
    "                    wf_x += wf_prev[tuple(sorted(set(X) & set(self.stable_partitions[j]), key=lambda x: x.index_id))]\n",
    "                \n",
    "                # add transition cost to the work function value (not sure if intersection with S_0 is correct or not..)\n",
    "                wf[X] = wf_x + self.compute_transition_cost((S_0 & set(P)) - C, set(X) - C)\n",
    "            \n",
    "            W[i] = wf\n",
    "            # initialize current state/recommended configuration of the WFA instance\n",
    "            recommendations[i] = list(set(P) & S_curr)\n",
    "\n",
    "        # replace current stable partitions, WFA instances and recommendations with the new ones\n",
    "        self.stable_partitions = new_partitions\n",
    "        self.W = W\n",
    "        self.current_recommendations = recommendations\n",
    "        if verbose: \n",
    "            print(f\"Replaced stable partitions, WFA instances and recommendations with new ones\")\n",
    "            #print(f\"New WFA instances:\")\n",
    "            #for i, wf in self.W.items():\n",
    "            #    print(f\"\\tWFA Instance #{i}: {wf}\")\n",
    "\n",
    "        self.C = {}\n",
    "        for P in self.stable_partitions:\n",
    "            for index in P: \n",
    "                self.C[index.index_id] = index      \n",
    "\n",
    "\n",
    "    # update WFA instance on each stable partition and get index configuration recommendation\n",
    "    def analyze_query(self, query_object, ibg, verbose):\n",
    "        new_recommendations = {}\n",
    "        # update WFA instance for each stable partition\n",
    "        for i in self.W:\n",
    "            if verbose: print(f\"Updating WFA instance: {i}\")\n",
    "            self.W[i], new_recommendations[i]  = self.process_WFA(query_object, self.W[i], self.current_recommendations[i], ibg, verbose)\n",
    "\n",
    "            # materialize new recommendation\n",
    "            indexes_added = set(new_recommendations[i]) - set(self.current_recommendations[i])\n",
    "            indexes_removed = set(self.current_recommendations[i]) - set(new_recommendations[i])\n",
    "            if verbose: print(f\"\\tWFA Instance #{i}, Num States: {len(self.W[i])}, New Recommendation: {[index.index_id for index in new_recommendations[i]]} --> Indexes Added: {[index.index_id for index in indexes_added]}, Indexes Removed: {[index.index_id for index in indexes_removed]}\")\n",
    "            \n",
    "            for index in indexes_added:\n",
    "                self.M[index.index_id] = index\n",
    "            for index in indexes_removed:\n",
    "                del self.M[index.index_id]    \n",
    "                \n",
    "            self.current_recommendations[i] = new_recommendations[i]\n",
    "\n",
    "            # TODO: need to implement the following function in pg_utils\n",
    "            # ... materialize_configuration(connection, indexes_added, indexes_removed)\n",
    "            \n",
    "\n",
    "\n",
    "    # update a WFA instance for the given query    \n",
    "    def process_WFA(self, query_object, wf, S_current, ibg, verbose):\n",
    "        # update work function values for each state in the WFA instance\n",
    "        wf_new = {}\n",
    "        p = {}\n",
    "        scores = {}\n",
    "        best_score = float('inf')\n",
    "        best_state = None\n",
    "        for Y in wf.keys():\n",
    "            # compute new work function value for state Y \n",
    "            min_wf_value = float('inf')\n",
    "            min_p = None\n",
    "            for X in wf.keys():\n",
    "                sorted_X = tuple(sorted(X, key=lambda x: x.index_id))\n",
    "                sorted_Y = tuple(sorted(Y, key=lambda x: x.index_id))\n",
    "                wf_value = wf[sorted_X] + ibg.get_cost_used(list(sorted_X))[0] + self.compute_transition_cost(sorted_X, sorted_Y)\n",
    "                if wf_value < min_wf_value:\n",
    "                    min_wf_value = wf_value\n",
    "                    min_p = sorted_X\n",
    "\n",
    "            wf_new[sorted_Y] = min_wf_value\n",
    "            p[sorted_Y] = min_p\n",
    "\n",
    "            # compute score for the state    \n",
    "            score = wf_new[sorted_Y] + self.compute_transition_cost(sorted_Y, S_current)\n",
    "            scores[sorted_Y] = score\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_state = p[sorted_Y]\n",
    "\n",
    "        if verbose:\n",
    "            #print(f\"Work function values for WFA instance:\")\n",
    "            #for Y, value in wf_new.items():\n",
    "            #    print(f\"\\tstate :{Y} , w_value: {value}, p: {p[Y]}, score: {scores[Y]}\")\n",
    "            print(f\"Best state: {best_state}, Best score: {best_score}\")\n",
    "        \n",
    "\n",
    "        return wf_new, best_state\n",
    "\n",
    "    # compute index benefit graph for the given query and candidate indexes\n",
    "    def compute_IBG(self, query_object, candidate_indexes):\n",
    "        return IBG(query_object, candidate_indexes)\n",
    "    \n",
    "\n",
    "    # extract candidate indexes from given query\n",
    "    def extract_indexes(self, query_object, include_cols=False):\n",
    "        return extract_query_indexes(query_object,  self.max_key_columns, include_cols)\n",
    "\n",
    "\n",
    "    # generate stable partitions/sets of indexes for next query in workload\n",
    "    def choose_candidates(self, n_pos, query_object, verbose):\n",
    "        # extract new candidate indexes from the query\n",
    "        new_indexes = self.extract_indexes(query_object)\n",
    "        # add new indexes to the list of all candidate indexes\n",
    "        num_new = 0\n",
    "        for index in new_indexes:\n",
    "            if index.index_id not in self.U:\n",
    "                self.U[index.index_id] = index\n",
    "                num_new += 1\n",
    "\n",
    "        if len(self.U) > self.max_U:\n",
    "            raise ValueError(\"Number of candidate indexes exceeds the maximum limit. Aborting WFIT...\")\n",
    "\n",
    "\n",
    "        if verbose: \n",
    "            print(f\"Extracted {num_new} new indexes from query.\")\n",
    "            print(f\"Candidate indexes (including those currently materialized), |U| = {len(self.U)}\")\n",
    "            print(f\"{[index.index_id for index in self.U.values()]}\")\n",
    "\n",
    "        # TODO: need mechanism to evict indexes from U that may have gone \"stale\" to prevent unbounded growth of U\n",
    "\n",
    "        \n",
    "        # compute index benefit graph for the query\n",
    "        if verbose: print(f\"Computing IBG...\")\n",
    "        ibg = self.compute_IBG(query_object, list(self.U.values()))\n",
    "        \n",
    "        # update statistics for the candidate indexes (n_pos is the position of the query in the workload sequence)\n",
    "        if verbose: print(f\"Updating statistics...\")\n",
    "        self.update_stats(n_pos, ibg, verbose=False)\n",
    "\n",
    "        # non-materialized candidate indexes \n",
    "        X = [self.U[index_id] for index_id in self.U if index_id not in self.M]\n",
    "        num_indexes = self.idxCnt - len(self.M)\n",
    "\n",
    "        # determine new set of candidate indexes to monitor for upcoming workload queries\n",
    "        if verbose: print(f\"Choosing top {num_indexes} indexes from {len(X)} non-materialized candidate indexes\")\n",
    "        top_indexes = self.top_indexes(n_pos, X, num_indexes, verbose)\n",
    "        D = self.M | top_indexes\n",
    "        if verbose: print(f\"New set of indexes to monitor for upcoming workload, |D| = {len(D)}\")\n",
    "\n",
    "        # generate new partitions by clustering the new candidate set\n",
    "        if verbose: print(f\"Choosing new partitions...\")\n",
    "        new_partitions, need_to_repartition = self.choose_partition(n_pos, D, verbose)\n",
    "        if verbose:\n",
    "            print(f\"Old partitions:\")\n",
    "            for P in self.stable_partitions:\n",
    "                print(f\"\\t{[index.index_id for index in P]}\")\n",
    "            print(\"New partitions:\")\n",
    "            for P in new_partitions:\n",
    "                print(f\"\\t{[index.index_id for index in P]}\")    \n",
    "\n",
    "        return new_partitions, need_to_repartition, ibg\n",
    "    \n",
    "\n",
    "    # partition the new candidate set into clusters \n",
    "    # (need to optimize this function, currently it is a naive implementation)\n",
    "    def choose_partition(self, N_workload, D, verbose):\n",
    "        \n",
    "        # compute total loss, i.e. sum of doi across indexes from pairs of partitions\n",
    "        def compute_loss(P, current_doi):\n",
    "            loss = 0\n",
    "            for i in range(len(P)):\n",
    "                for j in range(i+1, len(P)):\n",
    "                    for a in P[i]:\n",
    "                        for b in P[j]:\n",
    "                            loss += current_doi[(a.index_id, b.index_id)]\n",
    "            return loss\n",
    "        \n",
    "        # compute current doi values for all pairs of indexes in U\n",
    "        current_doi = defaultdict(int)\n",
    "        for (a_idx, b_idx) in self.intStats.keys():\n",
    "            # take max over incremental averages (optimistic estimate)\n",
    "            current_doi[(a_idx, b_idx)] = 0\n",
    "            doi_total = 0\n",
    "            for (n, doi) in self.intStats[(a_idx, b_idx)]:\n",
    "                doi_total += doi\n",
    "                doi_avg = doi_total / (N_workload-n+1)\n",
    "                current_doi[(a_idx, b_idx)] = max(current_doi[(a_idx, b_idx)], doi_avg)\n",
    "            # save symmetric doi value\n",
    "            current_doi[(b_idx, a_idx)] = current_doi[(a_idx, b_idx)]    \n",
    "\n",
    "        #if verbose:\n",
    "        #    print(\"Current degree of interaction:\")\n",
    "        #    for pair, doi in current_doi.items():\n",
    "        #        print(f\"\\tPair {pair}: {doi}\")     \n",
    "\n",
    "        # from each current stable partition, remove indexes not in D\n",
    "        P = []\n",
    "        for partition in self.stable_partitions:\n",
    "            P.append([index for index in partition if index.index_id in D])\n",
    "\n",
    "        # add a singleton partition containing each new index in D not in C\n",
    "        need_to_repartition = False\n",
    "        for index_id, index in D.items():\n",
    "            if index_id not in self.C:\n",
    "                P.append([index])\n",
    "                need_to_repartition = True\n",
    "        \n",
    "        # set the new partition as baseline solution if feasible\n",
    "        total_configurations = sum([2**len(partition) for partition in P])\n",
    "        if total_configurations <= self.stateCnt:\n",
    "            bestSolution = P\n",
    "            bestLoss = compute_loss(P, current_doi)\n",
    "        else:\n",
    "            bestSolution = None\n",
    "            bestLoss = float('inf')    \n",
    "\n",
    "        # perform randomized clustering to find better solution\n",
    "        for i in range(self.rand_cnt):\n",
    "            # create partition of D in singletons\n",
    "            P = [[index] for index in D.values()]\n",
    "            partition2id = {tuple(partition):i for i, partition in enumerate(P)}\n",
    "            loss_cache = {}\n",
    "            \n",
    "            #if verbose:\n",
    "            #    print(f\"Parition to id map: {partition2id}\")\n",
    "\n",
    "            # merge singletons until only one partition remains\n",
    "            while True:\n",
    "                # find all feasible merge candidates pairs (i.e. pairs with loss > 0 and 2^(|Pi|+|Pj|) <= stateCnt)\n",
    "                E = []\n",
    "                E1 = []\n",
    "\n",
    "                # get loss for all pairs of partitions\n",
    "                total_configurations = sum([2**len(partition) for partition in P])\n",
    "                for i in range(len(P)):\n",
    "                    for j in range(i+1, len(P)):\n",
    "                        Pi_id = partition2id[tuple(P[i])]\n",
    "                        Pj_id = partition2id[tuple(P[j])]\n",
    "                        if (Pi_id, Pj_id) in loss_cache:\n",
    "                            loss = loss_cache[(Pi_id, Pj_id)]\n",
    "                        else:\n",
    "                            loss = compute_loss([P[i], P[j]], current_doi)\n",
    "                            loss_cache[(Pi_id, Pj_id)] = loss\n",
    "\n",
    "                        # only include feasible merge pairs, i.e. a pair which can be merged without the total number of configs exceeding stateCnt\n",
    "                        total_configrations_after_merge = total_configurations - 2**len(P[i]) - 2**len(P[j]) + 2**(len(P[i]) + len(P[j]))\n",
    "                        if loss > 0 and total_configrations_after_merge <= self.stateCnt:\n",
    "                            E.append((P[i], P[j], loss))    \n",
    "                            if len(P[i]) == 1 and len(P[j]) == 1:\n",
    "                                E1.append((P[i],P[j], loss))\n",
    "\n",
    "                #if verbose:    \n",
    "                    #print(f\"E pairs: {[[(index.index_id for index in Pi), (index.index_id for index in Pj), loss] for (Pi, Pj, loss) in E]}\")\n",
    "                    #print(f\"E1 pairs: {[[(index.index_id for index in Pi), (index.index_id for index in Pj), loss] for (Pi, Pj, loss) in E1]}\")\n",
    "\n",
    "                if len(E) == 0:\n",
    "                    break\n",
    "                \n",
    "                elif len(E1) > 0:\n",
    "                    # merge a random pair of singletons, sample randomly from E1 weighted by loss (i.e. high loss pairs more likely to be merged)\n",
    "                    Pi, Pj, loss = random.choices(E1, weights=[loss for (Pi, Pj, loss) in E1], k=1)[0]\n",
    "                    Pij_merged = Pi + Pj\n",
    "                    P.remove(Pi)\n",
    "                    P.remove(Pj)\n",
    "                    P.append(Pij_merged) \n",
    "                    E1.remove((Pi, Pj, loss))  \n",
    "                    partition2id[tuple(Pij_merged)] = len(partition2id) \n",
    "                    #if verbose: \n",
    "                    #    print(f\"Merged singleton partitions {[index.index_id for index in Pi]} and {[index.index_id for index in Pj]} with loss {loss}\")\n",
    "\n",
    "                else:\n",
    "                    # merge a random pair of partitions, sample randomly from E weighted by normalized loss  \n",
    "                    Pi, Pj, loss = random.choices(E, weights=[loss / (2**(len(Pi) + len(Pj)) - 2**len(Pi) - 2**len(Pj)) for (Pi, Pj, loss) in E], k=1)[0]\n",
    "                    Pij_merged = Pi + Pj\n",
    "                    P.remove(Pi)\n",
    "                    P.remove(Pj)\n",
    "                    P.append(Pij_merged)   \n",
    "                    E.remove((Pi, Pj, loss)) \n",
    "                    partition2id[tuple(Pij_merged)] = len(partition2id) \n",
    "                    #if verbose:\n",
    "                    #    print(f\"Merged partitions {[index.index_id for index in Pi]} and {[index.index_id for index in Pj]} with loss {loss}\")    \n",
    "\n",
    "            # check if the new solution is better than the current best solution\n",
    "            loss = compute_loss(P, current_doi)\n",
    "            if loss < bestLoss:\n",
    "                bestSolution = P\n",
    "                bestLoss = loss\n",
    "\n",
    "        return bestSolution, need_to_repartition\n",
    "\n",
    "\n",
    "    # update candidate index statistics\n",
    "    def update_stats(self, n, ibg, verbose):\n",
    "        # update index benefit statistics\n",
    "        if verbose: print(\"Updating index benefit statistics...\")\n",
    "        for index in self.U.values():\n",
    "            max_benefit = ibg.compute_max_benefit(index)\n",
    "            #if verbose: print(f\"\\tibg max benefit for index {index.index_id}: {max_benefit}\")\n",
    "            self.idxStats[index.index_id].append((n, max_benefit))\n",
    "            #if verbose: print(f\"\\tIndex {index.index_id}: {self.idxStats[index.index_id]}\")\n",
    "            # evict old stats if the size exceeds histSize\n",
    "            self.idxStats[index.index_id] = self.idxStats[index.index_id][-self.histSize:]\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Index benefit statistics:\")\n",
    "            for index_id, stats in self.idxStats.items():\n",
    "                print(f\"\\tIndex {index_id}: {stats}\")\n",
    "\n",
    "\n",
    "        # update index interaction statistics\n",
    "        if verbose: print(\"Updating index interaction statistics...\")\n",
    "        for (a_idx, b_idx) in ibg.doi.keys():\n",
    "            d = ibg.doi[(a_idx, b_idx)]\n",
    "            #if verbose: print(f\"\\tibg doi for pair ({a_idx}, {b_idx}) : {d}\")\n",
    "            if d > 0:\n",
    "                self.intStats[(a_idx, b_idx)].append((n, d))\n",
    "            #if verbose: print(f\"\\tPair ({a_idx}, {b_idx}): {self.intStats[(a_idx, b_idx)]}\")\n",
    "            # evict old stats if the size exceeds histSize\n",
    "            self.intStats[(a_idx, b_idx)] = self.intStats[(a_idx, b_idx)][-self.histSize:]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Index interaction statistics:\")\n",
    "            for pair, stats in self.intStats.items():\n",
    "                print(f\"\\tPair {pair}: {stats}\")\n",
    "\n",
    "\n",
    "    # choose top num_indexes indexes from X with highest potential benefit\n",
    "    def top_indexes(self, N_workload, X, num_indexes, verbose, positive_scores_only=False):\n",
    "        if verbose:\n",
    "            print(f\"Non-materialized candidate indexes, X = {[index.index_id for index in X]}\")\n",
    "\n",
    "        # compute \"current benefit\" of each index in X (these are derived from statistics of observed benefits from recent queries)\n",
    "        score = {}\n",
    "        for index in X:\n",
    "            if len(self.idxStats[index.index_id]) == 0:\n",
    "                # zero current benefit if no statistics are available\n",
    "                current_benefit = 0\n",
    "            else:\n",
    "                # take the maximum over all incremental average benefits (optimistic estimate)\n",
    "                current_benefit = 0\n",
    "                b_total = 0\n",
    "                for (n, b) in self.idxStats[index.index_id]:\n",
    "                    b_total += b \n",
    "                    # incremental average benefit of index up to query n (higher weight/smaller denominator for more recent queries)\n",
    "                    benefit = b_total / (N_workload - n + 1)\n",
    "                    current_benefit = max(current_benefit, benefit)\n",
    "\n",
    "            # use current benefit to compute a score for the index\n",
    "            if index.index_id in self.C:\n",
    "                # if index already being monitored, then score is just current benefit\n",
    "                score[index.index_id] = current_benefit\n",
    "            else:\n",
    "                # if index not being monitored, then score is current benefit minus cost of creating the index\n",
    "                # (unmonitored indexes are penalized so that they are only chosen if they have high potential benefit, which helps keep C stable)\n",
    "                score[index.index_id] = current_benefit - self.get_index_creation_cost(index)\n",
    "\n",
    "        #if verbose:\n",
    "        #    print(\"Index scores:\")\n",
    "        #    for index_id, s in score.items():\n",
    "        #        print(f\"Index {index_id}: {s}\")\n",
    "\n",
    "        # get the top num_indexes indexes with highest scores (keep non-zero scores only)\n",
    "        if positive_scores_only:\n",
    "            top_indexes = [index_id for index_id, s in score.items() if s > 0]\n",
    "        else:\n",
    "            top_indexes = [index_id for index_id, s in score.items()]    \n",
    "        top_indexes = sorted(top_indexes, key=lambda x: score[x], reverse=True)[:num_indexes]\n",
    "        top_indexes = {index_id: self.U[index_id] for index_id in top_indexes}\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{len(top_indexes)} top indexes: {[index.index_id for index in top_indexes.values()]}\")\n",
    "\n",
    "        return top_indexes    \n",
    "\n",
    "\n",
    "    # TODO: return index creation cost\n",
    "    def get_index_creation_cost(self, index):\n",
    "        # use a default constant for now\n",
    "        return 0#1e6\n",
    "\n",
    "\n",
    "    # compute transition cost between two MTS states/configurations\n",
    "    def compute_transition_cost(self, S_old, S_new):\n",
    "        # find out which indexes are added\n",
    "        added_indexes = set(S_new) - set(S_old)\n",
    "        # compute cost of creating the added indexes\n",
    "        transition_cost = sum([self.get_index_creation_cost(index) for index in added_indexes])\n",
    "        return transition_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test WFIT implementation on sample SSB workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate an SSB workload\n",
    "workload = [qg.generate_query(i) for i in range(1, 10)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "template id: 7, query: \n",
      "                SELECT c_nation, s_nation, d_year, SUM(lo_revenue) AS revenue\n",
      "                FROM customer, lineorder, supplier, dwdate\n",
      "                WHERE lo_custkey = c_custkey\n",
      "                AND lo_suppkey = s_suppkey\n",
      "                AND lo_orderdate = d_datekey\n",
      "                AND c_region = 'MIDDLE EAST '\n",
      "                AND s_region = 'MIDDLE EAST '\n",
      "                AND d_year >= 1997 AND d_year <= 1997\n",
      "                GROUP BY c_nation, s_nation, d_year\n",
      "                ORDER BY d_year ASC, revenue DESC;\n",
      "            , payload: {'lineorder': ['lo_revenue'], 'dwdate': ['d_year'], 'customer': ['c_nation'], 'supplier': ['s_nation']}, predicates: {'lineorder': ['lo_custkey', 'lo_suppkey', 'lo_orderdate'], 'dwdate': ['d_year', 'd_datekey'], 'customer': ['c_custkey', 'c_region', 'c_nation'], 'supplier': ['s_suppkey', 's_region', 's_nation']}, order by: {'lineorder': ['lo_revenue'], 'dwdate': ['d_year']}, group by: {'customer': ['c_nation'], 'supplier': ['s_nation'], 'dwdate': ['d_year']}\n"
     ]
    }
   ],
   "source": [
    "print(workload[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n",
      "Initializing WFA instances for 1 stable partitions...\n",
      "Initial set of materialized indexes: ['IX_lineorder_lo_custkey']\n",
      "Stable partitions: [['IX_lineorder_lo_custkey']]\n",
      "Initial work function instances: \n",
      "\tWFA Instance #0: {(): 0, (<pg_utils.Index object at 0x7f4c682d2050>,): 0}\n",
      "\n",
      "Maximum number of candidate indexes tracked: 50\n",
      "Maximum number of MTS states/configurations: 300\n",
      "Maximum number of historical index statistics kept: 100\n",
      "Number of randomized clustering iterations: 50\n",
      "##################################################################\n",
      "\n",
      "Processing query 1\n",
      "Generating new partitions for query #1\n",
      "Extracted 19 new indexes from query.\n",
      "Candidate indexes (including those currently materialized), |U| = 20\n",
      "['IX_lineorder_lo_custkey', 'IX_lineorder_lo_orderdate', 'IX_lineorder_lo_discount', 'IX_lineorder_lo_quantity', 'IX_lineorder_lo_orderdate_lo_discount', 'IX_lineorder_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_orderdate', 'IX_lineorder_lo_discount_lo_quantity', 'IX_lineorder_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_discount', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_lineorder_lo_discount_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_orderdate_lo_discount', 'IX_lineorder_lo_quantity_lo_discount_lo_orderdate', 'IX_dwdate_d_datekey', 'IX_dwdate_d_year', 'IX_dwdate_d_datekey_d_year', 'IX_dwdate_d_year_d_datekey']\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 20\n",
      "Index id to integer mapping: {'IX_lineorder_lo_custkey': 0, 'IX_lineorder_lo_orderdate': 1, 'IX_lineorder_lo_discount': 2, 'IX_lineorder_lo_quantity': 3, 'IX_lineorder_lo_orderdate_lo_discount': 4, 'IX_lineorder_lo_orderdate_lo_quantity': 5, 'IX_lineorder_lo_discount_lo_orderdate': 6, 'IX_lineorder_lo_discount_lo_quantity': 7, 'IX_lineorder_lo_quantity_lo_orderdate': 8, 'IX_lineorder_lo_quantity_lo_discount': 9, 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity': 10, 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount': 11, 'IX_lineorder_lo_discount_lo_orderdate_lo_quantity': 12, 'IX_lineorder_lo_discount_lo_quantity_lo_orderdate': 13, 'IX_lineorder_lo_quantity_lo_orderdate_lo_discount': 14, 'IX_lineorder_lo_quantity_lo_discount_lo_orderdate': 15, 'IX_dwdate_d_datekey': 16, 'IX_dwdate_d_year': 17, 'IX_dwdate_d_datekey_d_year': 18, 'IX_dwdate_d_year_d_datekey': 19}\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of nodes in IBG: 4, Total number of what-if calls: 4, Time spent on what-if calls: 0.09394431114196777\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes: 100%|██████████| 4/4 [00:00<00:00, 721.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating statistics...\n",
      "Choosing top 49 indexes from 19 non-materialized candidate indexes\n",
      "Non-materialized candidate indexes, X = ['IX_lineorder_lo_orderdate', 'IX_lineorder_lo_discount', 'IX_lineorder_lo_quantity', 'IX_lineorder_lo_orderdate_lo_discount', 'IX_lineorder_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_orderdate', 'IX_lineorder_lo_discount_lo_quantity', 'IX_lineorder_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_discount', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_lineorder_lo_discount_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_orderdate_lo_discount', 'IX_lineorder_lo_quantity_lo_discount_lo_orderdate', 'IX_dwdate_d_datekey', 'IX_dwdate_d_year', 'IX_dwdate_d_datekey_d_year', 'IX_dwdate_d_year_d_datekey']\n",
      "19 top indexes: ['IX_dwdate_d_year_d_datekey', 'IX_dwdate_d_year', 'IX_dwdate_d_datekey_d_year', 'IX_lineorder_lo_orderdate', 'IX_lineorder_lo_discount', 'IX_lineorder_lo_quantity', 'IX_lineorder_lo_orderdate_lo_discount', 'IX_lineorder_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_orderdate', 'IX_lineorder_lo_discount_lo_quantity', 'IX_lineorder_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_discount', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_lineorder_lo_discount_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_orderdate_lo_discount', 'IX_lineorder_lo_quantity_lo_discount_lo_orderdate', 'IX_dwdate_d_datekey']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 20\n",
      "Choosing new partitions...\n",
      "Old partitions:\n",
      "\t['IX_lineorder_lo_custkey']\n",
      "New partitions:\n",
      "\t['IX_lineorder_lo_custkey']\n",
      "\t['IX_lineorder_lo_orderdate']\n",
      "\t['IX_lineorder_lo_discount']\n",
      "\t['IX_lineorder_lo_quantity']\n",
      "\t['IX_lineorder_lo_orderdate_lo_discount']\n",
      "\t['IX_lineorder_lo_orderdate_lo_quantity']\n",
      "\t['IX_lineorder_lo_discount_lo_orderdate']\n",
      "\t['IX_lineorder_lo_discount_lo_quantity']\n",
      "\t['IX_lineorder_lo_quantity_lo_orderdate']\n",
      "\t['IX_lineorder_lo_quantity_lo_discount']\n",
      "\t['IX_lineorder_lo_orderdate_lo_discount_lo_quantity']\n",
      "\t['IX_lineorder_lo_orderdate_lo_quantity_lo_discount']\n",
      "\t['IX_lineorder_lo_discount_lo_orderdate_lo_quantity']\n",
      "\t['IX_lineorder_lo_discount_lo_quantity_lo_orderdate']\n",
      "\t['IX_lineorder_lo_quantity_lo_orderdate_lo_discount']\n",
      "\t['IX_lineorder_lo_quantity_lo_discount_lo_orderdate']\n",
      "\t['IX_dwdate_d_datekey']\n",
      "\t['IX_dwdate_d_year_d_datekey', 'IX_dwdate_d_year', 'IX_dwdate_d_datekey_d_year']\n",
      "Repartitioning...\n",
      "Reinitializing WFA instances...\n",
      "Replaced stable partitions, WFA instances and recommendations with new ones\n",
      "Analyzing query...\n",
      "Updating WFA instance: 0\n",
      "Best state: (), Best score: 1431582.68\n",
      "\tWFA Instance #0, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: ['IX_lineorder_lo_custkey']\n",
      "Updating WFA instance: 1\n",
      "Best state: (), Best score: 1431582.68\n",
      "\tWFA Instance #1, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 2\n",
      "Best state: (), Best score: 1431582.68\n",
      "\tWFA Instance #2, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 3\n",
      "Best state: (), Best score: 1431582.68\n",
      "\tWFA Instance #3, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 4\n",
      "Best state: (), Best score: 1431582.68\n",
      "\tWFA Instance #4, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 5\n",
      "Best state: (), Best score: 1431582.68\n",
      "\tWFA Instance #5, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 6\n",
      "Best state: (), Best score: 1431582.68\n",
      "\tWFA Instance #6, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 7\n",
      "Best state: (), Best score: 1431582.68\n",
      "\tWFA Instance #7, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 8\n",
      "Best state: (), Best score: 1431582.68\n",
      "\tWFA Instance #8, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 9\n",
      "Best state: (), Best score: 1431582.68\n",
      "\tWFA Instance #9, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 10\n",
      "Best state: (), Best score: 1431582.68\n",
      "\tWFA Instance #10, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 11\n",
      "Best state: (), Best score: 1431582.68\n",
      "\tWFA Instance #11, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 12\n",
      "Best state: (), Best score: 1431582.68\n",
      "\tWFA Instance #12, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 13\n",
      "Best state: (), Best score: 1431582.68\n",
      "\tWFA Instance #13, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 14\n",
      "Best state: (), Best score: 1431582.68\n",
      "\tWFA Instance #14, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 15\n",
      "Best state: (), Best score: 1431582.68\n",
      "\tWFA Instance #15, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 16\n",
      "Best state: (), Best score: 1431582.68\n",
      "\tWFA Instance #16, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 17\n",
      "Best state: (<pg_utils.Index object at 0x7f4d146920d0>,), Best score: 1431518.14\n",
      "\tWFA Instance #17, Num States: 8, New Recommendation: ['IX_dwdate_d_year_d_datekey'] --> Indexes Added: ['IX_dwdate_d_year_d_datekey'], Indexes Removed: []\n",
      "Currently materialized indexes: ['IX_dwdate_d_year_d_datekey']\n",
      "Removing stale indexes from U...\n",
      "Number of indexes in U: 20\n",
      "Number of indexes removed: 17, Number of indexes remaining: 3\n",
      "*** WFIT recommendation: ['IX_dwdate_d_year_d_datekey']\n",
      "*** Simple recommendation: ['IX_dwdate_d_year_d_datekey']\n",
      "Total time taken for processing query #1: 0.11479043960571289 seconds\n",
      "(Partitioning: 0.11387443542480469 seconds, Repartitioning: 0.000118255615234375 seconds, Analyzing: 0.0007960796356201172 seconds)\n",
      "\n",
      "\n",
      "\n",
      "Processing query 2\n",
      "Generating new partitions for query #2\n",
      "Extracted 19 new indexes from query.\n",
      "Candidate indexes (including those currently materialized), |U| = 22\n",
      "['IX_dwdate_d_year', 'IX_dwdate_d_datekey_d_year', 'IX_dwdate_d_year_d_datekey', 'IX_lineorder_lo_orderdate', 'IX_lineorder_lo_discount', 'IX_lineorder_lo_quantity', 'IX_lineorder_lo_orderdate_lo_discount', 'IX_lineorder_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_orderdate', 'IX_lineorder_lo_discount_lo_quantity', 'IX_lineorder_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_discount', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_lineorder_lo_discount_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_orderdate_lo_discount', 'IX_lineorder_lo_quantity_lo_discount_lo_orderdate', 'IX_dwdate_d_datekey', 'IX_dwdate_d_yearmonthnum', 'IX_dwdate_d_datekey_d_yearmonthnum', 'IX_dwdate_d_yearmonthnum_d_datekey']\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 22\n",
      "Index id to integer mapping: {'IX_dwdate_d_year': 0, 'IX_dwdate_d_datekey_d_year': 1, 'IX_dwdate_d_year_d_datekey': 2, 'IX_lineorder_lo_orderdate': 3, 'IX_lineorder_lo_discount': 4, 'IX_lineorder_lo_quantity': 5, 'IX_lineorder_lo_orderdate_lo_discount': 6, 'IX_lineorder_lo_orderdate_lo_quantity': 7, 'IX_lineorder_lo_discount_lo_orderdate': 8, 'IX_lineorder_lo_discount_lo_quantity': 9, 'IX_lineorder_lo_quantity_lo_orderdate': 10, 'IX_lineorder_lo_quantity_lo_discount': 11, 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity': 12, 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount': 13, 'IX_lineorder_lo_discount_lo_orderdate_lo_quantity': 14, 'IX_lineorder_lo_discount_lo_quantity_lo_orderdate': 15, 'IX_lineorder_lo_quantity_lo_orderdate_lo_discount': 16, 'IX_lineorder_lo_quantity_lo_discount_lo_orderdate': 17, 'IX_dwdate_d_datekey': 18, 'IX_dwdate_d_yearmonthnum': 19, 'IX_dwdate_d_datekey_d_yearmonthnum': 20, 'IX_dwdate_d_yearmonthnum_d_datekey': 21}\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21\n",
      "Constructing IBG...\n",
      "Creating node # 4\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No index scans were explicitly noted in the query plan.\n",
      "Number of nodes in IBG: 20, Total number of what-if calls: 20, Time spent on what-if calls: 0.38213372230529785\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes: 100%|██████████| 20/20 [00:00<00:00, 479.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating statistics...\n",
      "Choosing top 49 indexes from 21 non-materialized candidate indexes\n",
      "Non-materialized candidate indexes, X = ['IX_dwdate_d_year', 'IX_dwdate_d_datekey_d_year', 'IX_lineorder_lo_orderdate', 'IX_lineorder_lo_discount', 'IX_lineorder_lo_quantity', 'IX_lineorder_lo_orderdate_lo_discount', 'IX_lineorder_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_orderdate', 'IX_lineorder_lo_discount_lo_quantity', 'IX_lineorder_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_discount', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_lineorder_lo_discount_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_orderdate_lo_discount', 'IX_lineorder_lo_quantity_lo_discount_lo_orderdate', 'IX_dwdate_d_datekey', 'IX_dwdate_d_yearmonthnum', 'IX_dwdate_d_datekey_d_yearmonthnum', 'IX_dwdate_d_yearmonthnum_d_datekey']\n",
      "21 top indexes: ['IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_lineorder_lo_orderdate_lo_discount', 'IX_lineorder_lo_orderdate_lo_quantity', 'IX_dwdate_d_yearmonthnum_d_datekey', 'IX_dwdate_d_yearmonthnum', 'IX_dwdate_d_year', 'IX_dwdate_d_datekey_d_yearmonthnum', 'IX_dwdate_d_datekey_d_year', 'IX_lineorder_lo_orderdate', 'IX_lineorder_lo_discount', 'IX_lineorder_lo_quantity', 'IX_lineorder_lo_discount_lo_orderdate', 'IX_lineorder_lo_discount_lo_quantity', 'IX_lineorder_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_discount', 'IX_lineorder_lo_discount_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_orderdate_lo_discount', 'IX_lineorder_lo_quantity_lo_discount_lo_orderdate', 'IX_dwdate_d_datekey']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 22\n",
      "Choosing new partitions...\n",
      "Old partitions:\n",
      "\t['IX_lineorder_lo_custkey']\n",
      "\t['IX_lineorder_lo_orderdate']\n",
      "\t['IX_lineorder_lo_discount']\n",
      "\t['IX_lineorder_lo_quantity']\n",
      "\t['IX_lineorder_lo_orderdate_lo_discount']\n",
      "\t['IX_lineorder_lo_orderdate_lo_quantity']\n",
      "\t['IX_lineorder_lo_discount_lo_orderdate']\n",
      "\t['IX_lineorder_lo_discount_lo_quantity']\n",
      "\t['IX_lineorder_lo_quantity_lo_orderdate']\n",
      "\t['IX_lineorder_lo_quantity_lo_discount']\n",
      "\t['IX_lineorder_lo_orderdate_lo_discount_lo_quantity']\n",
      "\t['IX_lineorder_lo_orderdate_lo_quantity_lo_discount']\n",
      "\t['IX_lineorder_lo_discount_lo_orderdate_lo_quantity']\n",
      "\t['IX_lineorder_lo_discount_lo_quantity_lo_orderdate']\n",
      "\t['IX_lineorder_lo_quantity_lo_orderdate_lo_discount']\n",
      "\t['IX_lineorder_lo_quantity_lo_discount_lo_orderdate']\n",
      "\t['IX_dwdate_d_datekey']\n",
      "\t['IX_dwdate_d_year_d_datekey', 'IX_dwdate_d_year', 'IX_dwdate_d_datekey_d_year']\n",
      "New partitions:\n",
      "\t['IX_lineorder_lo_orderdate']\n",
      "\t['IX_lineorder_lo_discount']\n",
      "\t['IX_lineorder_lo_discount_lo_orderdate']\n",
      "\t['IX_lineorder_lo_discount_lo_quantity']\n",
      "\t['IX_lineorder_lo_quantity_lo_orderdate']\n",
      "\t['IX_lineorder_lo_quantity_lo_discount']\n",
      "\t['IX_lineorder_lo_discount_lo_orderdate_lo_quantity']\n",
      "\t['IX_lineorder_lo_discount_lo_quantity_lo_orderdate']\n",
      "\t['IX_lineorder_lo_quantity_lo_orderdate_lo_discount']\n",
      "\t['IX_lineorder_lo_quantity_lo_discount_lo_orderdate']\n",
      "\t['IX_dwdate_d_datekey']\n",
      "\t['IX_dwdate_d_year_d_datekey', 'IX_dwdate_d_year', 'IX_dwdate_d_datekey_d_year']\n",
      "\t['IX_lineorder_lo_quantity', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_discount', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_lineorder_lo_orderdate_lo_quantity', 'IX_dwdate_d_datekey_d_yearmonthnum', 'IX_dwdate_d_yearmonthnum_d_datekey', 'IX_dwdate_d_yearmonthnum']\n",
      "Repartitioning...\n",
      "Reinitializing WFA instances...\n",
      "Replaced stable partitions, WFA instances and recommendations with new ones\n",
      "Analyzing query...\n",
      "Updating WFA instance: 0\n",
      "Best state: (), Best score: 27265041.689999998\n",
      "\tWFA Instance #0, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 1\n",
      "Best state: (), Best score: 27265041.689999998\n",
      "\tWFA Instance #1, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 2\n",
      "Best state: (), Best score: 27265041.689999998\n",
      "\tWFA Instance #2, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 3\n",
      "Best state: (), Best score: 27265041.689999998\n",
      "\tWFA Instance #3, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 4\n",
      "Best state: (), Best score: 27265041.689999998\n",
      "\tWFA Instance #4, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 5\n",
      "Best state: (), Best score: 27265041.689999998\n",
      "\tWFA Instance #5, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 6\n",
      "Best state: (), Best score: 27265041.689999998\n",
      "\tWFA Instance #6, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 7\n",
      "Best state: (), Best score: 27265041.689999998\n",
      "\tWFA Instance #7, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 8\n",
      "Best state: (), Best score: 27265041.689999998\n",
      "\tWFA Instance #8, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 9\n",
      "Best state: (), Best score: 27265041.689999998\n",
      "\tWFA Instance #9, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 10\n",
      "Best state: (), Best score: 27265041.689999998\n",
      "\tWFA Instance #10, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 11\n",
      "Best state: (), Best score: 27265041.689999998\n",
      "\tWFA Instance #11, Num States: 8, New Recommendation: [] --> Indexes Added: [], Indexes Removed: ['IX_dwdate_d_year_d_datekey']\n",
      "Updating WFA instance: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# instantiate WFIT\n",
    "C = extract_query_indexes(qg.generate_query(8), include_cols=True)  \n",
    "S_0 = C[0:1]\n",
    "#wfit = WFIT(S_0, idxCnt=20, stateCnt=1000, histSize=100, rand_cnt=10)\n",
    "wfit = WFIT(S_0, max_key_columns=3, stateCnt=300, rand_cnt=50)\n",
    "\n",
    "# process the workload\n",
    "for i, query in enumerate(workload):\n",
    "    print(f\"Processing query {i+1}\")\n",
    "    wfit.process_WFIT(query, verbose=True)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
