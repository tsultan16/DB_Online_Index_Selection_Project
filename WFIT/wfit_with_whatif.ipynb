{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WFIT Algorithm Implementation (Schnaitter 2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "import IPython\n",
    "notebook_path = IPython.get_ipython().starting_dir\n",
    "target_subdirectory_path = os.path.abspath(os.path.join(os.path.dirname(notebook_path), 'PostgreSQL'))\n",
    "sys.path.append(target_subdirectory_path)\n",
    "\n",
    "from pg_utils import *\n",
    "from ssb_qgen_class import *\n",
    "\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index Benefit Graph (IBG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, id, indexes):\n",
    "        self.id = id\n",
    "        self.indexes = indexes\n",
    "        self.children = []\n",
    "        self.parents = []\n",
    "        self.built = False\n",
    "        self.cost = None\n",
    "        self.used = None\n",
    "\n",
    "\n",
    "# class for creating and storing the IBG\n",
    "class IBG:\n",
    "    def __init__(self, query_object, C):\n",
    "        self.q = query_object\n",
    "        self.C = C\n",
    "        print(f\"Number of candidate indexes: {len(self.C)}\")\n",
    "        #print(f\"Candidate indexes: {self.C}\")\n",
    "        \n",
    "        # map index_id to integer\n",
    "        self.idx2id = {index.index_id:i for i, index in enumerate(self.C)}\n",
    "        self.idx2index = {index.index_id:index for index in self.C}\n",
    "        \n",
    "        # create a hash table for keeping track of all created nodes\n",
    "        self.nodes = {}\n",
    "        # create a root node\n",
    "        self.root = Node(self.get_configuration_id(self.C), self.C)\n",
    "        self.nodes[self.root.id] = self.root\n",
    "        print(f\"Created root node with id: {self.root.id}\")\n",
    "        # start the IBG construction\n",
    "        print(\"Constructing IBG...\")\n",
    "        self.construct_ibg(self.root)\n",
    "        # compute all pair degree of interaction\n",
    "        print(f\"Computing all pair degree of interaction...\")\n",
    "        self.doi = self.compute_all_pair_doi()\n",
    "\n",
    "\n",
    "    # assign unique string id to a configuration\n",
    "    def get_configuration_id(self, indexes):\n",
    "        # get sorted list of integer ids\n",
    "        ids = sorted([self.idx2id[idx.index_id] for idx in indexes])\n",
    "        return \"_\".join([str(i) for i in ids])\n",
    "    \n",
    "\n",
    "    # obtain cost and used indexes for a given configuration\n",
    "    def _get_cost_used(self, indexes):\n",
    "        conn = create_connection()\n",
    "        # create hypothetical indexes\n",
    "        hypo_indexes = bulk_create_hypothetical_indexes(conn, indexes)\n",
    "        # map oid to index object\n",
    "        oid2index = {}\n",
    "        for i in range(len(hypo_indexes)):\n",
    "            oid2index[hypo_indexes[i]] = indexes[i]\n",
    "        # get cost and used indexes\n",
    "        cost, indexes_used = get_query_cost_estimate_hypo_indexes(conn, self.q.query_string, show_plan=False)\n",
    "        # map used index oids to index objects\n",
    "        used = [oid2index[oid] for oid,scan_type,scan_cost in indexes_used]\n",
    "        # drop hypothetical indexes\n",
    "        bulk_drop_hypothetical_indexes(conn)\n",
    "        close_connection(conn)   \n",
    "        return cost, used\n",
    "\n",
    "    # recursive IBG construction algorithm\n",
    "    def construct_ibg(self, Y):\n",
    "        if Y.built:\n",
    "            return \n",
    "        \n",
    "        # obtain query optimizers cost and used indexes\n",
    "        cost, used = self._get_cost_used(Y.indexes)\n",
    "        Y.cost = cost\n",
    "        Y.used = used\n",
    "        Y.built = True\n",
    "        \n",
    "        #print(f\"Creating node for configuration: {[idx.index_id for idx in Y.indexes]}\")\n",
    "        #print(f\"Cost: {cost}, Used indexes:\")\n",
    "        #for idx in used:\n",
    "        #    print(f\"{idx}\")\n",
    "\n",
    "        # create children\n",
    "        for a in Y.used:\n",
    "            # create a new configuration with index a removed from Y\n",
    "            X_indexes = [index for index in Y.indexes if index != a]\n",
    "            X_id = self.get_configuration_id(X_indexes)\n",
    "            \n",
    "            # if X is not in the hash table, create a new node and recursively build it\n",
    "            if X_id not in self.nodes:\n",
    "                X = Node(X_id, X_indexes)\n",
    "                X.parents.append(Y)\n",
    "                self.nodes[X_id] = X\n",
    "                Y.children.append(X)\n",
    "                self.construct_ibg(X)\n",
    "\n",
    "            else:\n",
    "                X = self.nodes[X_id]\n",
    "                Y.children.append(X)\n",
    "                X.parents.append(Y)\n",
    "\n",
    "\n",
    "    # use IBG to obtain estimated cost and used indexes for arbitrary subset of C\n",
    "    def get_cost_used(self, X):\n",
    "        # get id of the configuration\n",
    "        id = self.get_configuration_id(X)\n",
    "        # check if the configuration is in the IBG\n",
    "        if id in self.nodes:\n",
    "            cost, used = self.nodes[id].cost, self.nodes[id].used\n",
    "        \n",
    "        # if not in the IBG, traverse the IBG to find a covering node\n",
    "        else:\n",
    "            Y = self.find_covering_node(X)              \n",
    "            cost, used = Y.cost, Y.used\n",
    "\n",
    "        return cost, used    \n",
    "\n",
    "\n",
    "    # traverses the IBG to find a node that removes indexes not in X (i.e. a covering node for X)\n",
    "    def find_covering_node(self, X):\n",
    "        X_indexes = set([index.index_id for index in X])\n",
    "        Y = self.root\n",
    "        Y_indexes = set([index.index_id for index in Y.indexes])\n",
    "        # traverse IBG to find covering node\n",
    "        while (len(Y_indexes - X_indexes) != 0) or (len(Y.children) > 0):               \n",
    "            # traverse down to the child node that removes an index not in X\n",
    "            child_found = False\n",
    "            for child in Y.children:\n",
    "                child_indexes = set([index.index_id for index in child.indexes])\n",
    "                child_indexes_removed = Y_indexes - child_indexes\n",
    "                child_indexes_removed_not_in_X = child_indexes_removed - X_indexes\n",
    "        \n",
    "                # check if child removes an index not in X\n",
    "                if len(child_indexes_removed_not_in_X) > 0:\n",
    "                    Y = child\n",
    "                    Y_indexes = child_indexes\n",
    "                    child_found = True\n",
    "                    break\n",
    "\n",
    "            # if no children remove indexes not in X    \n",
    "            if not child_found:\n",
    "                break    \n",
    "    \n",
    "        return Y        \n",
    "\n",
    "    # compute benefit of an index for a given configuration \n",
    "    # input X is a list of index objects and 'a' is a single index object\n",
    "    # X must not contain 'a'\n",
    "    def compute_benefit(self, a, X):\n",
    "        if a in X:\n",
    "            # zero benefit if 'a' is already in X\n",
    "            #raise ValueError(\"Index 'a' is already in X\")\n",
    "            return 0\n",
    "        \n",
    "        # get cost  for X\n",
    "        cost_X = self.get_cost_used(X)[0]\n",
    "        # create a new configuration with index a added to X\n",
    "        X_a = X + [a]\n",
    "        # get cost for X + {a}\n",
    "        cost_X_a = self.get_cost_used(X_a)[0]\n",
    "        # compute benefit\n",
    "        benefit = cost_X - cost_X_a\n",
    "        return benefit \n",
    "\n",
    "\n",
    "    # compute maximum benefit of adding an index to any possibe configuration\n",
    "    def compute_max_benefit(self, a):\n",
    "        max_benefit = float('-inf')\n",
    "        for id, node in self.nodes.items():\n",
    "            #print(f\"Computing benefit for node: {[index.index_id for index in node.indexes]}\")\n",
    "            benefit = self.compute_benefit(a, node.indexes)\n",
    "            if benefit > max_benefit:\n",
    "                max_benefit = benefit\n",
    "\n",
    "        return max_benefit\n",
    "    \n",
    "    # compute the degree of interaction between two indexes a,b in configuration X \n",
    "    def compute_doi_configuration(self, a, b, X):\n",
    "        # X must not contain a or b\n",
    "        if a in X or b in X:\n",
    "            raise ValueError(\"a or b is already in X\")\n",
    "\n",
    "        doi = abs(self.compute_benefit(a, X) - self.compute_benefit(a, X + [b]))\n",
    "        doi /= self.get_cost_used(X + [a,b])[0]   \n",
    "        return doi\n",
    "   \n",
    "    \n",
    "    # computes the degree of interaction between all pairs of indexes (a,b) in candidate set C\n",
    "    # Note: doi is symmetric, i.e. doi(a,b) = doi(b,a)\n",
    "    def compute_all_pair_doi(self):\n",
    "        # hash table for storing doi values\n",
    "        doi = {}\n",
    "        # intialize doi values to zero\n",
    "        for i in range(len(self.C)):\n",
    "            for j in range(i+1, len(self.C)):\n",
    "                doi[(self.C[i].index_id, self.C[j].index_id)] = 0\n",
    "\n",
    "        S_idxs = set([index.index_id for index in self.C])\n",
    "\n",
    "        # iterate over each IBG node\n",
    "        for Y in self.nodes.values():\n",
    "            # remove Y.used from S\n",
    "            Y_idxs = set([index.index_id for index in Y.indexes])\n",
    "            S_Y = list(S_idxs - Y_idxs)\n",
    "            # iterate over all pairs of indexes in S_Y\n",
    "            for i in range(len(S_Y)):\n",
    "                for j in range(i+1, len(S_Y)):\n",
    "                    a_idx = S_Y[i]\n",
    "                    b_idx = S_Y[j]\n",
    "                     \n",
    "                    # find Ya covering node in IBG\n",
    "                    Ya = (Y_idxs - {a_idx, b_idx}) | {a_idx}\n",
    "                    Ya = [self.idx2index[idx] for idx in Ya]\n",
    "                    Ya = self.find_covering_node(Ya).indexes\n",
    "                    # find Yab covering node in IBG\n",
    "                    Yab = (Y_idxs - {a_idx, b_idx}) | {a_idx, b_idx}\n",
    "                    Yab = [self.idx2index[idx] for idx in Yab]\n",
    "                    Yab = self.find_covering_node(Yab).indexes\n",
    "\n",
    "                    used_Y = self.get_cost_used(Y.indexes)[1]\n",
    "                    used_Ya = self.get_cost_used(Ya)[1]\n",
    "                    used_Yab = self.get_cost_used(Yab)[1]\n",
    "                    \n",
    "                    Uab = set([index.index_id for index in used_Y]) | set([index.index_id for index in used_Ya]) | set([index.index_id for index in used_Yab]) \n",
    "                    # find Yb_minus covering node in IBG \n",
    "                    Yb_minus = list((Uab - {a_idx, b_idx}) | {b_idx})\n",
    "                    Yb_minus = [self.idx2index[idx] for idx in Yb_minus]\n",
    "                    Yb_minus = self.find_covering_node(Yb_minus).indexes\n",
    "                    # find Yb_plus covering node in IBG\n",
    "                    Yb_plus = list((Y_idxs - {a_idx, b_idx}) | {b_idx})\n",
    "                    Yb_plus = [self.idx2index[idx] for idx in Yb_plus]\n",
    "                    Yb_plus = self.find_covering_node(Yb_plus).indexes\n",
    "\n",
    "                    # generate quadruples\n",
    "                    quadruples = [(Y.indexes, Ya, Yb_minus, Yab), (Y.indexes, Ya, Yb_plus, Yab)]\n",
    "\n",
    "                    # compute doi using the quadruples\n",
    "                    for Y_indexes, Ya_indexes, Yb_indexes, Yab_indexes in quadruples:\n",
    "                        cost_Y = self.get_cost_used(Y_indexes)[0]\n",
    "                        cost_Ya = self.get_cost_used(Ya_indexes)[0]\n",
    "                        cost_Yb = self.get_cost_used(Yb_indexes)[0]\n",
    "                        cost_Yab = self.get_cost_used(Yab_indexes)[0]\n",
    "                        d = abs(cost_Y - cost_Ya - cost_Yb + cost_Yab) / cost_Yab\n",
    "                        if (a_idx, b_idx) in doi:\n",
    "                            doi[(a_idx,b_idx)] = max(doi[(a_idx,b_idx)], d)\n",
    "                        elif (b_idx, a_idx) in doi:\n",
    "                            doi[(b_idx,a_idx)] = max(doi[(b_idx,a_idx)], d)\n",
    "                        else:\n",
    "                            raise ValueError(\"Invalid pair of indexes\")    \n",
    "                            \n",
    "        \n",
    "        return doi\n",
    "\n",
    "\n",
    "    # get precomputed degree of interaction between a pair of indexes\n",
    "    def get_doi_pair(self, a, b):\n",
    "        if (a.index_id, b.index_id) in self.doi:\n",
    "            return self.doi[(a.index_id, b.index_id)]\n",
    "        elif (b.index_id, a.index_id) in self.doi:\n",
    "            return self.doi[(b.index_id, a.index_id)]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid pair of indexes\")\n",
    "\n",
    "\n",
    "    # function for printing the IBG, using BFS level order traversal\n",
    "    def print_ibg(self):\n",
    "        q = [self.root]\n",
    "        # traverse level by level, print all node ids in a level in a single line before moving to the next level\n",
    "        while len(q) > 0:\n",
    "            next_q = []\n",
    "            for node in q:\n",
    "                print(f\"{node.id} -> \", end=\"\")\n",
    "                for child in node.children:\n",
    "                    next_q.append(child)\n",
    "            print()\n",
    "            q = next_q  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an SSB query generator object\n",
    "qg = QGEN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "template id: 14, query: \n",
      "                SELECT lo_linenumber, lo_quantity, lo_orderdate  \n",
      "                FROM lineorder\n",
      "                WHERE lo_linenumber >= 3 AND lo_linenumber <= 4\n",
      "                AND lo_quantity = 32;\n",
      "            , payload: {'lineorder': ['lo_linenumber', 'lo_quantity', 'lo_orderdate']}, predicates: {'lineorder': ['lo_linenumber', 'lo_quantity']}, order by: {}, group by: {}\n",
      "Number of candidate indexes: 12\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Computing all pair degree of interaction...\n",
      "0_1_2_3_4_5_6_7_8_9_10_11 -> \n",
      "0_1_2_3_4_5_6_7_8_9_10 -> \n",
      "0_1_2_3_4_5_6_8_9_10 -> \n",
      "0_1_2_3_4_5_6_8_10 -> \n",
      "0_1_2_4_5_6_8_10 -> \n",
      "0_1_2_4_5_6_8 -> \n",
      "IBG     --> Cost: 21589.56, Used indexes: ['IXN_lineorder_lo_quantity_lo_linenumber_lo_o']\n",
      "What-if --> Cost: 21589.56, Used indexes: ['IXN_lineorder_lo_quantity_lo_linenumber_lo_o']\n",
      "\n",
      "Maximum benefit of adding index IX_lineorder_lo_linenumber: 0\n",
      "\n",
      "DOI between indexes IX_lineorder_lo_linenumber and IX_lineorder_lo_quantity : 0.0\n",
      "in configuration ['IXN_lineorder_lo_linenumber_lo_o', 'IXN_lineorder_lo_linenumber_lo_q', 'IXN_lineorder_lo_quantity_lo_l', 'IXN_lineorder_lo_quantity_lo_o', 'IX_lineorder_lo_linenumber_lo_quantity']\n",
      "\n",
      "DOI between indexes IX_lineorder_lo_linenumber and IX_lineorder_lo_quantity : 0\n"
     ]
    }
   ],
   "source": [
    "# test IBG \n",
    "\n",
    "query = qg.generate_query(14)\n",
    "print(query)\n",
    "\n",
    "C = extract_query_indexes(qg.generate_query(14), include_cols=True)  \n",
    "\n",
    "ibg = IBG(query, C)\n",
    "\n",
    "ibg.print_ibg()\n",
    "\n",
    "# pick random subset of candidate indexes\n",
    "X = random.sample(ibg.C, 8)\n",
    "cost, used = ibg.get_cost_used(X)\n",
    "print(f\"IBG     --> Cost: {cost}, Used indexes: {[idx.index_id for idx in used]}\")\n",
    "\n",
    "cost, used = ibg._get_cost_used(X)\n",
    "print(f\"What-if --> Cost: {cost}, Used indexes: {[idx.index_id for idx in used]}\")\n",
    "\n",
    "# pick two indexes and a configuration\n",
    "a = ibg.C[0]\n",
    "b = ibg.C[4] \n",
    "X = [ibg.C[1], ibg.C[2], ibg.C[5], ibg.C[6], ibg.C[8]]\n",
    "\n",
    "# compute maximum benefit of adding index 'a' \n",
    "max_benefit = ibg.compute_max_benefit(a)\n",
    "print(f\"\\nMaximum benefit of adding index {a.index_id}: {max_benefit}\")\n",
    "\n",
    "# compute degree of interaction between indexes 'a' and 'b' in configuration X\n",
    "doi = ibg.compute_doi_configuration(a, b, X)\n",
    "print(f\"\\nDOI between indexes {a.index_id} and {b.index_id} : {doi}\")\n",
    "print(f\"in configuration {[idx.index_id for idx in X]}\")\n",
    "\n",
    "# compute configuration independent degree of interaction between indexes 'a' and 'b'\n",
    "doi = ibg.get_doi_pair(a, b)\n",
    "print(f\"\\nDOI between indexes {a.index_id} and {b.index_id} : {doi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for key, value in ibg.doi.items():\n",
    "#    print(f\"doi({key[0]},   {key[1]}) = {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WFIT class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WFIT:\n",
    "\n",
    "    def __init__(self, S_0=[], idxCnt=1000, stateCnt=1000, histSize=100):\n",
    "        # initial set of materialzed indexes\n",
    "        self.S_0 = S_0\n",
    "        # parameter for maximum number of candidate indexes tracked \n",
    "        self.idxCnt = idxCnt\n",
    "        # parameter for maximum number of MTS states/configurations\n",
    "        self.stateCnt = stateCnt\n",
    "        # parameter for maximum number of historical index statistics kept\n",
    "        self.histSize = histSize\n",
    "        # growing list of candidate indexes\n",
    "        self.U = {}\n",
    "        # index benefit and interaction statistics\n",
    "        self.idxStats = defaultdict(list)\n",
    "        self.intStats = defaultdict(list)\n",
    "        # list of currently monitored indexes\n",
    "        self.C = {index.index_id:index for index in S_0} \n",
    "        # list of currently materialized indexes\n",
    "        self.M = {index.index_id:index for index in S_0}  \n",
    "        # initialize stable partitions (each partition is a singleton set of indexes from S_0)\n",
    "        self.stable_partitions = [{index} for index in S_0]\n",
    "        self.n_pos = 0\n",
    "\n",
    "\n",
    "    # update WFIT step for next query in workload (this is the main interface for generating an index configuration recommendation)\n",
    "    def process_WFIT(self, query_object, verbose=False):\n",
    "        self.n_pos += 1\n",
    "        # generate new partitions \n",
    "        if verbose: print(f\"Generating new partitions for query #{self.n_pos}\")\n",
    "        new_partitions = self.choose_candidates(self.n_pos, query_object, verbose)\n",
    "        # repartition if necessary\n",
    "        if verbose: print(f\"Repartitioning...\")\n",
    "        self.repartition(new_partitions)\n",
    "        # analyze the query\n",
    "        if verbose: print(f\"Analyzing query...\")\n",
    "        self.analyze_query(query_object)\n",
    "\n",
    "\n",
    "    # TODO: repartition the stable partitions based on the new partitions\n",
    "    def repartition(self, new_partitions):\n",
    "        pass\n",
    "\n",
    "\n",
    "    # TODO: update WFA instance on each stable partition\n",
    "    def analyze_query(self, query_object):\n",
    "        pass\n",
    "\n",
    "\n",
    "    # TODO: update a WFA instance for the given query    \n",
    "    def process_WFA(self, query_object, w, S, S_curr):\n",
    "        pass    \n",
    "\n",
    "\n",
    "    # compute index benefit graph for the given query and candidate indexes\n",
    "    def compute_IBG(self, query_object, candidate_indexes):\n",
    "        return IBG(query_object, candidate_indexes)\n",
    "    \n",
    "\n",
    "    # extract candidate indexes from given query\n",
    "    def extract_indexes(self, query_object, include_cols=True):\n",
    "        return extract_query_indexes(query_object, include_cols)\n",
    "\n",
    "\n",
    "    # generate stable partitions/sets of indexes for next query in workload\n",
    "    def choose_candidates(self, n_pos, query_object, verbose):\n",
    "        # extract new candidate indexes from the query\n",
    "        new_indexes = self.extract_indexes(query_object)\n",
    "        if verbose: print(f\"Extracted {len(new_indexes)} indexes from query.\")\n",
    "        # add new indexes to the list of all candidate indexes\n",
    "        for index in new_indexes:\n",
    "            if index.index_id not in self.U:\n",
    "                self.U[index.index_id] = index\n",
    "        \n",
    "        if verbose: print(f\"Num candidate indexes (including those currently materialized), |U| = {len(self.U)}\")\n",
    "        \n",
    "\n",
    "        # TODO: check if the number of candidate indexes exceeds the limit, then need to evict some indexes\n",
    "        \n",
    "        # compute index benefit graph for the query\n",
    "        if verbose: print(f\"Computing IBG...\")\n",
    "        ibg = self.compute_IBG(query_object, list(self.U.values()))\n",
    "        \n",
    "        # update statistics for the candidate indexes (n_pos is the position of the query in the workload sequence)\n",
    "        if verbose: print(f\"Updating statistics...\")\n",
    "        self.update_stats(n_pos, ibg, verbose=False)\n",
    "\n",
    "        # non-materialized candidate indexes \n",
    "        X = [self.U[index_id] for index_id in self.U if index_id not in self.M]\n",
    "        num_indexes = self.idxCnt - len(self.M)\n",
    "\n",
    "        # determine new set of candidate indexes to monitor for upcoming workload queries\n",
    "        if verbose: print(f\"Choosing top {num_indexes} indexes from {len(X)} non-materialized candidate indexes\")\n",
    "        top_indexes = self.top_indexes(n_pos, X, num_indexes, verbose)\n",
    "        D = self.M | top_indexes\n",
    "        if verbose: print(f\"New set of indexes to monitor for upcoming workload, |D| = {len(D)}\")\n",
    "\n",
    "        # generate new partitions by clustering the new candidate set\n",
    "        if verbose: print(f\"Generating new partitions...\")\n",
    "        new_partitions = self.choose_partition(D)\n",
    "\n",
    "\n",
    "\n",
    "    # TODO: partition the new candidate set into clusters\n",
    "    def choose_partition(self, D):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "    # update candidate index statistics\n",
    "    def update_stats(self, n, ibg, verbose):\n",
    "        # update index benefit statistics\n",
    "        for index in self.U.values():\n",
    "            max_benefit = ibg.compute_max_benefit(index)\n",
    "            self.idxStats[index.index_id].append((n, max_benefit))\n",
    "            # evict old stats if the size exceeds histSize\n",
    "            self.idxStats[index.index_id] = self.idxStats[index.index_id][-self.histSize:]\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Index benefit statistics:\")\n",
    "            for index_id, stats in self.idxStats.items():\n",
    "                print(f\"Index {index_id}: {stats}\")\n",
    "\n",
    "\n",
    "        # update index interaction statistics\n",
    "        for (a_idx, b_idx) in ibg.doi.keys():\n",
    "            d = ibg.doi[(a_idx, b_idx)]\n",
    "            if d > 0:\n",
    "                self.intStats[(a_idx, b_idx)].append((n, doi))\n",
    "            # evict old stats if the size exceeds histSize\n",
    "            self.intStats[(a_idx, b_idx)] = self.intStats[(a_idx, b_idx)][-self.histSize:]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Index interaction statistics:\")\n",
    "            for pair, stats in self.intStats.items():\n",
    "                print(f\"Pair {pair}: {stats}\")\n",
    "\n",
    "\n",
    "    # choose top num_indexes indexes from X with highest potential benefit\n",
    "    def top_indexes(self, N_workload, X, num_indexes, verbose):\n",
    "        if verbose:\n",
    "            print(f\"Non-materialized candidate indexes, X = {[index.index_id for index in X]}\")\n",
    "\n",
    "        # compute \"current benefit\" of each index in X (these are derived from statistics of observed benefits from recent queries)\n",
    "        score = {}\n",
    "        for index in X:\n",
    "            if len(self.idxStats[index.index_id]) == 0:\n",
    "                # zero current benefit if no statistics are available\n",
    "                current_benefit = 0\n",
    "            else:\n",
    "                benefits = []\n",
    "                b_total = 0\n",
    "                for (n, b) in self.idxStats[index.index_id]:\n",
    "                    b_total += b \n",
    "                    # cumulative average benefit of index up to query n (higher weight/smaller denominator for more recent queries)\n",
    "                    benefit = b_total / (N_workload - n + 1)\n",
    "                    benefits.append(benefit)\n",
    "\n",
    "                # take the maximum over all cumulative average benefits \n",
    "                current_benefit = max(benefits)    \n",
    "\n",
    "            # use current benefit to compute a score for the index\n",
    "            if index.index_id in self.C:\n",
    "                # if index already being monitored, then score is just current benefit\n",
    "                score[index.index_id] = current_benefit\n",
    "            else:\n",
    "                # if index not being monitored, then score is current benefit minus cost of creating the index\n",
    "                # (unmonitored indexes are penalized so that they are only chosen if they have high potential benefit, which helps keep C stable)\n",
    "                score[index.index_id] = current_benefit - self.get_index_creation_cost(index)\n",
    "\n",
    "        #if verbose:\n",
    "        #    print(\"Index scores:\")\n",
    "        #    for index_id, s in score.items():\n",
    "        #        print(f\"Index {index_id}: {s}\")\n",
    "\n",
    "        # get the top num_indexes indexes with highest scores (keep non-zero scores only)\n",
    "        top_indexes = [index_id for index_id, s in score.items() if s > 0]\n",
    "        top_indexes = sorted(top_indexes, key=lambda x: score[x], reverse=True)[:num_indexes]\n",
    "        top_indexes = {index_id: self.U[index_id] for index_id in top_indexes}\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{len(top_indexes)} top indexes: {[index.index_id for index in top_indexes.values()]}\")\n",
    "\n",
    "        return top_indexes    \n",
    "\n",
    "\n",
    "    # TODO: return index creation cost\n",
    "    def get_index_creation_cost(self, index):\n",
    "        return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test WFIT implementation on sample SSB workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate an SSB workload\n",
    "workload = [qg.generate_query(i) for i in range(1, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 1\n",
      "Generating new partitions for query #1\n",
      "Extracted 42 indexes from query.\n",
      "Num candidate indexes (including those currently materialized), |U| = 42\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 42\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Computing all pair degree of interaction...\n",
      "Updating statistics...\n",
      "Choosing top 19 indexes from 42 non-materialized candidate indexes\n",
      "Non-materialized candidate indexes, X = ['IX_lineorder_lo_orderdate', 'IXN_lineorder_lo_orderdate_lo_e', 'IXN_lineorder_lo_orderdate_lo_d', 'IXN_lineorder_lo_orderdate_lo_e_lo_d', 'IX_lineorder_lo_discount', 'IXN_lineorder_lo_discount_lo_e', 'IX_lineorder_lo_quantity', 'IXN_lineorder_lo_quantity_lo_e', 'IXN_lineorder_lo_quantity_lo_d', 'IXN_lineorder_lo_quantity_lo_e_lo_d', 'IX_lineorder_lo_orderdate_lo_discount', 'IXN_lineorder_lo_orderdate_lo_discount_lo_e', 'IX_lineorder_lo_orderdate_lo_quantity', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_d', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e_lo_d', 'IX_lineorder_lo_discount_lo_orderdate', 'IXN_lineorder_lo_discount_lo_orderdate_lo_e', 'IX_lineorder_lo_discount_lo_quantity', 'IXN_lineorder_lo_discount_lo_quantity_lo_e', 'IX_lineorder_lo_quantity_lo_orderdate', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_e', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_d', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_e_lo_d', 'IX_lineorder_lo_quantity_lo_discount', 'IXN_lineorder_lo_quantity_lo_discount_lo_e', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_discount_lo_e', 'IX_lineorder_lo_discount_lo_orderdate_lo_quantity', 'IXN_lineorder_lo_discount_lo_orderdate_lo_quantity_lo_e', 'IX_lineorder_lo_discount_lo_quantity_lo_orderdate', 'IXN_lineorder_lo_discount_lo_quantity_lo_orderdate_lo_e', 'IX_lineorder_lo_quantity_lo_orderdate_lo_discount', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_discount_lo_e', 'IX_lineorder_lo_quantity_lo_discount_lo_orderdate', 'IXN_lineorder_lo_quantity_lo_discount_lo_orderdate_lo_e', 'IX_dwdate_d_datekey', 'IX_dwdate_d_year', 'IX_dwdate_d_datekey_d_year', 'IX_dwdate_d_year_d_datekey']\n",
      "13 top indexes: ['IXN_lineorder_lo_orderdate_lo_quantity_lo_discount_lo_e', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e_lo_d', 'IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_discount_lo_e', 'IXN_lineorder_lo_quantity_lo_discount_lo_orderdate_lo_e', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_e_lo_d', 'IXN_lineorder_lo_discount_lo_orderdate_lo_quantity_lo_e', 'IXN_lineorder_lo_discount_lo_quantity_lo_orderdate_lo_e', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_dwdate_d_year_d_datekey', 'IX_dwdate_d_year', 'IX_dwdate_d_datekey_d_year']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 14\n",
      "Generating new partitions...\n",
      "Repartitioning...\n",
      "Analyzing query...\n",
      "\n",
      "\n",
      "\n",
      "Processing query 2\n",
      "Generating new partitions for query #2\n",
      "Extracted 42 indexes from query.\n",
      "Num candidate indexes (including those currently materialized), |U| = 45\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 45\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42_43_44\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Computing all pair degree of interaction...\n",
      "Updating statistics...\n",
      "Choosing top 19 indexes from 45 non-materialized candidate indexes\n",
      "Non-materialized candidate indexes, X = ['IX_lineorder_lo_orderdate', 'IXN_lineorder_lo_orderdate_lo_e', 'IXN_lineorder_lo_orderdate_lo_d', 'IXN_lineorder_lo_orderdate_lo_e_lo_d', 'IX_lineorder_lo_discount', 'IXN_lineorder_lo_discount_lo_e', 'IX_lineorder_lo_quantity', 'IXN_lineorder_lo_quantity_lo_e', 'IXN_lineorder_lo_quantity_lo_d', 'IXN_lineorder_lo_quantity_lo_e_lo_d', 'IX_lineorder_lo_orderdate_lo_discount', 'IXN_lineorder_lo_orderdate_lo_discount_lo_e', 'IX_lineorder_lo_orderdate_lo_quantity', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_d', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e_lo_d', 'IX_lineorder_lo_discount_lo_orderdate', 'IXN_lineorder_lo_discount_lo_orderdate_lo_e', 'IX_lineorder_lo_discount_lo_quantity', 'IXN_lineorder_lo_discount_lo_quantity_lo_e', 'IX_lineorder_lo_quantity_lo_orderdate', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_e', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_d', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_e_lo_d', 'IX_lineorder_lo_quantity_lo_discount', 'IXN_lineorder_lo_quantity_lo_discount_lo_e', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_discount_lo_e', 'IX_lineorder_lo_discount_lo_orderdate_lo_quantity', 'IXN_lineorder_lo_discount_lo_orderdate_lo_quantity_lo_e', 'IX_lineorder_lo_discount_lo_quantity_lo_orderdate', 'IXN_lineorder_lo_discount_lo_quantity_lo_orderdate_lo_e', 'IX_lineorder_lo_quantity_lo_orderdate_lo_discount', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_discount_lo_e', 'IX_lineorder_lo_quantity_lo_discount_lo_orderdate', 'IXN_lineorder_lo_quantity_lo_discount_lo_orderdate_lo_e', 'IX_dwdate_d_datekey', 'IX_dwdate_d_year', 'IX_dwdate_d_datekey_d_year', 'IX_dwdate_d_year_d_datekey', 'IX_dwdate_d_yearmonthnum', 'IX_dwdate_d_datekey_d_yearmonthnum', 'IX_dwdate_d_yearmonthnum_d_datekey']\n",
      "19 top indexes: ['IXN_lineorder_lo_orderdate_lo_quantity_lo_discount_lo_e', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e_lo_d', 'IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_discount_lo_e', 'IXN_lineorder_lo_quantity_lo_discount_lo_orderdate_lo_e', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_e_lo_d', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IXN_lineorder_lo_discount_lo_orderdate_lo_quantity_lo_e', 'IXN_lineorder_lo_discount_lo_quantity_lo_orderdate_lo_e', 'IX_lineorder_lo_orderdate_lo_discount', 'IXN_lineorder_lo_orderdate_lo_discount_lo_e', 'IX_lineorder_lo_orderdate_lo_quantity', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_d', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e', 'IX_dwdate_d_yearmonthnum_d_datekey', 'IX_dwdate_d_yearmonthnum', 'IX_dwdate_d_year_d_datekey', 'IX_dwdate_d_year']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 20\n",
      "Generating new partitions...\n",
      "Repartitioning...\n",
      "Analyzing query...\n",
      "\n",
      "\n",
      "\n",
      "Processing query 3\n",
      "Generating new partitions for query #3\n",
      "Extracted 42 indexes from query.\n",
      "Num candidate indexes (including those currently materialized), |U| = 48\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 48\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42_43_44_45_46_47\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Computing all pair degree of interaction...\n",
      "Updating statistics...\n",
      "Choosing top 19 indexes from 48 non-materialized candidate indexes\n",
      "Non-materialized candidate indexes, X = ['IX_lineorder_lo_orderdate', 'IXN_lineorder_lo_orderdate_lo_e', 'IXN_lineorder_lo_orderdate_lo_d', 'IXN_lineorder_lo_orderdate_lo_e_lo_d', 'IX_lineorder_lo_discount', 'IXN_lineorder_lo_discount_lo_e', 'IX_lineorder_lo_quantity', 'IXN_lineorder_lo_quantity_lo_e', 'IXN_lineorder_lo_quantity_lo_d', 'IXN_lineorder_lo_quantity_lo_e_lo_d', 'IX_lineorder_lo_orderdate_lo_discount', 'IXN_lineorder_lo_orderdate_lo_discount_lo_e', 'IX_lineorder_lo_orderdate_lo_quantity', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_d', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e_lo_d', 'IX_lineorder_lo_discount_lo_orderdate', 'IXN_lineorder_lo_discount_lo_orderdate_lo_e', 'IX_lineorder_lo_discount_lo_quantity', 'IXN_lineorder_lo_discount_lo_quantity_lo_e', 'IX_lineorder_lo_quantity_lo_orderdate', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_e', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_d', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_e_lo_d', 'IX_lineorder_lo_quantity_lo_discount', 'IXN_lineorder_lo_quantity_lo_discount_lo_e', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_discount_lo_e', 'IX_lineorder_lo_discount_lo_orderdate_lo_quantity', 'IXN_lineorder_lo_discount_lo_orderdate_lo_quantity_lo_e', 'IX_lineorder_lo_discount_lo_quantity_lo_orderdate', 'IXN_lineorder_lo_discount_lo_quantity_lo_orderdate_lo_e', 'IX_lineorder_lo_quantity_lo_orderdate_lo_discount', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_discount_lo_e', 'IX_lineorder_lo_quantity_lo_discount_lo_orderdate', 'IXN_lineorder_lo_quantity_lo_discount_lo_orderdate_lo_e', 'IX_dwdate_d_datekey', 'IX_dwdate_d_year', 'IX_dwdate_d_datekey_d_year', 'IX_dwdate_d_year_d_datekey', 'IX_dwdate_d_yearmonthnum', 'IX_dwdate_d_datekey_d_yearmonthnum', 'IX_dwdate_d_yearmonthnum_d_datekey', 'IX_dwdate_d_weeknuminyear', 'IX_dwdate_d_datekey_d_weeknuminyear', 'IX_dwdate_d_weeknuminyear_d_datekey']\n",
      "19 top indexes: ['IXN_lineorder_lo_orderdate_lo_quantity_lo_discount_lo_e', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e_lo_d', 'IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_discount', 'IXN_lineorder_lo_orderdate_lo_discount_lo_e', 'IX_lineorder_lo_orderdate_lo_quantity', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_d', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e', 'IXN_lineorder_lo_discount_lo_orderdate_lo_quantity_lo_e', 'IXN_lineorder_lo_discount_lo_quantity_lo_orderdate_lo_e', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_discount_lo_e', 'IXN_lineorder_lo_quantity_lo_discount_lo_orderdate_lo_e', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_e_lo_d', 'IX_lineorder_lo_orderdate', 'IXN_lineorder_lo_orderdate_lo_d', 'IXN_lineorder_lo_orderdate_lo_e', 'IXN_lineorder_lo_orderdate_lo_e_lo_d']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 20\n",
      "Generating new partitions...\n",
      "Repartitioning...\n",
      "Analyzing query...\n",
      "\n",
      "\n",
      "\n",
      "Processing query 4\n",
      "Generating new partitions for query #4\n",
      "Extracted 58 indexes from query.\n",
      "Num candidate indexes (including those currently materialized), |U| = 101\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 101\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42_43_44_45_46_47_48_49_50_51_52_53_54_55_56_57_58_59_60_61_62_63_64_65_66_67_68_69_70_71_72_73_74_75_76_77_78_79_80_81_82_83_84_85_86_87_88_89_90_91_92_93_94_95_96_97_98_99_100\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Computing all pair degree of interaction...\n",
      "Updating statistics...\n",
      "Choosing top 19 indexes from 101 non-materialized candidate indexes\n",
      "Non-materialized candidate indexes, X = ['IX_lineorder_lo_orderdate', 'IXN_lineorder_lo_orderdate_lo_e', 'IXN_lineorder_lo_orderdate_lo_d', 'IXN_lineorder_lo_orderdate_lo_e_lo_d', 'IX_lineorder_lo_discount', 'IXN_lineorder_lo_discount_lo_e', 'IX_lineorder_lo_quantity', 'IXN_lineorder_lo_quantity_lo_e', 'IXN_lineorder_lo_quantity_lo_d', 'IXN_lineorder_lo_quantity_lo_e_lo_d', 'IX_lineorder_lo_orderdate_lo_discount', 'IXN_lineorder_lo_orderdate_lo_discount_lo_e', 'IX_lineorder_lo_orderdate_lo_quantity', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_d', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e_lo_d', 'IX_lineorder_lo_discount_lo_orderdate', 'IXN_lineorder_lo_discount_lo_orderdate_lo_e', 'IX_lineorder_lo_discount_lo_quantity', 'IXN_lineorder_lo_discount_lo_quantity_lo_e', 'IX_lineorder_lo_quantity_lo_orderdate', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_e', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_d', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_e_lo_d', 'IX_lineorder_lo_quantity_lo_discount', 'IXN_lineorder_lo_quantity_lo_discount_lo_e', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_discount_lo_e', 'IX_lineorder_lo_discount_lo_orderdate_lo_quantity', 'IXN_lineorder_lo_discount_lo_orderdate_lo_quantity_lo_e', 'IX_lineorder_lo_discount_lo_quantity_lo_orderdate', 'IXN_lineorder_lo_discount_lo_quantity_lo_orderdate_lo_e', 'IX_lineorder_lo_quantity_lo_orderdate_lo_discount', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_discount_lo_e', 'IX_lineorder_lo_quantity_lo_discount_lo_orderdate', 'IXN_lineorder_lo_quantity_lo_discount_lo_orderdate_lo_e', 'IX_dwdate_d_datekey', 'IX_dwdate_d_year', 'IX_dwdate_d_datekey_d_year', 'IX_dwdate_d_year_d_datekey', 'IX_dwdate_d_yearmonthnum', 'IX_dwdate_d_datekey_d_yearmonthnum', 'IX_dwdate_d_yearmonthnum_d_datekey', 'IX_dwdate_d_weeknuminyear', 'IX_dwdate_d_datekey_d_weeknuminyear', 'IX_dwdate_d_weeknuminyear_d_datekey', 'IXN_lineorder_lo_orderdate_lo_r', 'IX_lineorder_lo_partkey', 'IXN_lineorder_lo_partkey_lo_r', 'IX_lineorder_lo_suppkey', 'IXN_lineorder_lo_suppkey_lo_r', 'IX_lineorder_lo_orderdate_lo_partkey', 'IXN_lineorder_lo_orderdate_lo_partkey_lo_r', 'IX_lineorder_lo_orderdate_lo_suppkey', 'IXN_lineorder_lo_orderdate_lo_suppkey_lo_r', 'IX_lineorder_lo_partkey_lo_orderdate', 'IXN_lineorder_lo_partkey_lo_orderdate_lo_r', 'IX_lineorder_lo_partkey_lo_suppkey', 'IXN_lineorder_lo_partkey_lo_suppkey_lo_r', 'IX_lineorder_lo_suppkey_lo_orderdate', 'IXN_lineorder_lo_suppkey_lo_orderdate_lo_r', 'IX_lineorder_lo_suppkey_lo_partkey', 'IXN_lineorder_lo_suppkey_lo_partkey_lo_r', 'IX_lineorder_lo_orderdate_lo_partkey_lo_suppkey', 'IXN_lineorder_lo_orderdate_lo_partkey_lo_suppkey_lo_r', 'IX_lineorder_lo_orderdate_lo_suppkey_lo_partkey', 'IXN_lineorder_lo_orderdate_lo_suppkey_lo_partkey_lo_r', 'IX_lineorder_lo_partkey_lo_orderdate_lo_suppkey', 'IXN_lineorder_lo_partkey_lo_orderdate_lo_suppkey_lo_r', 'IX_lineorder_lo_partkey_lo_suppkey_lo_orderdate', 'IXN_lineorder_lo_partkey_lo_suppkey_lo_orderdate_lo_r', 'IX_lineorder_lo_suppkey_lo_orderdate_lo_partkey', 'IXN_lineorder_lo_suppkey_lo_orderdate_lo_partkey_lo_r', 'IX_lineorder_lo_suppkey_lo_partkey_lo_orderdate', 'IXN_lineorder_lo_suppkey_lo_partkey_lo_orderdate_lo_r', 'IXN_dwdate_d_datekey_d_ye', 'IX_part_p_partkey', 'IXN_part_p_partkey_p_br', 'IX_part_p_category', 'IXN_part_p_category_p_br', 'IX_part_p_brand', 'IX_part_p_partkey_p_category', 'IXN_part_p_partkey_p_category_p_br', 'IX_part_p_partkey_p_brand', 'IX_part_p_category_p_partkey', 'IXN_part_p_category_p_partkey_p_br', 'IX_part_p_category_p_brand', 'IX_part_p_brand_p_partkey', 'IX_part_p_brand_p_category', 'IX_part_p_partkey_p_category_p_brand', 'IX_part_p_partkey_p_brand_p_category', 'IX_part_p_category_p_partkey_p_brand', 'IX_part_p_category_p_brand_p_partkey', 'IX_part_p_brand_p_partkey_p_category', 'IX_part_p_brand_p_category_p_partkey', 'IX_supplier_s_suppkey', 'IX_supplier_s_region', 'IX_supplier_s_suppkey_s_region', 'IX_supplier_s_region_s_suppkey']\n",
      "19 top indexes: ['IXN_lineorder_lo_orderdate_lo_quantity_lo_discount_lo_e', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e_lo_d', 'IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_discount', 'IXN_lineorder_lo_orderdate_lo_discount_lo_e', 'IX_lineorder_lo_orderdate_lo_quantity', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_d', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e', 'IXN_lineorder_lo_discount_lo_orderdate_lo_quantity_lo_e', 'IXN_lineorder_lo_discount_lo_quantity_lo_orderdate_lo_e', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_discount_lo_e', 'IXN_lineorder_lo_quantity_lo_discount_lo_orderdate_lo_e', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_e_lo_d', 'IXN_lineorder_lo_partkey_lo_orderdate_lo_suppkey_lo_r', 'IXN_lineorder_lo_partkey_lo_suppkey_lo_orderdate_lo_r', 'IX_lineorder_lo_orderdate', 'IXN_lineorder_lo_orderdate_lo_d']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 20\n",
      "Generating new partitions...\n",
      "Repartitioning...\n",
      "Analyzing query...\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instantiate WFIT\n",
    "C = extract_query_indexes(qg.generate_query(14), include_cols=True)  \n",
    "S_0 = C[0:1]\n",
    "wfit = WFIT(S_0, idxCnt=20, stateCnt=1000, histSize=100)\n",
    "\n",
    "# process the workload\n",
    "for i, query in enumerate(workload):\n",
    "    print(f\"Processing query {i+1}\")\n",
    "    wfit.process_WFIT(query, verbose=True)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{<pg_utils.Index at 0x7f5be826c890>}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
