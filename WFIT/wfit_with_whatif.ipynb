{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WFIT Algorithm Implementation (Schnaitter 2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "import IPython\n",
    "notebook_path = IPython.get_ipython().starting_dir\n",
    "target_subdirectory_path = os.path.abspath(os.path.join(os.path.dirname(notebook_path), 'PostgreSQL'))\n",
    "sys.path.append(target_subdirectory_path)\n",
    "\n",
    "from pg_utils import *\n",
    "from ssb_qgen_class import *\n",
    "\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "import time\n",
    "import random\n",
    "from more_itertools import powerset\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index Benefit Graph (IBG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, id, indexes):\n",
    "        self.id = id\n",
    "        self.indexes = indexes\n",
    "        self.children = []\n",
    "        self.parents = []\n",
    "        self.built = False\n",
    "        self.cost = None\n",
    "        self.used = None\n",
    "\n",
    "\n",
    "# class for creating and storing the IBG\n",
    "class IBG:\n",
    "    # Class-level cache\n",
    "    #_class_cache = {}\n",
    "\n",
    "    def __init__(self, query_object, C):\n",
    "        self.q = query_object\n",
    "        self.C = C\n",
    "        print(f\"Number of candidate indexes: {len(self.C)}\")\n",
    "        #print(f\"Candidate indexes: {self.C}\")\n",
    "        \n",
    "        # get hypothetical sizes of all the candidate indexes\n",
    "        print(\"Getting hypothetical sizes of candidate indexes...\")\n",
    "        self.get_hypo_sizes()\n",
    "\n",
    "        # map index_id to integer\n",
    "        self.idx2id = {index.index_id:i for i, index in enumerate(self.C)}\n",
    "        self.idx2index = {index.index_id:index for index in self.C}\n",
    "        #print(f\"Index id to integer mapping: {self.idx2id}\")\n",
    "        \n",
    "        # create a hash table for keeping track of all created nodes\n",
    "        self.nodes = {}\n",
    "        # create a root node\n",
    "        self.root = Node(self.get_configuration_id(self.C), self.C)\n",
    "        self.nodes[self.root.id] = self.root\n",
    "        print(f\"Created root node with id: {self.root.id}\")\n",
    "        \n",
    "        self.total_whatif_calls = 0\n",
    "        self.total_whatif_time = 0\n",
    "        self.node_count = 0\n",
    "\n",
    "        # start the IBG construction\n",
    "        print(\"Constructing IBG...\")\n",
    "        self.construct_ibg(self.root)\n",
    "        print(f\"Number of nodes in IBG: {len(self.nodes)}, Total number of what-if calls: {self.total_whatif_calls}, Time spent on what-if calls: {self.total_whatif_time}\")\n",
    "        # compute all pair degree of interaction\n",
    "        print(f\"Computing all pair degree of interaction...\")\n",
    "        start_time = time.time()\n",
    "        #self.doi = self.compute_all_pair_doi()\n",
    "        self.doi = self.compute_all_pair_doi_parallel(num_workers=10, max_nodes=180)\n",
    "        #self.doi = self.compute_all_pair_doi_simple()\n",
    "        #self.doi = self.compute_all_pair_doi_naive(num_samples=256)\n",
    "        end_time = time.time()\n",
    "        print(f\"Time spent on computing all pair degree of interaction: {end_time - start_time}\")\n",
    "\n",
    "    # assign unique string id to a configuration\n",
    "    def get_configuration_id(self, indexes):\n",
    "        # get sorted list of integer ids\n",
    "        ids = sorted([self.idx2id[idx.index_id] for idx in indexes])\n",
    "        return \"_\".join([str(i) for i in ids])\n",
    "    \n",
    "\n",
    "    # get hypothetical sizes of all the candidate indexes\n",
    "    def get_hypo_sizes(self):\n",
    "        conn = create_connection()\n",
    "        hypo_indexes = bulk_create_hypothetical_indexes(conn, self.C, return_size=True)\n",
    "        close_connection(conn)\n",
    "        \n",
    "        for i in range(len(hypo_indexes)):\n",
    "            self.C[i].size = hypo_indexes[i][1]\n",
    "        \n",
    "\n",
    "    def _get_cost_used(self, indexes):\n",
    "        # Convert indexes to a tuple to make it hashable\n",
    "        #indexes_tuple = tuple(sorted(indexes, key=lambda x: x.index_id))\n",
    "        # Check if the result is already in the class-level cache\n",
    "        #if indexes_tuple in self._class_cache:\n",
    "        #    return self._class_cache[indexes_tuple]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        conn = create_connection()\n",
    "        # create hypothetical indexes\n",
    "        hypo_indexes = bulk_create_hypothetical_indexes(conn, indexes)\n",
    "        # map oid to index object\n",
    "        oid2index = {}\n",
    "        for i in range(len(hypo_indexes)):\n",
    "            oid2index[hypo_indexes[i]] = indexes[i]\n",
    "        # get cost and used indexes\n",
    "        cost, indexes_used = get_query_cost_estimate_hypo_indexes(conn, self.q.query_string, show_plan=False)\n",
    "        # map used index oids to index objects\n",
    "        used = [oid2index[oid] for oid, scan_type, scan_cost in indexes_used]\n",
    "        # drop hypothetical indexes\n",
    "        bulk_drop_hypothetical_indexes(conn)\n",
    "        close_connection(conn)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Store the result in the class-level cache\n",
    "        #self._class_cache[indexes_tuple] = (cost, used)\n",
    "        self.total_whatif_calls += 1\n",
    "        self.total_whatif_time += end_time - start_time\n",
    "\n",
    "        return cost, used\n",
    "\n",
    "    # Ensure the indexes parameter is hashable\n",
    "    def _cached_get_cost_used(self, indexes):\n",
    "        return self._get_cost_used(tuple(indexes))\n",
    "\n",
    "    \n",
    "    # IBG construction\n",
    "    def construct_ibg(self, root, max_iters=None):\n",
    "        queue = deque([root])\n",
    "        \n",
    "        num_iters = 0\n",
    "        while queue:\n",
    "\n",
    "            Y = queue.popleft()\n",
    "            \n",
    "            if Y.built:\n",
    "                continue\n",
    "            \n",
    "            # obtain query optimizer's cost and used indexes\n",
    "            self.node_count += 1\n",
    "            print(f\"Creating node # {self.node_count}\", end=\"\\r\")\n",
    "            \n",
    "            cost, used = self._cached_get_cost_used(Y.indexes)\n",
    "            Y.cost = cost\n",
    "            Y.used = used\n",
    "            Y.built = True\n",
    "            \n",
    "            # create children\n",
    "            for a in Y.used:\n",
    "                # create a new configuration with index a removed from Y\n",
    "                X_indexes = [index for index in Y.indexes if index != a]\n",
    "                X_id = self.get_configuration_id(X_indexes)\n",
    "                \n",
    "                # if X is not in the hash table, create a new node and add it to the queue\n",
    "                if X_id not in self.nodes:\n",
    "                    X = Node(X_id, X_indexes)\n",
    "                    X.parents.append(Y)\n",
    "                    self.nodes[X_id] = X\n",
    "                    Y.children.append(X)\n",
    "                    queue.append(X)\n",
    "                else:\n",
    "                    X = self.nodes[X_id]\n",
    "                    Y.children.append(X)\n",
    "                    X.parents.append(Y)\n",
    "\n",
    "            num_iters += 1\n",
    "            if max_iters is not None and num_iters >= max_iters:\n",
    "                break        \n",
    "\n",
    "\n",
    "    # use IBG to obtain estimated cost and used indexes for arbitrary subset of C\n",
    "    def get_cost_used(self, X):\n",
    "        # get id of the configuration\n",
    "        id = self.get_configuration_id(X)\n",
    "        # check if the configuration is in the IBG\n",
    "        if id in self.nodes:\n",
    "            cost, used = self.nodes[id].cost, self.nodes[id].used\n",
    "        \n",
    "        # if not in the IBG, traverse the IBG to find a covering node\n",
    "        else:\n",
    "            Y = self.find_covering_node(X)              \n",
    "            cost, used = Y.cost, Y.used\n",
    "\n",
    "        return cost, used    \n",
    "\n",
    "\n",
    "    # traverses the IBG to find a node that removes indexes not in X (i.e. a covering node for X)\n",
    "    def find_covering_node(self, X):\n",
    "        X_indexes = set([index.index_id for index in X])\n",
    "        Y = self.root\n",
    "        Y_indexes = set([index.index_id for index in Y.indexes])\n",
    "        # traverse IBG to find covering node\n",
    "        while (len(Y_indexes - X_indexes) != 0) or (len(Y.children) > 0):               \n",
    "            # traverse down to the child node that removes an index not in X\n",
    "            child_found = False\n",
    "            for child in Y.children:\n",
    "                child_indexes = set([index.index_id for index in child.indexes])\n",
    "                child_indexes_removed = Y_indexes - child_indexes\n",
    "                child_indexes_removed_not_in_X = child_indexes_removed - X_indexes\n",
    "        \n",
    "                # check if child removes an index not in X\n",
    "                if len(child_indexes_removed_not_in_X) > 0:\n",
    "                    Y = child\n",
    "                    Y_indexes = child_indexes\n",
    "                    child_found = True\n",
    "                    break\n",
    "\n",
    "            # if no children remove indexes not in X    \n",
    "            if not child_found:\n",
    "                break    \n",
    "    \n",
    "        return Y        \n",
    "\n",
    "    # compute benefit of an index for a given configuration \n",
    "    # input X is a list of index objects and 'a' is a single index object\n",
    "    # X must not contain 'a'\n",
    "    def compute_benefit(self, a, X):\n",
    "        if a in X:\n",
    "            # zero benefit if 'a' is already in X\n",
    "            #raise ValueError(\"Index 'a' is already in X\")\n",
    "            return 0\n",
    "        \n",
    "        # get cost  for X\n",
    "        cost_X = self.get_cost_used(X)[0]\n",
    "        # create a new configuration with index a added to X\n",
    "        X_a = X + [a]\n",
    "        # get cost for X + {a}\n",
    "        cost_X_a = self.get_cost_used(X_a)[0]\n",
    "        # compute benefit\n",
    "        benefit = cost_X - cost_X_a\n",
    "        return benefit \n",
    "\n",
    "\n",
    "    # compute maximum benefit of adding an index to any possibe configuration\n",
    "    def compute_max_benefit(self, a):\n",
    "        max_benefit = float('-inf')\n",
    "        for id, node in self.nodes.items():\n",
    "            #print(f\"Computing benefit for node: {[index.index_id for index in node.indexes]}\")\n",
    "            benefit = self.compute_benefit(a, node.indexes)\n",
    "            if benefit > max_benefit:\n",
    "                max_benefit = benefit\n",
    "\n",
    "        return max_benefit\n",
    "    \n",
    "    # compute the degree of interaction between two indexes a,b in configuration X \n",
    "    def compute_doi_configuration(self, a, b, X=[], normalize=True):\n",
    "        # X must not contain a or b\n",
    "        if a in X or b in X:\n",
    "            raise ValueError(\"a or b is already in X\")\n",
    "\n",
    "        doi = abs(self.compute_benefit(a, X) - self.compute_benefit(a, X + [b]))\n",
    "        if normalize:\n",
    "            doi /= self.get_cost_used(X + [a,b])[0]   \n",
    "        return doi\n",
    "   \n",
    "    \n",
    "    # Cache the results of find_covering_node and get_cost_used to avoid redundant calculations\n",
    "    @lru_cache(maxsize=None)\n",
    "    def cached_find_covering_node(self, indexes):\n",
    "        return self.find_covering_node(tuple(indexes))\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def cached_get_cost_used(self, indexes):\n",
    "        return self.get_cost_used(tuple(indexes))\n",
    "\n",
    "\n",
    "\n",
    "    # computes the degree of interaction between all pairs of indexes (a,b) in candidate set C\n",
    "    # Note: doi is symmetric, i.e. doi(a,b) = doi(b,a)\n",
    "\n",
    "    # simple version of compute_all_pair_doi, without parallelization\n",
    "    def compute_all_pair_doi_simple(self):\n",
    "        # hash table for storing doi values\n",
    "        doi = {}\n",
    "        # intialize doi values to zero\n",
    "        for i in range(len(self.C)):\n",
    "            for j in range(i+1, len(self.C)):\n",
    "                d = self.compute_doi_configuration(self.C[i], self.C[j])\n",
    "                doi[(self.C[i].index_id, self.C[j].index_id)] = d\n",
    "                doi[(self.C[j].index_id, self.C[i].index_id)] = d\n",
    "\n",
    "        return doi\n",
    "\n",
    "    # Naive version of compute_all_pair_doi, with random sampling of configurations\n",
    "    def compute_all_pair_doi_naive(self, num_samples=100):\n",
    "        doi = {}\n",
    "        \n",
    "        for i in range(len(self.C)):\n",
    "            for j in range(i + 1, len(self.C)):\n",
    "                doi[(self.C[i].index_id, self.C[j].index_id)] = 0\n",
    "                doi[(self.C[j].index_id, self.C[i].index_id)] = 0\n",
    "        \n",
    "        # sample random configurations: X subset C (must include empty set configuration)\n",
    "        for i in tqdm(range(num_samples), desc=\"Sampling configurations\"):\n",
    "            if i == 0:\n",
    "                X = []\n",
    "            else:\n",
    "                X = random.sample(self.C, random.randint(1, len(self.C)))\n",
    "\n",
    "            # compute doi for all pairs (a, b) in U\\X \n",
    "            for i in range(len(self.C)):\n",
    "                for j in range(i+1, len(self.C)):\n",
    "                    a = self.C[i]\n",
    "                    b = self.C[j]\n",
    "                    if a not in X and b not in X:\n",
    "                        d = self.compute_doi_configuration(a, b, X)\n",
    "                        doi[(a.index_id, b.index_id)] = max(doi[(a.index_id, b.index_id)], d)\n",
    "                        doi[(b.index_id, a.index_id)] = max(doi[(b.index_id, a.index_id)], d)        \n",
    "        \n",
    "        return doi    \n",
    "\n",
    "    # original version of compute_all_pair_doi, with optional max_nodes parameter for random sampling of nodes for efficient approximation\n",
    "    def compute_all_pair_doi(self, max_nodes=None):\n",
    "        # hash table for storing doi values\n",
    "        doi = {}\n",
    "        # intialize doi values to zero\n",
    "        for i in range(len(self.C)):\n",
    "            for j in range(i+1, len(self.C)):\n",
    "                doi[(self.C[i].index_id, self.C[j].index_id)] = 0\n",
    "                doi[(self.C[j].index_id, self.C[i].index_id)] = 0\n",
    "\n",
    "        S_idxs = set([index.index_id for index in self.C])\n",
    "\n",
    "        # sample max_nodes number of nodes from the chunk\n",
    "        if max_nodes is not None:\n",
    "            nodes_sample = random.sample(self.nodes.values(), min(max_nodes, len(self.nodes)))\n",
    "        else:\n",
    "            nodes_sample = self.nodes.values()\n",
    "\n",
    "        # iterate over each IBG node\n",
    "        for Y in tqdm(nodes_sample, desc=\"Processing nodes\"):\n",
    "            \n",
    "            # remove Y.used from S\n",
    "            Y_idxs = set([index.index_id for index in Y.indexes])\n",
    "            used_Y = Y.used\n",
    "            Y_used_idxs = set([index.index_id for index in used_Y])\n",
    "            S_Y = list(S_idxs - Y_used_idxs)\n",
    "            # iterate over all pairs of indexes in S_Y\n",
    "            for i in range(len(S_Y)):\n",
    "                for j in range(i+1, len(S_Y)):\n",
    "                    a_idx = S_Y[i]\n",
    "                    b_idx = S_Y[j]\n",
    "                     \n",
    "                    # find Ya covering node in IBG\n",
    "                    Ya = (Y_idxs - {a_idx, b_idx}) | {a_idx}\n",
    "                    Ya = [self.idx2index[idx] for idx in Ya]\n",
    "                    Ya = self.cached_find_covering_node(tuple(Ya))\n",
    "                    # find Yab covering node in IBG\n",
    "                    Yab = (Y_idxs - {a_idx, b_idx}) | {a_idx, b_idx}\n",
    "                    Yab = [self.idx2index[idx] for idx in Yab]\n",
    "                    Yab = self.cached_find_covering_node(tuple(Yab))\n",
    "\n",
    "                    #used_Y = self.cached_get_cost_used(tuple(Y.indexes))[1]\n",
    "                    #used_Ya = self.cached_get_cost_used(tuple(Ya))[1]\n",
    "                    #used_Yab = self.cached_get_cost_used(tuple(Yab))[1]\n",
    "                    used_Ya = Ya.used\n",
    "                    used_Yab = Yab.used\n",
    "\n",
    "                    Uab = set([index.index_id for index in used_Y]) | set([index.index_id for index in used_Ya]) | set([index.index_id for index in used_Yab]) \n",
    "                    # find Yb_minus covering node in IBG \n",
    "                    Yb_minus = list((Uab - {a_idx, b_idx}) | {b_idx})\n",
    "                    Yb_minus = [self.idx2index[idx] for idx in Yb_minus]\n",
    "                    Yb_minus = self.cached_find_covering_node(tuple(Yb_minus))\n",
    "                    # find Yb_plus covering node in IBG\n",
    "                    Yb_plus = list((Y_idxs - {a_idx, b_idx}) | {b_idx})\n",
    "                    Yb_plus = [self.idx2index[idx] for idx in Yb_plus]\n",
    "                    Yb_plus = self.cached_find_covering_node(tuple(Yb_plus))\n",
    "\n",
    "                    # generate quadruples\n",
    "                    quadruples = [(Y.indexes, Ya.indexes, Yb_minus.indexes, Yab.indexes), (Y.indexes, Ya.indexes, Yb_plus.indexes, Yab.indexes)]\n",
    "\n",
    "                    # compute doi using the quadruples\n",
    "                    for Y_indexes, Ya_indexes, Yb_indexes, Yab_indexes in quadruples:\n",
    "                        cost_Y = self.cached_get_cost_used(tuple(Y_indexes))[0]\n",
    "                        cost_Ya = self.cached_get_cost_used(tuple(Ya_indexes))[0]\n",
    "                        cost_Yb = self.cached_get_cost_used(tuple(Yb_indexes))[0]\n",
    "                        cost_Yab = self.cached_get_cost_used(tuple(Yab_indexes))[0]\n",
    "                        # can ignore the normalization terms in denominator to get an absolute measure of doi\n",
    "                        d = abs(cost_Y - cost_Ya - cost_Yb + cost_Yab) / cost_Yab\n",
    "                        # save doi value for the pair\n",
    "                        doi[(a_idx,b_idx)] = max(doi[(a_idx,b_idx)], d)\n",
    "                        # save doi value for the symmetric pair\n",
    "                        doi[(b_idx,a_idx)] = max(doi[(b_idx,a_idx)], d)     \n",
    "                            \n",
    "        return doi\n",
    "\n",
    "\n",
    "    # parallelized version of compute_all_pair_doi\n",
    "    def compute_all_pair_doi_parallel(self, num_workers=16, max_nodes=None):\n",
    "        doi = {}\n",
    "        \n",
    "        for i in range(len(self.C)):\n",
    "            for j in range(i + 1, len(self.C)):\n",
    "                doi[(self.C[i].index_id, self.C[j].index_id)] = 0\n",
    "                doi[(self.C[j].index_id, self.C[i].index_id)] = 0\n",
    "        \n",
    "        S_idxs = set([index.index_id for index in self.C])\n",
    "        \n",
    "        if max_nodes is not None:\n",
    "            nodes_list = random.sample(list(self.nodes.values()), min(max_nodes, len(self.nodes)))\n",
    "        else:    \n",
    "            nodes_list = list(self.nodes.values())\n",
    "        \n",
    "        chunk_size = max(1, len(nodes_list) // num_workers)\n",
    "\n",
    "        chunks = [nodes_list[i:i + chunk_size] for i in range(0, len(nodes_list), chunk_size)]\n",
    "        \n",
    "        args = [(chunk, self.C, self.idx2index, S_idxs, self.cached_find_covering_node, self.cached_get_cost_used) for chunk in chunks]\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            results = list(tqdm(executor.map(process_node_chunk, args), total=len(chunks), desc=\"Processing nodes in parallel\"))\n",
    "        \n",
    "        for result in results:\n",
    "            for key, value in result.items():\n",
    "                doi[key] = max(doi.get(key, 0), value)\n",
    "        \n",
    "        return doi\n",
    "    \n",
    "    \n",
    "    # get precomputed degree of interaction between a pair of indexes\n",
    "    def get_doi_pair(self, a, b):\n",
    "            return self.doi[(a.index_id, b.index_id)]\n",
    "\n",
    "\n",
    "    # function for printing the IBG, using BFS level order traversal\n",
    "    def print_ibg(self):\n",
    "        q = [self.root]\n",
    "        # traverse level by level, print all node ids in a level in a single line before moving to the next level\n",
    "        while len(q) > 0:\n",
    "            next_q = []\n",
    "            for node in q:\n",
    "                print(f\"{node.id} -> \", end=\"\")\n",
    "                for child in node.children:\n",
    "                    next_q.append(child)\n",
    "            print()\n",
    "            q = next_q  \n",
    "\n",
    "\n",
    "def process_node_chunk(args):\n",
    "    nodes_chunk, C, idx2index, S_idxs, cached_find_covering_node, cached_get_cost_used = args\n",
    "    doi_chunk = {}\n",
    "    \n",
    "    for Y in nodes_chunk:\n",
    "        Y_idxs = set([index.index_id for index in Y.indexes])\n",
    "        used_Y = Y.used\n",
    "        Y_used_idxs = set([index.index_id for index in used_Y])\n",
    "        S_Y = list(S_idxs - Y_used_idxs)\n",
    "        \n",
    "        for i in range(len(S_Y)):\n",
    "            for j in range(i + 1, len(S_Y)):\n",
    "                a_idx = S_Y[i]\n",
    "                b_idx = S_Y[j]\n",
    "                \n",
    "                Ya = (Y_idxs - {a_idx, b_idx}) | {a_idx}\n",
    "                Ya = [idx2index[idx] for idx in Ya]\n",
    "                Ya = cached_find_covering_node(tuple(Ya))\n",
    "                \n",
    "                Yab = (Y_idxs - {a_idx, b_idx}) | {a_idx, b_idx}\n",
    "                Yab = [idx2index[idx] for idx in Yab]\n",
    "                Yab = cached_find_covering_node(tuple(Yab))\n",
    "                \n",
    "                used_Ya = Ya.used\n",
    "                used_Yab = Yab.used\n",
    "                \n",
    "                Uab = set([index.index_id for index in used_Y]) | set([index.index_id for index in used_Ya]) | set([index.index_id for index in used_Yab])\n",
    "                \n",
    "                Yb_minus = list((Uab - {a_idx, b_idx}) | {b_idx})\n",
    "                Yb_minus = [idx2index[idx] for idx in Yb_minus]\n",
    "                Yb_minus = cached_find_covering_node(tuple(Yb_minus))\n",
    "                \n",
    "                Yb_plus = list((Y_idxs - {a_idx, b_idx}) | {b_idx})\n",
    "                Yb_plus = [idx2index[idx] for idx in Yb_plus]\n",
    "                Yb_plus = cached_find_covering_node(tuple(Yb_plus))\n",
    "                \n",
    "                quadruples = [(Y.indexes, Ya.indexes, Yb_minus.indexes, Yab.indexes), (Y.indexes, Ya.indexes, Yb_plus.indexes, Yab.indexes)]\n",
    "                \n",
    "                for Y_indexes, Ya_indexes, Yb_indexes, Yab_indexes in quadruples:\n",
    "                    cost_Y = cached_get_cost_used(tuple(Y_indexes))[0]\n",
    "                    cost_Ya = cached_get_cost_used(tuple(Ya_indexes))[0]\n",
    "                    cost_Yb = cached_get_cost_used(tuple(Yb_indexes))[0]\n",
    "                    cost_Yab = cached_get_cost_used(tuple(Yab_indexes))[0]\n",
    "                    \n",
    "                    d = abs(cost_Y - cost_Ya - cost_Yb + cost_Yab) / cost_Yab\n",
    "                    doi_chunk[(a_idx, b_idx)] = max(doi_chunk.get((a_idx, b_idx), 0), d)\n",
    "                    doi_chunk[(b_idx, a_idx)] = max(doi_chunk.get((b_idx, a_idx), 0), d)\n",
    "    \n",
    "    return doi_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an SSB query generator object\n",
    "qg = QGEN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "template id: 1, query: \n",
      "                SELECT SUM(lo_extendedprice * lo_discount) AS revenue\n",
      "                FROM lineorder, dwdate\n",
      "                WHERE lo_orderdate = d_datekey\n",
      "                AND d_year = 1993\n",
      "                AND lo_discount BETWEEN 5 AND 7 \n",
      "                AND lo_quantity < 46;\n",
      "            , payload: {'lineorder': ['lo_extendedprice', 'lo_discount']}, predicates: {'lineorder': ['lo_orderdate', 'lo_discount', 'lo_quantity'], 'dwdate': ['d_datekey', 'd_year']}, order by: {}, group by: {}\n",
      "Number of candidate indexes: 19\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of nodes in IBG: 4, Total number of what-if calls: 4, Time spent on what-if calls: 0.17232346534729004\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 4/4 [00:00<00:00, 21788.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent on computing all pair degree of interaction: 0.023831844329833984\n",
      "0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18 -> \n",
      "0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17 -> \n",
      "0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_17 -> \n",
      "0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15 -> \n",
      "IBG     --> Cost: 1442088.69, Used indexes: ['IX_dwdate_d_datekey_d_year']\n",
      "What-if --> Cost: 1442088.69, Used indexes: ['IX_dwdate_d_datekey_d_year']\n",
      "\n",
      "Maximum benefit of adding index IX_lineorder_lo_orderdate: 0\n",
      "\n",
      "DOI between indexes IX_lineorder_lo_orderdate and IX_lineorder_lo_orderdate_lo_quantity : 0.0\n",
      "in configuration ['IX_lineorder_lo_discount', 'IX_lineorder_lo_quantity', 'IX_lineorder_lo_discount_lo_orderdate', 'IX_lineorder_lo_discount_lo_quantity', 'IX_lineorder_lo_quantity_lo_discount']\n",
      "\n",
      "DOI between indexes IX_lineorder_lo_orderdate and IX_lineorder_lo_orderdate_lo_quantity : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test IBG \n",
    "\n",
    "query = qg.generate_query(1)\n",
    "print(query)\n",
    "\n",
    "C = extract_query_indexes(qg.generate_query(1), include_cols=False)  \n",
    "\n",
    "ibg = IBG(query, C)\n",
    "\n",
    "ibg.print_ibg()\n",
    "\n",
    "# pick random subset of candidate indexes\n",
    "X = random.sample(ibg.C, 8)\n",
    "cost, used = ibg.get_cost_used(X)\n",
    "print(f\"IBG     --> Cost: {cost}, Used indexes: {[idx.index_id for idx in used]}\")\n",
    "\n",
    "cost, used = ibg._cached_get_cost_used(X)\n",
    "print(f\"What-if --> Cost: {cost}, Used indexes: {[idx.index_id for idx in used]}\")\n",
    "\n",
    "# pick two indexes and a configuration\n",
    "a = ibg.C[0]\n",
    "b = ibg.C[4] \n",
    "X = [ibg.C[1], ibg.C[2], ibg.C[5], ibg.C[6], ibg.C[8]]\n",
    "\n",
    "# compute maximum benefit of adding index 'a' \n",
    "max_benefit = ibg.compute_max_benefit(a)\n",
    "print(f\"\\nMaximum benefit of adding index {a.index_id}: {max_benefit}\")\n",
    "\n",
    "# compute degree of interaction between indexes 'a' and 'b' in configuration X\n",
    "doi = ibg.compute_doi_configuration(a, b, X)\n",
    "print(f\"\\nDOI between indexes {a.index_id} and {b.index_id} : {doi}\")\n",
    "print(f\"in configuration {[idx.index_id for idx in X]}\")\n",
    "\n",
    "# compute configuration independent degree of interaction between indexes 'a' and 'b'\n",
    "doi = ibg.get_doi_pair(a, b)\n",
    "print(f\"\\nDOI between indexes {a.index_id} and {b.index_id} : {doi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for key, value in ibg.doi.items():\n",
    "#    print(f\"doi({key[0]},   {key[1]}) = {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WFIT class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WFIT:\n",
    "\n",
    "    def __init__(self, S_0=[], max_key_columns=None, include_cols=False, max_U=60, idxCnt=50, stateCnt=200, histSize=100, rand_cnt=1):\n",
    "        # initial set of materialzed indexes\n",
    "        self.S_0 = S_0\n",
    "        # maximum number of key columns in an index\n",
    "        self.max_key_columns = max_key_columns\n",
    "        # allow include columns in indexes\n",
    "        self.include_cols = include_cols\n",
    "        # maximum number of candidate indexes for IBG \n",
    "        self.max_U = max_U\n",
    "        # parameter for maximum number of candidate indexes tracked \n",
    "        self.idxCnt = idxCnt\n",
    "        # parameter for maximum number of MTS states/configurations\n",
    "        self.stateCnt = stateCnt\n",
    "        # parameter for maximum number of historical index statistics kept\n",
    "        self.histSize = histSize\n",
    "        # parameter for number of randomized clustering iterations\n",
    "        self.rand_cnt = rand_cnt\n",
    "        # growing list of candidate indexes (initially contains S_0)\n",
    "        self.U = {index.index_id:index for index in S_0}\n",
    "        # index benefit and interaction statistics\n",
    "        self.idxStats = defaultdict(list)\n",
    "        self.intStats = defaultdict(list)\n",
    "        # list of currently monitored indexes\n",
    "        self.C = {index.index_id:index for index in S_0} \n",
    "        # list of currently materialized indexes\n",
    "        self.M = {index.index_id:index for index in S_0}  \n",
    "        # initialize stable partitions (each partition is a singleton set of indexes from S_0)\n",
    "        self.stable_partitions = [[index] for index in S_0]\n",
    "        self.n_pos = 0\n",
    "\n",
    "        print(f\"##################################################################\")\n",
    "        # initialize work function instance for each stable partition\n",
    "        self.W = self.initilize_WFA(self.stable_partitions)\n",
    "        # initialize current recommendations for each stable partition\n",
    "        self.current_recommendations = {i:indexes for i, indexes in enumerate(self.stable_partitions)}\n",
    "\n",
    "\n",
    "        print(f\"Initial set of materialized indexes: {[index.index_id for index in S_0]}\")\n",
    "        print(f\"Stable partitions: {[[index.index_id for index in P] for P in self.stable_partitions]}\")\n",
    "        print(f\"Initial work function instances: \")\n",
    "        for i, wf in self.W.items():\n",
    "            print(f\"\\tWFA Instance #{i}: {wf}\")\n",
    "\n",
    "        print(f\"\\nMaximum number of candidate indexes tracked: {idxCnt}\")\n",
    "        print(f\"Maximum number of MTS states/configurations: {stateCnt}\")\n",
    "        print(f\"Maximum number of historical index statistics kept: {histSize}\")\n",
    "        print(f\"Number of randomized clustering iterations: {rand_cnt}\")\n",
    "        print(f\"##################################################################\\n\")\n",
    "\n",
    "        # set random seed\n",
    "        random.seed(1234)\n",
    "\n",
    "        # track total cost \n",
    "        self.total_cost_wfit = 0\n",
    "        self.total_cost_simple = 0\n",
    "        self.total_no_index_cost = 0\n",
    "\n",
    "\n",
    "    # initialize a WFA instance for each stable partition\n",
    "    def initilize_WFA(self, stable_partitions):\n",
    "        print(f\"Initializing WFA instances for {len(stable_partitions)} stable partitions...\")\n",
    "        W = {}\n",
    "        for i, P in enumerate(stable_partitions):\n",
    "            # initialize all MTS states, i.e. power set of indexes in the partition\n",
    "            states = [tuple(sorted(state, key=lambda x: x.index_id)) for state in powerset(P)]\n",
    "            # initialize work function instance for the partition\n",
    "            W[i] = {tuple(X):self.compute_transition_cost(self.S_0, X) for X in states}    \n",
    "            \n",
    "        return W\n",
    "\n",
    "\n",
    "    # update WFIT step for next query in workload (this is the MAIN INTERFACE for generating an index configuration recommendation)\n",
    "    def process_WFIT(self, query_object, remove_stale_U=True, remove_stale_freq=1, verbose=False):\n",
    "        self.n_pos += 1        \n",
    "        previous_config = list(self.M.values())\n",
    "\n",
    "        # generate new partitions \n",
    "        if verbose: print(f\"Generating new partitions for query #{self.n_pos}\")\n",
    "        start_time_1 = time.time()\n",
    "        new_partitions, need_to_repartition, ibg = self.choose_candidates(self.n_pos, query_object, verbose)\n",
    "        end_time_1 = time.time()\n",
    "\n",
    "        # repartition if necessary\n",
    "        start_time_2 = time.time()\n",
    "        if need_to_repartition:\n",
    "            if verbose: print(f\"Repartitioning...\")\n",
    "            self.repartition(new_partitions, verbose)\n",
    "        end_time_2 = time.time()\n",
    "        \n",
    "        # analyze the query\n",
    "        if verbose: print(f\"Analyzing query...\")\n",
    "        start_time_3 = time.time()\n",
    "        self.analyze_query(query_object, ibg, verbose)\n",
    "        end_time_3 = time.time()    \n",
    "\n",
    "        if verbose: \n",
    "            print(f\"New indexes added this round: {[index.index_id for index in (set(self.M.values()) - set(previous_config))]}\")\n",
    "            print(f\"Currently materialized indexes: {[index.index_id for index in self.M.values()]}\") \n",
    "\n",
    "        # remove stale indexes from U\n",
    "        if remove_stale_U and (self.n_pos % remove_stale_freq == 0):\n",
    "            if verbose: print(f\"Removing stale indexes from U...\")\n",
    "            self.remove_stale_indexes_U(verbose)\n",
    "\n",
    "        # simple recommendation, just the used indexes in the IBG root node\n",
    "        self.get_simple_recommendation_ibg(ibg)\n",
    "\n",
    "        # compute hypothetical speedup from switching to new configuration\n",
    "        new_config = list(self.M.values())\n",
    "        conn = create_connection()\n",
    "        speedup_wfit, query_execution_cost_wfit = hypo_query_speedup(conn, query_object, previous_config, new_config)\n",
    "        self.total_cost_wfit += query_execution_cost_wfit + sum([self.get_index_creation_cost(index) for index in (set(new_config) - set(previous_config))])    \n",
    "        # also compute speed up for the simple recommendation\n",
    "        speedup_simple, query_execution_cost_simple = hypo_query_speedup(conn, query_object, previous_config, list(ibg.root.used))\n",
    "        self.total_cost_simple += query_execution_cost_simple + sum([self.get_index_creation_cost(index) for index in (set(ibg.root.used) - set(previous_config))])\n",
    "        # estimates cost for executing the query without any indexes\n",
    "        self.total_no_index_cost += hypo_query_cost(conn, query_object, [])\n",
    "        close_connection(conn)   \n",
    "\n",
    "        print(f\"*** Speedup WFIT: {speedup_wfit}, Speedup Simple: {speedup_simple}\")\n",
    "        print(f\"*** Total cost WFIT: {self.total_cost_wfit}, Total cost Simple: {self.total_cost_simple}, WFIT/Simple: {self.total_cost_wfit/self.total_cost_simple}\")\n",
    "        print(f\"*** Total cost without any indexes: {self.total_no_index_cost}\")\n",
    "\n",
    "        print(f\"Total recommendation time taken for query #{self.n_pos}: {end_time_3 - start_time_1} seconds\")\n",
    "        print(f\"(Partitioning: {end_time_1 - start_time_1} seconds, Repartitioning: {end_time_2 - start_time_2} seconds, Analyzing: {end_time_3 - start_time_3} seconds)\")\n",
    "\n",
    "\n",
    "    # Simple baseline recommendation: just the used indexes in the IBG root node, i.e. these are the indexes from \n",
    "    # the full set of candidate indexes which are used in the query plan\n",
    "    def get_simple_recommendation_ibg(self, ibg):\n",
    "        simple_recommendation = ibg.root.used  \n",
    "        wfit_recommendation = [index.index_id for i in self.current_recommendations for index in self.current_recommendations[i]]\n",
    "        print(f\"*** WFIT recommendation: {sorted(wfit_recommendation)}\")\n",
    "        print(f\"*** Simple recommendation: {sorted([index.index_id for index in simple_recommendation])}\") \n",
    "\n",
    "\n",
    "    # check for stale indexes in U and remove them\n",
    "    def remove_stale_indexes_U(self, verbose, min_overlapping_columns=0):\n",
    "        # find out which indexes have loweest benefit statistics\n",
    "        avg_benefit = {}\n",
    "        for index_id in self.U:\n",
    "            # compute average benefit of the index from all stats\n",
    "            avg_benefit[index_id] = sum([stat[1] for stat in self.idxStats[index_id]]) / len(self.idxStats[index_id])\n",
    "\n",
    "            \n",
    "        # sort indexes by average benefit\n",
    "        sorted_indexes = sorted(avg_benefit, key=avg_benefit.get, reverse=True)\n",
    "\n",
    "        # mark all indexes with zero benefit and not in M and S_0 as stale\n",
    "        stale_indexes = set()\n",
    "        for index_id in sorted_indexes:\n",
    "            if avg_benefit[index_id] == 0 and index_id not in self.M and index_id not in self.S_0:\n",
    "                stale_indexes.add(index_id)\n",
    "\n",
    "        # remove stale indexes from U\n",
    "        print(f\"Number of indexes in U: {len(self.U)}\")\n",
    "        num_removed = 0\n",
    "        for index_id in stale_indexes:\n",
    "            #if verbose: print(f\"Removing stale index: {index_id}\")\n",
    "            del self.U[index_id]\n",
    "            #if verbose: print(f\"Number of indexes in U after removal: {len(self.U)}\")\n",
    "            num_removed += 1\n",
    "\n",
    "        # keep at most self.max_U of highest benefit indexes in U\n",
    "        if len(self.U) > self.max_U:\n",
    "            self.U = {index_id:index for index_id, index in self.U.items() if index_id in sorted_indexes[:self.max_U]}\n",
    "\n",
    "        if verbose:\n",
    "            #print(f\"Average benefit of indexes:\")\n",
    "            #for index_id in sorted_indexes:\n",
    "            #    print(f\"\\tIndex {index_id}: {avg_benefit[index_id]}, Stale: {index_id in stale_indexes}\")\n",
    "                \n",
    "            print(f\"Number of indexes removed: {num_removed}, Number of indexes remaining: {len(self.U)}\")\n",
    "            #print(f\"Indexes in U: {self.U.keys()}\")\n",
    "                \n",
    "\n",
    "    # repartition the stable partitions based on the new partitions\n",
    "    def repartition(self, new_partitions, verbose):\n",
    "        # all indexes recommmendations across the WFA instances from previous round\n",
    "        S_curr = set(chain(*self.current_recommendations.values()))\n",
    "        C = set(self.C.values()) \n",
    "        S_0 = set(self.S_0)\n",
    "\n",
    "        # compute L2-norm of the work function across all partitions\n",
    "        #l2_norm_wf_old = 0\n",
    "        #for i, wf in self.W.items():\n",
    "        #    l2_norm_wf_old += sum([wf[X]**2 for X in wf])\n",
    "\n",
    "        \n",
    "        # re-initizlize WFA instances and recommendations for each new partition\n",
    "        if verbose: print(f\"Reinitializing WFA instances...\")\n",
    "        W = {}\n",
    "        recommendations = {}\n",
    "        for i, P in enumerate(new_partitions):\n",
    "            partition_all_configs = [tuple(sorted(state, key=lambda x: x.index_id)) for state in powerset(P)]\n",
    "            wf = {}\n",
    "            # initialize work function values for each state\n",
    "            for X in partition_all_configs:\n",
    "                wf_x = 0\n",
    "                for j, wf_prev in self.W.items(): \n",
    "                    wf_x += wf_prev[tuple(sorted(set(X) & set(self.stable_partitions[j]), key=lambda x: x.index_id))]\n",
    "                \n",
    "                # add transition cost to the work function value (not sure if intersection with S_0 is correct or not..)\n",
    "                wf[X] = wf_x + self.compute_transition_cost(S_0 & (set(P) - C), set(X) - C)\n",
    "            \n",
    "            W[i] = wf\n",
    "            # initialize current state/recommended configuration of the WFA instance\n",
    "            recommendations[i] = list(set(P) & S_curr)\n",
    "\n",
    "        \"\"\"\n",
    "        # compute l2 norm of the work function across all partitions\n",
    "        l2_norm_wf_new = 0\n",
    "        for i, wf in W.items():\n",
    "            l2_norm_wf_new += sum([wf[X]**2 for X in wf])\n",
    "        # rescale work function values to maintain the same l2 norm (otherwise wf values will keep increasing\n",
    "        # due to the summation terms in repartitioning)\n",
    "        for i, wf in W.items():\n",
    "            for X in wf:\n",
    "                wf[X] = wf[X] * (l2_norm_wf_old / l2_norm_wf_new)    \n",
    "        \"\"\"\n",
    "\n",
    "        # replace current stable partitions, WFA instances and recommendations with the new ones\n",
    "        self.stable_partitions = new_partitions\n",
    "        self.W = W\n",
    "        self.current_recommendations = recommendations\n",
    "        if verbose: \n",
    "            print(f\"Replaced stable partitions, WFA instances and recommendations with new ones\")\n",
    "            #print(f\"New WFA instances:\")\n",
    "            #for i, wf in self.W.items():\n",
    "            #    print(f\"\\tWFA Instance #{i}: {wf}\")\n",
    "\n",
    "        self.C = {}\n",
    "        for P in self.stable_partitions:\n",
    "            for index in P: \n",
    "                self.C[index.index_id] = index      \n",
    "\n",
    "\n",
    "    # update WFA instance on each stable partition and get index configuration recommendation\n",
    "    def analyze_query(self, query_object, ibg, verbose):\n",
    "        new_recommendations = {}\n",
    "        # update WFA instance for each stable partition\n",
    "        for i in self.W:\n",
    "            if verbose: print(f\"Updating WFA instance: {i}\")\n",
    "            self.W[i], new_recommendations[i]  = self.process_WFA(query_object, self.W[i], self.current_recommendations[i], ibg, verbose)\n",
    "\n",
    "            # materialize new recommendation\n",
    "            indexes_added = set(new_recommendations[i]) - set(self.current_recommendations[i])\n",
    "            indexes_removed = set(self.current_recommendations[i]) - set(new_recommendations[i])\n",
    "            if verbose: print(f\"\\tWFA Instance #{i}, Num States: {len(self.W[i])}, New Recommendation: {[index.index_id for index in new_recommendations[i]]} --> Indexes Added: {[index.index_id for index in indexes_added]}, Indexes Removed: {[index.index_id for index in indexes_removed]}\")\n",
    "            \n",
    "            for index in indexes_added:\n",
    "                self.M[index.index_id] = index\n",
    "            for index in indexes_removed:\n",
    "                del self.M[index.index_id]    \n",
    "                \n",
    "            self.current_recommendations[i] = new_recommendations[i]\n",
    "\n",
    "            # TODO: need to implement the following function in pg_utils\n",
    "            # ... materialize_configuration(connection, indexes_added, indexes_removed)\n",
    "            \n",
    "\n",
    "\n",
    "    # update a WFA instance for the given query    \n",
    "    def process_WFA(self, query_object, wf, S_current, ibg, verbose):\n",
    "        # update work function values for each state in the WFA instance\n",
    "        wf_new = {}\n",
    "        p = {}\n",
    "        scores = {}\n",
    "        best_score = float('inf')\n",
    "        best_state = None\n",
    "        for Y in wf.keys():\n",
    "            # compute new work function value for state Y \n",
    "            min_wf_value = float('inf')\n",
    "            min_p = None\n",
    "            for X in wf.keys():\n",
    "                sorted_X = tuple(sorted(X, key=lambda x: x.index_id))\n",
    "                sorted_Y = tuple(sorted(Y, key=lambda x: x.index_id))\n",
    "                wf_value = wf[sorted_X] + ibg.get_cost_used(list(sorted_X))[0] + self.compute_transition_cost(sorted_X, sorted_Y)\n",
    "                if wf_value < min_wf_value:\n",
    "                    min_wf_value = wf_value\n",
    "                    min_p = sorted_X\n",
    "\n",
    "            wf_new[sorted_Y] = min_wf_value\n",
    "            p[sorted_Y] = min_p\n",
    "\n",
    "            # compute score for the state    \n",
    "            score = min_wf_value + self.compute_transition_cost(sorted_Y, S_current)\n",
    "            scores[sorted_Y] = score\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_state = min_p\n",
    "\n",
    "        if verbose:\n",
    "            #print(f\"Work function values for WFA instance:\")\n",
    "            #for Y, value in wf_new.items():\n",
    "            #    print(f\"\\tstate :{Y} , w_value: {value}, p: {p[Y]}, score: {scores[Y]}\")\n",
    "            print(f\"Best state: {best_state}, Best score: {best_score}\")\n",
    "        \n",
    "\n",
    "        return wf_new, best_state\n",
    "\n",
    "    # compute index benefit graph for the given query and candidate indexes\n",
    "    def compute_IBG(self, query_object, candidate_indexes):\n",
    "        return IBG(query_object, candidate_indexes)\n",
    "    \n",
    "\n",
    "    # extract candidate indexes from given query\n",
    "    def extract_indexes(self, query_object):\n",
    "        return extract_query_indexes(query_object,  self.max_key_columns, self.include_cols)\n",
    "\n",
    "\n",
    "    # generate stable partitions/sets of indexes for next query in workload\n",
    "    def choose_candidates(self, n_pos, query_object, verbose):\n",
    "        # extract new candidate indexes from the query\n",
    "        new_indexes = self.extract_indexes(query_object)\n",
    "        # add new indexes to the list of all candidate indexes\n",
    "        num_new = 0\n",
    "        for index in new_indexes:\n",
    "            if index.index_id not in self.U:\n",
    "                self.U[index.index_id] = index\n",
    "                num_new += 1\n",
    "\n",
    "        #if len(self.U) > self.max_U:\n",
    "        #    raise ValueError(\"Number of candidate indexes exceeds the maximum limit. Aborting WFIT...\")\n",
    "\n",
    "\n",
    "        if verbose: \n",
    "            print(f\"Extracted {num_new} new indexes from query.\")\n",
    "            print(f\"Candidate indexes (including those currently materialized), |U| = {len(self.U)}\")\n",
    "            #print(f\"{[index.index_id for index in self.U.values()]}\")\n",
    "\n",
    "        # TODO: need mechanism to evict indexes from U that may have gone \"stale\" to prevent unbounded growth of U\n",
    "\n",
    "        \n",
    "        # compute index benefit graph for the query\n",
    "        if verbose: print(f\"Computing IBG...\")\n",
    "        ibg = self.compute_IBG(query_object, list(self.U.values()))\n",
    "\n",
    "        #if verbose: print(f\"Candidate index sizes in Mb: {[(index.index_id,index.size) for index in self.U.values()]}\")\n",
    "        \n",
    "        # update statistics for the candidate indexes (n_pos is the position of the query in the workload sequence)\n",
    "        if verbose: print(f\"Updating statistics...\")\n",
    "        self.update_stats(n_pos, ibg, verbose=False)\n",
    "\n",
    "        # non-materialized candidate indexes \n",
    "        X = [self.U[index_id] for index_id in self.U if index_id not in self.M]\n",
    "        num_indexes = self.idxCnt - len(self.M)\n",
    "\n",
    "        # determine new set of candidate indexes to monitor for upcoming workload queries\n",
    "        if verbose: print(f\"Choosing top {num_indexes} indexes from {len(X)} non-materialized candidate indexes\")\n",
    "        top_indexes = self.top_indexes(n_pos, X, num_indexes, verbose)\n",
    "        D = self.M | top_indexes\n",
    "        if verbose: print(f\"New set of indexes to monitor for upcoming workload, |D| = {len(D)}\")\n",
    "\n",
    "        # generate new partitions by clustering the new candidate set\n",
    "        if verbose: print(f\"Choosing new partitions...\")\n",
    "        new_partitions, need_to_repartition = self.choose_partition(n_pos, D, verbose)\n",
    "        if verbose:\n",
    "            print(f\"Old partitions:\")\n",
    "            for P in self.stable_partitions:\n",
    "                print(f\"\\t{[index.index_id for index in P]}\")\n",
    "            print(\"New partitions:\")\n",
    "            for P in new_partitions:\n",
    "                print(f\"\\t{[index.index_id for index in P]}\")    \n",
    "\n",
    "        return new_partitions, need_to_repartition, ibg\n",
    "    \n",
    "\n",
    "    # partition the new candidate set into clusters \n",
    "    # (need to optimize this function, currently it is a naive implementation)\n",
    "    def choose_partition(self, N_workload, D, verbose):\n",
    "        \n",
    "        # compute total loss, i.e. sum of doi across indexes from pairs of partitions\n",
    "        def compute_loss(P, current_doi):\n",
    "            loss = 0\n",
    "            for i in range(len(P)):\n",
    "                for j in range(i+1, len(P)):\n",
    "                    for a in P[i]:\n",
    "                        for b in P[j]:\n",
    "                            loss += current_doi[(a.index_id, b.index_id)]\n",
    "            return loss\n",
    "        \n",
    "        # compute current doi values for all pairs of indexes in U\n",
    "        current_doi = defaultdict(int)\n",
    "        for (a_idx, b_idx) in self.intStats.keys():\n",
    "            # take max over incremental averages (optimistic estimate)\n",
    "            current_doi[(a_idx, b_idx)] = 0\n",
    "            doi_total = 0\n",
    "            for (n, doi) in self.intStats[(a_idx, b_idx)]:\n",
    "                doi_total += doi\n",
    "                doi_avg = doi_total / (N_workload-n+1)\n",
    "                current_doi[(a_idx, b_idx)] = max(current_doi[(a_idx, b_idx)], doi_avg)\n",
    "            # save symmetric doi value\n",
    "            current_doi[(b_idx, a_idx)] = current_doi[(a_idx, b_idx)]    \n",
    "\n",
    "        #if verbose:\n",
    "        #    print(\"Current degree of interaction:\")\n",
    "        #    for pair, doi in current_doi.items():\n",
    "        #        print(f\"\\tPair {pair}: {doi}\")     \n",
    "\n",
    "        # from each current stable partition, remove indexes not in D\n",
    "        P = []\n",
    "        for partition in self.stable_partitions:\n",
    "            P.append([index for index in partition if index.index_id in D])\n",
    "\n",
    "        # add a singleton partition containing each new index in D not in C\n",
    "        need_to_repartition = False\n",
    "        for index_id, index in D.items():\n",
    "            if index_id not in self.C:\n",
    "                P.append([index])\n",
    "                need_to_repartition = True\n",
    "        \n",
    "        # set the new partition as baseline solution if feasible\n",
    "        total_configurations = sum([2**len(partition) for partition in P])\n",
    "        if total_configurations <= self.stateCnt:\n",
    "            bestSolution = P\n",
    "            bestLoss = compute_loss(P, current_doi)\n",
    "        else:\n",
    "            bestSolution = None\n",
    "            bestLoss = float('inf')    \n",
    "\n",
    "        # perform randomized clustering to find better solution\n",
    "        for i in range(self.rand_cnt):\n",
    "            # create partition of D in singletons\n",
    "            P = [[index] for index in D.values()]\n",
    "            partition2id = {tuple(partition):i for i, partition in enumerate(P)}\n",
    "            loss_cache = {}\n",
    "            \n",
    "            #if verbose:\n",
    "            #    print(f\"Parition to id map: {partition2id}\")\n",
    "\n",
    "            # merge singletons until only one partition remains\n",
    "            while True:\n",
    "                # find all feasible merge candidates pairs (i.e. pairs with loss > 0 and 2^(|Pi|+|Pj|) <= stateCnt)\n",
    "                E = []\n",
    "                E1 = []\n",
    "\n",
    "                # get loss for all pairs of partitions\n",
    "                total_configurations = sum([2**len(partition) for partition in P])\n",
    "                for i in range(len(P)):\n",
    "                    for j in range(i+1, len(P)):\n",
    "                        Pi_id = partition2id[tuple(P[i])]\n",
    "                        Pj_id = partition2id[tuple(P[j])]\n",
    "                        if (Pi_id, Pj_id) in loss_cache:\n",
    "                            loss = loss_cache[(Pi_id, Pj_id)]\n",
    "                        else:\n",
    "                            loss = compute_loss([P[i], P[j]], current_doi)\n",
    "                            loss_cache[(Pi_id, Pj_id)] = loss\n",
    "\n",
    "                        # only include feasible merge pairs, i.e. a pair which can be merged without the total number of configs exceeding stateCnt\n",
    "                        total_configrations_after_merge = total_configurations - 2**len(P[i]) - 2**len(P[j]) + 2**(len(P[i]) + len(P[j]))\n",
    "                        if loss > 0 and total_configrations_after_merge <= self.stateCnt:\n",
    "                            E.append((P[i], P[j], loss))    \n",
    "                            if len(P[i]) == 1 and len(P[j]) == 1:\n",
    "                                E1.append((P[i],P[j], loss))\n",
    "\n",
    "                #if verbose:    \n",
    "                    #print(f\"E pairs: {[[(index.index_id for index in Pi), (index.index_id for index in Pj), loss] for (Pi, Pj, loss) in E]}\")\n",
    "                    #print(f\"E1 pairs: {[[(index.index_id for index in Pi), (index.index_id for index in Pj), loss] for (Pi, Pj, loss) in E1]}\")\n",
    "\n",
    "                if len(E) == 0:\n",
    "                    break\n",
    "                \n",
    "                elif len(E1) > 0:\n",
    "                    # merge a random pair of singletons, sample randomly from E1 weighted by loss (i.e. high loss pairs more likely to be merged)\n",
    "                    Pi, Pj, loss = random.choices(E1, weights=[loss for (Pi, Pj, loss) in E1], k=1)[0]\n",
    "                    Pij_merged = Pi + Pj\n",
    "                    P.remove(Pi)\n",
    "                    P.remove(Pj)\n",
    "                    P.append(Pij_merged) \n",
    "                    E1.remove((Pi, Pj, loss))  \n",
    "                    partition2id[tuple(Pij_merged)] = len(partition2id) \n",
    "                    #if verbose: \n",
    "                    #    print(f\"Merged singleton partitions {[index.index_id for index in Pi]} and {[index.index_id for index in Pj]} with loss {loss}\")\n",
    "\n",
    "                else:\n",
    "                    # merge a random pair of partitions, sample randomly from E weighted by normalized loss  \n",
    "                    Pi, Pj, loss = random.choices(E, weights=[loss / (2**(len(Pi) + len(Pj)) - 2**len(Pi) - 2**len(Pj)) for (Pi, Pj, loss) in E], k=1)[0]\n",
    "                    Pij_merged = Pi + Pj\n",
    "                    P.remove(Pi)\n",
    "                    P.remove(Pj)\n",
    "                    P.append(Pij_merged)   \n",
    "                    E.remove((Pi, Pj, loss)) \n",
    "                    partition2id[tuple(Pij_merged)] = len(partition2id) \n",
    "                    #if verbose:\n",
    "                    #    print(f\"Merged partitions {[index.index_id for index in Pi]} and {[index.index_id for index in Pj]} with loss {loss}\")    \n",
    "\n",
    "            # check if the new solution is better than the current best solution\n",
    "            loss = compute_loss(P, current_doi)\n",
    "            if loss < bestLoss:\n",
    "                bestSolution = P\n",
    "                bestLoss = loss\n",
    "\n",
    "        return bestSolution, need_to_repartition\n",
    "\n",
    "\n",
    "    # update candidate index statistics\n",
    "    def update_stats(self, n, ibg, verbose):\n",
    "        # update index benefit statistics\n",
    "        if verbose: print(\"Updating index benefit statistics...\")\n",
    "        for index in self.U.values():\n",
    "            max_benefit = ibg.compute_max_benefit(index)\n",
    "            #if verbose: print(f\"\\tibg max benefit for index {index.index_id}: {max_benefit}\")\n",
    "            self.idxStats[index.index_id].append((n, max_benefit))\n",
    "            #if verbose: print(f\"\\tIndex {index.index_id}: {self.idxStats[index.index_id]}\")\n",
    "            # evict old stats if the size exceeds histSize\n",
    "            self.idxStats[index.index_id] = self.idxStats[index.index_id][-self.histSize:]\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Index benefit statistics:\")\n",
    "            for index_id, stats in self.idxStats.items():\n",
    "                print(f\"\\tIndex {index_id}: {stats}\")\n",
    "\n",
    "\n",
    "        # update index interaction statistics\n",
    "        if verbose: print(\"Updating index interaction statistics...\")\n",
    "        for (a_idx, b_idx) in ibg.doi.keys():\n",
    "            d = ibg.doi[(a_idx, b_idx)]\n",
    "            #if verbose: print(f\"\\tibg doi for pair ({a_idx}, {b_idx}) : {d}\")\n",
    "            if d > 0:\n",
    "                self.intStats[(a_idx, b_idx)].append((n, d))\n",
    "            #if verbose: print(f\"\\tPair ({a_idx}, {b_idx}): {self.intStats[(a_idx, b_idx)]}\")\n",
    "            # evict old stats if the size exceeds histSize\n",
    "            self.intStats[(a_idx, b_idx)] = self.intStats[(a_idx, b_idx)][-self.histSize:]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Index interaction statistics:\")\n",
    "            for pair, stats in self.intStats.items():\n",
    "                print(f\"\\tPair {pair}: {stats}\")\n",
    "\n",
    "\n",
    "    # choose top num_indexes indexes from X with highest potential benefit\n",
    "    def top_indexes(self, N_workload, X, num_indexes, verbose, positive_scores_only=False):\n",
    "        #if verbose:\n",
    "        #    print(f\"Non-materialized candidate indexes, X = {[index.index_id for index in X]}\")\n",
    "\n",
    "        # compute \"current benefit\" of each index in X (these are derived from statistics of observed benefits from recent queries)\n",
    "        score = {}\n",
    "        for index in X:\n",
    "            if len(self.idxStats[index.index_id]) == 0:\n",
    "                # zero current benefit if no statistics are available\n",
    "                current_benefit = 0\n",
    "            else:\n",
    "                # take the maximum over all incremental average benefits (optimistic estimate)\n",
    "                current_benefit = 0\n",
    "                b_total = 0\n",
    "                for (n, b) in self.idxStats[index.index_id]:\n",
    "                    b_total += b \n",
    "                    # incremental average benefit of index up to query n (higher weight/smaller denominator for more recent queries)\n",
    "                    benefit = b_total / (N_workload - n + 1)\n",
    "                    current_benefit = max(current_benefit, benefit)\n",
    "\n",
    "            # use current benefit to compute a score for the index\n",
    "            if index.index_id in self.C:\n",
    "                # if index already being monitored, then score is just current benefit\n",
    "                score[index.index_id] = current_benefit\n",
    "            else:\n",
    "                # if index not being monitored, then score is current benefit minus cost of creating the index\n",
    "                # (unmonitored indexes are penalized so that they are only chosen if they have high potential benefit, which helps keep C stable)\n",
    "                score[index.index_id] = current_benefit - self.get_index_creation_cost(index)\n",
    "\n",
    "        #if verbose:\n",
    "        #    print(\"Index scores:\")\n",
    "        #    for index_id, s in score.items():\n",
    "        #        print(f\"Index {index_id}: {s}\")\n",
    "\n",
    "        # get the top num_indexes indexes with highest scores (keep non-zero scores only)\n",
    "        if positive_scores_only:\n",
    "            top_indexes = [index_id for index_id, s in score.items() if s > 0]\n",
    "        else:\n",
    "            top_indexes = [index_id for index_id, s in score.items()]    \n",
    "        top_indexes = sorted(top_indexes, key=lambda x: score[x], reverse=True)[:num_indexes]\n",
    "        top_indexes = {index_id: self.U[index_id] for index_id in top_indexes}\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{len(top_indexes)} top indexes: {[index.index_id for index in top_indexes.values()]}\")\n",
    "\n",
    "        return top_indexes    \n",
    "\n",
    "\n",
    "    # return index creation cost (using estimated index size as proxy for cost)\n",
    "    def get_index_creation_cost(self, index):\n",
    "        # return estimated size of index\n",
    "        return index.size * 256  #* 1024 * 1024\n",
    "\n",
    "\n",
    "    # compute transition cost between two MTS states/configurations\n",
    "    def compute_transition_cost(self, S_old, S_new):\n",
    "        # find out which indexes are added\n",
    "        added_indexes = set(S_new) - set(S_old)\n",
    "        # compute cost of creating the added indexes\n",
    "        transition_cost = sum([self.get_index_creation_cost(index) for index in added_indexes])\n",
    "        return transition_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test WFIT implementation on sample SSB workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate an SSB workload\n",
    "workload = [qg.generate_query(i) for i in range(1, 10)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "template id: 2, query: \n",
      "                SELECT SUM(lo_extendedprice * lo_discount) AS revenue\n",
      "                FROM lineorder, dwdate\n",
      "                WHERE lo_orderdate = d_datekey\n",
      "                AND d_yearmonthnum = 199402\n",
      "                AND lo_discount BETWEEN 1  AND 3 \n",
      "                AND lo_quantity BETWEEN 22 AND 30;\n",
      "            , payload: {'lineorder': ['lo_extendedprice', 'lo_discount']}, predicates: {'lineorder': ['lo_orderdate', 'lo_discount', 'lo_quantity'], 'dwdate': ['d_datekey', 'd_yearmonthnum']}, order by: {}, group by: {}\n"
     ]
    }
   ],
   "source": [
    "print(workload[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n",
      "Initializing WFA instances for 0 stable partitions...\n",
      "Initial set of materialized indexes: []\n",
      "Stable partitions: []\n",
      "Initial work function instances: \n",
      "\n",
      "Maximum number of candidate indexes tracked: 20\n",
      "Maximum number of MTS states/configurations: 1000\n",
      "Maximum number of historical index statistics kept: 100\n",
      "Number of randomized clustering iterations: 500\n",
      "##################################################################\n",
      "\n",
      "Processing query 1\n",
      "Generating new partitions for query #1\n",
      "Extracted 42 new indexes from query.\n",
      "Candidate indexes (including those currently materialized), |U| = 42\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 42\n",
      "Getting hypothetical sizes of candidate indexes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of nodes in IBG: 36, Total number of what-if calls: 36, Time spent on what-if calls: 1.3444890975952148\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 12/12 [00:00<00:00, 533.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent on computing all pair degree of interaction: 1.1441395282745361\n",
      "Updating statistics...\n",
      "Choosing top 20 indexes from 42 non-materialized candidate indexes\n",
      "20 top indexes: ['IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IX_dwdate_d_year', 'IX_dwdate_d_year_d_datekey', 'IX_dwdate_d_datekey', 'IX_dwdate_d_datekey_d_year', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_discount_lo_e', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e_lo_d', 'IX_lineorder_lo_orderdate', 'IX_lineorder_lo_discount', 'IX_lineorder_lo_quantity', 'IXN_lineorder_lo_orderdate_lo_d', 'IXN_lineorder_lo_quantity_lo_d', 'IX_lineorder_lo_orderdate_lo_discount', 'IX_lineorder_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_orderdate', 'IX_lineorder_lo_discount_lo_quantity', 'IX_lineorder_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_discount', 'IXN_lineorder_lo_orderdate_lo_e', 'IXN_lineorder_lo_discount_lo_e']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 20\n",
      "Choosing new partitions...\n",
      "Old partitions:\n",
      "New partitions:\n",
      "\t['IX_lineorder_lo_orderdate']\n",
      "\t['IX_lineorder_lo_discount']\n",
      "\t['IXN_lineorder_lo_quantity_lo_d']\n",
      "\t['IX_lineorder_lo_orderdate_lo_discount']\n",
      "\t['IX_lineorder_lo_orderdate_lo_quantity']\n",
      "\t['IX_lineorder_lo_discount_lo_orderdate']\n",
      "\t['IX_lineorder_lo_discount_lo_quantity']\n",
      "\t['IX_lineorder_lo_quantity_lo_orderdate']\n",
      "\t['IX_lineorder_lo_quantity_lo_discount']\n",
      "\t['IXN_lineorder_lo_orderdate_lo_e']\n",
      "\t['IXN_lineorder_lo_discount_lo_e']\n",
      "\t['IX_lineorder_lo_quantity', 'IX_dwdate_d_datekey', 'IXN_lineorder_lo_orderdate_lo_d', 'IX_dwdate_d_year', 'IX_dwdate_d_year_d_datekey', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_discount_lo_e', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e_lo_d', 'IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IX_dwdate_d_datekey_d_year']\n",
      "Repartitioning...\n",
      "Reinitializing WFA instances...\n",
      "Replaced stable partitions, WFA instances and recommendations with new ones\n",
      "Analyzing query...\n",
      "Updating WFA instance: 0\n",
      "Best state: (), Best score: 1428768.34\n",
      "\tWFA Instance #0, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 1\n",
      "Best state: (), Best score: 1428768.34\n",
      "\tWFA Instance #1, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 2\n",
      "Best state: (), Best score: 1428768.34\n",
      "\tWFA Instance #2, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 3\n",
      "Best state: (), Best score: 1428768.34\n",
      "\tWFA Instance #3, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 4\n",
      "Best state: (), Best score: 1428768.34\n",
      "\tWFA Instance #4, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 5\n",
      "Best state: (), Best score: 1428768.34\n",
      "\tWFA Instance #5, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 6\n",
      "Best state: (), Best score: 1428768.34\n",
      "\tWFA Instance #6, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 7\n",
      "Best state: (), Best score: 1428768.34\n",
      "\tWFA Instance #7, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 8\n",
      "Best state: (), Best score: 1428768.34\n",
      "\tWFA Instance #8, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 9\n",
      "Best state: (), Best score: 1428768.34\n",
      "\tWFA Instance #9, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 10\n",
      "Best state: (), Best score: 1428768.34\n",
      "\tWFA Instance #10, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 11\n",
      "Best state: (<pg_utils.Index object at 0x7f3f17f134d0>, <pg_utils.Index object at 0x7f3f17f12750>), Best score: 1421480.68\n",
      "\tWFA Instance #11, Num States: 512, New Recommendation: ['IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IX_dwdate_d_year'] --> Indexes Added: ['IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IX_dwdate_d_year'], Indexes Removed: []\n",
      "New indexes added this round: ['IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IX_dwdate_d_year']\n",
      "Currently materialized indexes: ['IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IX_dwdate_d_year']\n",
      "Removing stale indexes from U...\n",
      "Number of indexes in U: 42\n",
      "Number of indexes removed: 31, Number of indexes remaining: 11\n",
      "*** WFIT recommendation: ['IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IX_dwdate_d_year']\n",
      "*** Simple recommendation: ['IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IX_dwdate_d_year_d_datekey']\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "*** Speedup WFIT: 8.999963591165832, Speedup Simple: 9.000530544148557\n",
      "*** Total cost WFIT: 1421480.68, Total cost Simple: 1421480.68, WFIT/Simple: 1.0\n",
      "*** Total cost without any indexes: 1428768.34\n",
      "Total recommendation time taken for query #1: 8.056721925735474 seconds\n",
      "(Partitioning: 3.03765606880188 seconds, Repartitioning: 0.001804351806640625 seconds, Analyzing: 5.017258405685425 seconds)\n",
      "\n",
      "\n",
      "\n",
      "Processing query 2\n",
      "Generating new partitions for query #2\n",
      "Extracted 34 new indexes from query.\n",
      "Candidate indexes (including those currently materialized), |U| = 45\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 45\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42_43_44\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of nodes in IBG: 64, Total number of what-if calls: 64, Time spent on what-if calls: 2.5716054439544678\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 11/11 [00:01<00:00,  9.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent on computing all pair degree of interaction: 2.49654221534729\n",
      "Updating statistics...\n",
      "Choosing top 18 indexes from 43 non-materialized candidate indexes\n",
      "18 top indexes: ['IXN_lineorder_lo_orderdate_lo_quantity_lo_discount_lo_e', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e_lo_d', 'IX_lineorder_lo_orderdate_lo_quantity', 'IX_lineorder_lo_orderdate_lo_discount', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_d', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_discount_lo_e', 'IXN_lineorder_lo_quantity_lo_discount_lo_orderdate_lo_e', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_e_lo_d', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e', 'IX_dwdate_d_year_d_datekey', 'IX_dwdate_d_yearmonthnum', 'IX_dwdate_d_yearmonthnum_d_datekey', 'IX_dwdate_d_datekey_d_year', 'IX_lineorder_lo_orderdate', 'IXN_lineorder_lo_orderdate_lo_e', 'IXN_lineorder_lo_orderdate_lo_d']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 20\n",
      "Choosing new partitions...\n",
      "Old partitions:\n",
      "\t['IX_lineorder_lo_orderdate']\n",
      "\t['IX_lineorder_lo_discount']\n",
      "\t['IXN_lineorder_lo_quantity_lo_d']\n",
      "\t['IX_lineorder_lo_orderdate_lo_discount']\n",
      "\t['IX_lineorder_lo_orderdate_lo_quantity']\n",
      "\t['IX_lineorder_lo_discount_lo_orderdate']\n",
      "\t['IX_lineorder_lo_discount_lo_quantity']\n",
      "\t['IX_lineorder_lo_quantity_lo_orderdate']\n",
      "\t['IX_lineorder_lo_quantity_lo_discount']\n",
      "\t['IXN_lineorder_lo_orderdate_lo_e']\n",
      "\t['IXN_lineorder_lo_discount_lo_e']\n",
      "\t['IX_lineorder_lo_quantity', 'IX_dwdate_d_datekey', 'IXN_lineorder_lo_orderdate_lo_d', 'IX_dwdate_d_year', 'IX_dwdate_d_year_d_datekey', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_discount_lo_e', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e_lo_d', 'IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IX_dwdate_d_datekey_d_year']\n",
      "New partitions:\n",
      "\t['IX_lineorder_lo_orderdate']\n",
      "\t['IXN_lineorder_lo_orderdate_lo_e']\n",
      "\t['IX_dwdate_d_yearmonthnum', 'IX_dwdate_d_yearmonthnum_d_datekey']\n",
      "\t['IX_lineorder_lo_orderdate_lo_discount', 'IXN_lineorder_lo_quantity_lo_discount_lo_orderdate_lo_e', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_discount_lo_e', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_e_lo_d', 'IX_dwdate_d_datekey_d_year', 'IX_dwdate_d_year', 'IX_dwdate_d_year_d_datekey']\n",
      "\t['IXN_lineorder_lo_orderdate_lo_d', 'IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e_lo_d', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_d', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_discount_lo_e', 'IX_lineorder_lo_orderdate_lo_quantity']\n",
      "Repartitioning...\n",
      "Reinitializing WFA instances...\n",
      "Replaced stable partitions, WFA instances and recommendations with new ones\n",
      "Analyzing query...\n",
      "Updating WFA instance: 0\n",
      "Best state: (), Best score: 18625552.860000003\n",
      "\tWFA Instance #0, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 1\n",
      "Best state: (), Best score: 18625552.860000003\n",
      "\tWFA Instance #1, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 2\n",
      "Best state: (<pg_utils.Index object at 0x7f3c371e6e50>,), Best score: 18625498.430000003\n",
      "\tWFA Instance #2, Num States: 4, New Recommendation: ['IX_dwdate_d_yearmonthnum'] --> Indexes Added: ['IX_dwdate_d_yearmonthnum'], Indexes Removed: []\n",
      "Updating WFA instance: 3\n",
      "Best state: (<pg_utils.Index object at 0x7f3f17f12750>, <pg_utils.Index object at 0x7f3c36f1db10>), Best score: 18420056.240000002\n",
      "\tWFA Instance #3, Num States: 128, New Recommendation: ['IX_dwdate_d_year', 'IX_lineorder_lo_orderdate_lo_discount'] --> Indexes Added: ['IX_lineorder_lo_orderdate_lo_discount'], Indexes Removed: []\n",
      "Updating WFA instance: 4\n",
      "Best state: (<pg_utils.Index object at 0x7f3f17f134d0>,), Best score: 17150382.96\n",
      "\tWFA Instance #4, Num States: 512, New Recommendation: ['IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e'] --> Indexes Added: [], Indexes Removed: []\n",
      "New indexes added this round: ['IX_lineorder_lo_orderdate_lo_discount', 'IX_dwdate_d_yearmonthnum']\n",
      "Currently materialized indexes: ['IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IX_dwdate_d_year', 'IX_dwdate_d_yearmonthnum', 'IX_lineorder_lo_orderdate_lo_discount']\n",
      "Removing stale indexes from U...\n",
      "Number of indexes in U: 45\n",
      "Number of indexes removed: 24, Number of indexes remaining: 21\n",
      "*** WFIT recommendation: ['IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IX_dwdate_d_year', 'IX_dwdate_d_yearmonthnum', 'IX_lineorder_lo_orderdate_lo_discount']\n",
      "*** Simple recommendation: ['IXN_lineorder_lo_orderdate_lo_quantity_lo_discount_lo_e', 'IX_dwdate_d_yearmonthnum_d_datekey']\n",
      "No index scans were explicitly noted in the query plan.\n",
      "*** Speedup WFIT: 1.0056889639914346, Speedup Simple: 1.5047679132131024\n",
      "*** Total cost WFIT: 2046100.79, Total cost Simple: 2692492.74, WFIT/Simple: 0.7599280620530103\n",
      "*** Total cost without any indexes: 2916388.7800000003\n",
      "Total recommendation time taken for query #2: 13.81366777420044 seconds\n",
      "(Partitioning: 5.793499231338501 seconds, Repartitioning: 0.006012678146362305 seconds, Analyzing: 8.014153003692627 seconds)\n",
      "\n",
      "\n",
      "\n",
      "Processing query 3\n",
      "Generating new partitions for query #3\n",
      "Extracted 27 new indexes from query.\n",
      "Candidate indexes (including those currently materialized), |U| = 48\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 48\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42_43_44_45_46_47\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of nodes in IBG: 261, Total number of what-if calls: 261, Time spent on what-if calls: 8.464707136154175\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 10/10 [00:08<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent on computing all pair degree of interaction: 9.145102739334106\n",
      "Updating statistics...\n",
      "Choosing top 16 indexes from 44 non-materialized candidate indexes\n",
      "16 top indexes: ['IXN_lineorder_lo_orderdate_lo_quantity_lo_discount_lo_e', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e_lo_d', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_d', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_discount_lo_e', 'IXN_lineorder_lo_quantity_lo_discount_lo_orderdate_lo_e', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_e_lo_d', 'IXN_lineorder_lo_orderdate_lo_discount_lo_e', 'IX_lineorder_lo_orderdate', 'IXN_lineorder_lo_orderdate_lo_d', 'IXN_lineorder_lo_orderdate_lo_e', 'IXN_lineorder_lo_discount_lo_orderdate_lo_quantity_lo_e', 'IXN_lineorder_lo_discount_lo_quantity_lo_orderdate_lo_e']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 20\n",
      "Choosing new partitions...\n",
      "Old partitions:\n",
      "\t['IX_lineorder_lo_orderdate']\n",
      "\t['IXN_lineorder_lo_orderdate_lo_e']\n",
      "\t['IX_dwdate_d_yearmonthnum', 'IX_dwdate_d_yearmonthnum_d_datekey']\n",
      "\t['IX_lineorder_lo_orderdate_lo_discount', 'IXN_lineorder_lo_quantity_lo_discount_lo_orderdate_lo_e', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_discount_lo_e', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_e_lo_d', 'IX_dwdate_d_datekey_d_year', 'IX_dwdate_d_year', 'IX_dwdate_d_year_d_datekey']\n",
      "\t['IXN_lineorder_lo_orderdate_lo_d', 'IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e_lo_d', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_d', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_discount_lo_e', 'IX_lineorder_lo_orderdate_lo_quantity']\n",
      "New partitions:\n",
      "\t['IX_lineorder_lo_orderdate']\n",
      "\t['IXN_lineorder_lo_orderdate_lo_e']\n",
      "\t['IX_dwdate_d_yearmonthnum']\n",
      "\t['IX_lineorder_lo_orderdate_lo_discount', 'IXN_lineorder_lo_quantity_lo_discount_lo_orderdate_lo_e', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_discount_lo_e', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_e_lo_d', 'IX_dwdate_d_year']\n",
      "\t['IXN_lineorder_lo_orderdate_lo_d', 'IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e_lo_d', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_d', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_discount_lo_e', 'IX_lineorder_lo_orderdate_lo_quantity']\n",
      "\t['IXN_lineorder_lo_orderdate_lo_discount_lo_e']\n",
      "\t['IXN_lineorder_lo_discount_lo_orderdate_lo_quantity_lo_e']\n",
      "\t['IXN_lineorder_lo_discount_lo_quantity_lo_orderdate_lo_e']\n",
      "Repartitioning...\n",
      "Reinitializing WFA instances...\n",
      "Replaced stable partitions, WFA instances and recommendations with new ones\n",
      "Analyzing query...\n",
      "Updating WFA instance: 0\n",
      "Best state: (<pg_utils.Index object at 0x7f3c9c65de10>,), Best score: 92405525.08000003\n",
      "\tWFA Instance #0, Num States: 2, New Recommendation: ['IX_lineorder_lo_orderdate'] --> Indexes Added: ['IX_lineorder_lo_orderdate'], Indexes Removed: []\n",
      "Updating WFA instance: 1\n",
      "Best state: (<pg_utils.Index object at 0x7f3e8676bb50>,), Best score: 92674937.08000003\n",
      "\tWFA Instance #1, Num States: 2, New Recommendation: ['IXN_lineorder_lo_orderdate_lo_e'] --> Indexes Added: ['IXN_lineorder_lo_orderdate_lo_e'], Indexes Removed: []\n",
      "Updating WFA instance: 2\n",
      "Best state: (<pg_utils.Index object at 0x7f3c371e6e50>,), Best score: 92941869.51000002\n",
      "\tWFA Instance #2, Num States: 2, New Recommendation: ['IX_dwdate_d_yearmonthnum'] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 3\n",
      "Best state: (<pg_utils.Index object at 0x7f3f17f12750>, <pg_utils.Index object at 0x7f3c36f1db10>), Best score: 91626210.04000002\n",
      "\tWFA Instance #3, Num States: 32, New Recommendation: ['IX_dwdate_d_year', 'IX_lineorder_lo_orderdate_lo_discount'] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 4\n",
      "Best state: (<pg_utils.Index object at 0x7f3f17f134d0>,), Best score: 91450697.78000003\n",
      "\tWFA Instance #4, Num States: 512, New Recommendation: ['IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e'] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 5\n",
      "Best state: (<pg_utils.Index object at 0x7f3c36f1ccd0>,), Best score: 92660122.11000003\n",
      "\tWFA Instance #5, Num States: 2, New Recommendation: ['IXN_lineorder_lo_orderdate_lo_discount_lo_e'] --> Indexes Added: ['IXN_lineorder_lo_orderdate_lo_discount_lo_e'], Indexes Removed: []\n",
      "Updating WFA instance: 6\n",
      "Best state: (), Best score: 92941869.51000002\n",
      "\tWFA Instance #6, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 7\n",
      "Best state: (), Best score: 92941869.51000002\n",
      "\tWFA Instance #7, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "New indexes added this round: ['IX_lineorder_lo_orderdate', 'IXN_lineorder_lo_orderdate_lo_discount_lo_e', 'IXN_lineorder_lo_orderdate_lo_e']\n",
      "Currently materialized indexes: ['IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IX_dwdate_d_year', 'IX_dwdate_d_yearmonthnum', 'IX_lineorder_lo_orderdate_lo_discount', 'IX_lineorder_lo_orderdate', 'IXN_lineorder_lo_orderdate_lo_e', 'IXN_lineorder_lo_orderdate_lo_discount_lo_e']\n",
      "Removing stale indexes from U...\n",
      "Number of indexes in U: 48\n",
      "Number of indexes removed: 19, Number of indexes remaining: 29\n",
      "*** WFIT recommendation: ['IXN_lineorder_lo_orderdate_lo_discount_lo_e', 'IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IXN_lineorder_lo_orderdate_lo_e', 'IX_dwdate_d_year', 'IX_dwdate_d_yearmonthnum', 'IX_lineorder_lo_orderdate', 'IX_lineorder_lo_orderdate_lo_discount']\n",
      "*** Simple recommendation: ['IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IX_dwdate_d_year']\n",
      "No index scans were explicitly noted in the query plan.\n",
      "*** Speedup WFIT: 1.0, Speedup Simple: 1.0\n",
      "*** Total cost WFIT: 4115951.21, Total cost Simple: 2696087.16, WFIT/Simple: 1.52663877899259\n",
      "*** Total cost without any indexes: 4411214.94\n",
      "Total recommendation time taken for query #3: 25.952606916427612 seconds\n",
      "(Partitioning: 18.380057096481323 seconds, Repartitioning: 0.004487752914428711 seconds, Analyzing: 7.568056106567383 seconds)\n",
      "\n",
      "\n",
      "\n",
      "Processing query 4\n",
      "Generating new partitions for query #4\n",
      "Extracted 53 new indexes from query.\n",
      "Candidate indexes (including those currently materialized), |U| = 82\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 82\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42_43_44_45_46_47_48_49_50_51_52_53_54_55_56_57_58_59_60_61_62_63_64_65_66_67_68_69_70_71_72_73_74_75_76_77_78_79_80_81\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of nodes in IBG: 192, Total number of what-if calls: 192, Time spent on what-if calls: 7.4994401931762695\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 10/10 [00:31<00:00,  3.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent on computing all pair degree of interaction: 32.674964904785156\n",
      "Updating statistics...\n",
      "Choosing top 13 indexes from 75 non-materialized candidate indexes\n",
      "13 top indexes: ['IXN_lineorder_lo_orderdate_lo_quantity_lo_discount_lo_e', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e_lo_d', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_d', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e', 'IXN_lineorder_lo_discount_lo_orderdate_lo_quantity_lo_e', 'IXN_lineorder_lo_discount_lo_quantity_lo_orderdate_lo_e', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_discount_lo_e', 'IXN_lineorder_lo_quantity_lo_discount_lo_orderdate_lo_e', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_e_lo_d', 'IXN_lineorder_lo_orderdate_lo_d']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 20\n",
      "Choosing new partitions...\n",
      "Old partitions:\n",
      "\t['IX_lineorder_lo_orderdate']\n",
      "\t['IXN_lineorder_lo_orderdate_lo_e']\n",
      "\t['IX_dwdate_d_yearmonthnum']\n",
      "\t['IX_lineorder_lo_orderdate_lo_discount', 'IXN_lineorder_lo_quantity_lo_discount_lo_orderdate_lo_e', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_discount_lo_e', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_e_lo_d', 'IX_dwdate_d_year']\n",
      "\t['IXN_lineorder_lo_orderdate_lo_d', 'IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e_lo_d', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_d', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_discount_lo_e', 'IX_lineorder_lo_orderdate_lo_quantity']\n",
      "\t['IXN_lineorder_lo_orderdate_lo_discount_lo_e']\n",
      "\t['IXN_lineorder_lo_discount_lo_orderdate_lo_quantity_lo_e']\n",
      "\t['IXN_lineorder_lo_discount_lo_quantity_lo_orderdate_lo_e']\n",
      "New partitions:\n",
      "\t['IX_lineorder_lo_orderdate']\n",
      "\t['IXN_lineorder_lo_orderdate_lo_e']\n",
      "\t['IX_dwdate_d_yearmonthnum']\n",
      "\t['IX_lineorder_lo_orderdate_lo_discount', 'IXN_lineorder_lo_quantity_lo_discount_lo_orderdate_lo_e', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_discount_lo_e', 'IXN_lineorder_lo_quantity_lo_orderdate_lo_e_lo_d', 'IX_dwdate_d_year']\n",
      "\t['IXN_lineorder_lo_orderdate_lo_d', 'IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_e_lo_d', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_d', 'IXN_lineorder_lo_orderdate_lo_quantity_lo_discount_lo_e', 'IX_lineorder_lo_orderdate_lo_quantity']\n",
      "\t['IXN_lineorder_lo_orderdate_lo_discount_lo_e']\n",
      "\t['IXN_lineorder_lo_discount_lo_orderdate_lo_quantity_lo_e']\n",
      "\t['IXN_lineorder_lo_discount_lo_quantity_lo_orderdate_lo_e']\n",
      "Analyzing query...\n",
      "Updating WFA instance: 0\n",
      "Best state: (<pg_utils.Index object at 0x7f3c9c65de10>,), Best score: 93729723.56000003\n",
      "\tWFA Instance #0, Num States: 2, New Recommendation: ['IX_lineorder_lo_orderdate'] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 1\n",
      "Best state: (<pg_utils.Index object at 0x7f3e8676bb50>,), Best score: 93999135.56000003\n",
      "\tWFA Instance #1, Num States: 2, New Recommendation: ['IXN_lineorder_lo_orderdate_lo_e'] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 2\n",
      "Best state: (<pg_utils.Index object at 0x7f3c371e6e50>,), Best score: 94266067.99000002\n",
      "\tWFA Instance #2, Num States: 2, New Recommendation: ['IX_dwdate_d_yearmonthnum'] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 3\n",
      "Best state: (<pg_utils.Index object at 0x7f3f17f12750>, <pg_utils.Index object at 0x7f3c36f1db10>), Best score: 92950408.52000003\n",
      "\tWFA Instance #3, Num States: 32, New Recommendation: ['IX_dwdate_d_year', 'IX_lineorder_lo_orderdate_lo_discount'] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 4\n",
      "Best state: (<pg_utils.Index object at 0x7f3f17f134d0>,), Best score: 92774896.26000004\n",
      "\tWFA Instance #4, Num States: 512, New Recommendation: ['IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e'] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 5\n",
      "Best state: (<pg_utils.Index object at 0x7f3c36f1ccd0>,), Best score: 93984320.59000003\n",
      "\tWFA Instance #5, Num States: 2, New Recommendation: ['IXN_lineorder_lo_orderdate_lo_discount_lo_e'] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 6\n",
      "Best state: (), Best score: 94266067.99000002\n",
      "\tWFA Instance #6, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 7\n",
      "Best state: (), Best score: 94266067.99000002\n",
      "\tWFA Instance #7, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "New indexes added this round: []\n",
      "Currently materialized indexes: ['IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IX_dwdate_d_year', 'IX_dwdate_d_yearmonthnum', 'IX_lineorder_lo_orderdate_lo_discount', 'IX_lineorder_lo_orderdate', 'IXN_lineorder_lo_orderdate_lo_e', 'IXN_lineorder_lo_orderdate_lo_discount_lo_e']\n",
      "Removing stale indexes from U...\n",
      "Number of indexes in U: 82\n",
      "Number of indexes removed: 39, Number of indexes remaining: 43\n",
      "*** WFIT recommendation: ['IXN_lineorder_lo_orderdate_lo_discount_lo_e', 'IXN_lineorder_lo_orderdate_lo_discount_lo_quantity_lo_e', 'IXN_lineorder_lo_orderdate_lo_e', 'IX_dwdate_d_year', 'IX_dwdate_d_yearmonthnum', 'IX_lineorder_lo_orderdate', 'IX_lineorder_lo_orderdate_lo_discount']\n",
      "*** Simple recommendation: ['IXN_lineorder_lo_partkey_lo_suppkey_lo_orderdate_lo_r', 'IX_part_p_category_p_partkey_p_brand', 'IX_supplier_s_region_s_suppkey']\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "*** Speedup WFIT: 1.0, Speedup Simple: 8.71232389301975\n",
      "*** Total cost WFIT: 5440149.6899999995, Total cost Simple: 4126868.58, WFIT/Simple: 1.318227024811146\n",
      "*** Total cost without any indexes: 5735413.42\n",
      "Total recommendation time taken for query #4: 54.69730186462402 seconds\n",
      "(Partitioning: 40.915199279785156 seconds, Repartitioning: 2.384185791015625e-07 seconds, Analyzing: 13.782100439071655 seconds)\n",
      "\n",
      "\n",
      "\n",
      "Processing query 5\n",
      "Generating new partitions for query #5\n",
      "Extracted 39 new indexes from query.\n",
      "Candidate indexes (including those currently materialized), |U| = 82\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 82\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42_43_44_45_46_47_48_49_50_51_52_53_54_55_56_57_58_59_60_61_62_63_64_65_66_67_68_69_70_71_72_73_74_75_76_77_78_79_80_81\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of nodes in IBG: 144, Total number of what-if calls: 144, Time spent on what-if calls: 3.0056331157684326\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel:   0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# instantiate WFIT\n",
    "C = extract_query_indexes(qg.generate_query(8), include_cols=False)  \n",
    "S_0 = []#C[0:1]\n",
    "#wfit = WFIT(S_0, idxCnt=20, stateCnt=1000, histSize=100, rand_cnt=10)\n",
    "wfit = WFIT(S_0, max_key_columns=3, include_cols=True, idxCnt=20, stateCnt=1000, rand_cnt=500)\n",
    "\n",
    "# process the workload\n",
    "for i, query in enumerate(workload):\n",
    "    print(f\"Processing query {i+1}\")\n",
    "    wfit.process_WFIT(query, remove_stale_U=True, remove_stale_freq=1, verbose=True)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
