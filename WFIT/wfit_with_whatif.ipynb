{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WFIT Algorithm Implementation (Schnaitter 2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "import IPython\n",
    "notebook_path = IPython.get_ipython().starting_dir\n",
    "target_subdirectory_path = os.path.abspath(os.path.join(os.path.dirname(notebook_path), 'PostgreSQL'))\n",
    "sys.path.append(target_subdirectory_path)\n",
    "\n",
    "from pg_utils import *\n",
    "from ssb_qgen_class import *\n",
    "\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "import time\n",
    "import random\n",
    "from more_itertools import powerset\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index Benefit Graph (IBG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, id, indexes):\n",
    "        self.id = id\n",
    "        self.indexes = indexes\n",
    "        self.children = []\n",
    "        self.parents = []\n",
    "        self.built = False\n",
    "        self.cost = None\n",
    "        self.used = []\n",
    "\n",
    "\n",
    "# class for creating and storing the IBG\n",
    "class IBG:\n",
    "    # Class-level cache\n",
    "    #_class_cache = {}\n",
    "\n",
    "    def __init__(self, query_object, C, max_nodes=1000):\n",
    "        self.q = query_object\n",
    "        self.C = C\n",
    "        print(f\"Number of candidate indexes: {len(self.C)}\")\n",
    "        #print(f\"Candidate indexes: {self.C}\")\n",
    "        \n",
    "        # get hypothetical sizes of all the candidate indexes\n",
    "        print(\"Getting hypothetical sizes of candidate indexes...\")\n",
    "        self.get_hypo_sizes()\n",
    "\n",
    "        # map index_id to integer\n",
    "        self.idx2id = {index.index_id:i for i, index in enumerate(self.C)}\n",
    "        self.idx2index = {index.index_id:index for index in self.C}\n",
    "        #print(f\"Index id to integer mapping: {self.idx2id}\")\n",
    "        \n",
    "        # create a hash table for keeping track of all created nodes\n",
    "        self.nodes = {}\n",
    "        # create a root node\n",
    "        self.root = Node(self.get_configuration_id(self.C), self.C)\n",
    "        self.nodes[self.root.id] = self.root\n",
    "        print(f\"Created root node with id: {self.root.id}\")\n",
    "        \n",
    "        self.total_whatif_calls = 0\n",
    "        self.total_whatif_time = 0\n",
    "        self.node_count = 0\n",
    "\n",
    "        # start the IBG construction\n",
    "        print(\"Constructing IBG...\")\n",
    "        self.construct_ibg(self.root, max_nodes=max_nodes)\n",
    "        print(f\"Number of nodes in IBG: {len(self.nodes)}, Total number of what-if calls: {self.total_whatif_calls}, Time spent on what-if calls: {self.total_whatif_time}\")\n",
    "        # compute all pair degree of interaction\n",
    "        print(f\"Computing all pair degree of interaction...\")\n",
    "        start_time = time.time()\n",
    "        #self.doi = self.compute_all_pair_doi()\n",
    "        self.doi = self.compute_all_pair_doi_parallel(num_workers=8, max_nodes=100)\n",
    "        #self.doi = self.compute_all_pair_doi_simple()\n",
    "        #self.doi = self.compute_all_pair_doi_naive(num_samples=256)\n",
    "        print(f\"All pair doi:\")\n",
    "        for key, value in self.doi.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"Time spent on computing all pair degree of interaction: {end_time - start_time}\")\n",
    "\n",
    "    # assign unique string id to a configuration\n",
    "    def get_configuration_id(self, indexes):\n",
    "        # get sorted list of integer ids\n",
    "        ids = sorted([self.idx2id[idx.index_id] for idx in indexes])\n",
    "        return \"_\".join([str(i) for i in ids])\n",
    "    \n",
    "\n",
    "    # get hypothetical sizes of all the candidate indexes\n",
    "    def get_hypo_sizes(self):\n",
    "        conn = create_connection()\n",
    "        hypo_indexes = bulk_create_hypothetical_indexes(conn, self.C, return_size=True)\n",
    "        close_connection(conn)\n",
    "        \n",
    "        for i in range(len(hypo_indexes)):\n",
    "            self.C[i].size = hypo_indexes[i][1]\n",
    "        \n",
    "\n",
    "    def _get_cost_used(self, indexes):\n",
    "        # Convert indexes to a tuple to make it hashable\n",
    "        #indexes_tuple = tuple(sorted(indexes, key=lambda x: x.index_id))\n",
    "        # Check if the result is already in the class-level cache\n",
    "        #if indexes_tuple in self._class_cache:\n",
    "        #    return self._class_cache[indexes_tuple]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        conn = create_connection()\n",
    "        # create hypothetical indexes\n",
    "        hypo_indexes = bulk_create_hypothetical_indexes(conn, indexes)\n",
    "        # map oid to index object\n",
    "        oid2index = {}\n",
    "        for i in range(len(hypo_indexes)):\n",
    "            oid2index[hypo_indexes[i]] = indexes[i]\n",
    "        # get cost and used indexes\n",
    "        cost, indexes_used = get_query_cost_estimate_hypo_indexes(conn, self.q.query_string, show_plan=False)\n",
    "        # map used index oids to index objects\n",
    "        used = [oid2index[oid] for oid, scan_type, scan_cost in indexes_used]\n",
    "        # drop hypothetical indexes\n",
    "        bulk_drop_hypothetical_indexes(conn)\n",
    "        close_connection(conn)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Store the result in the class-level cache\n",
    "        #self._class_cache[indexes_tuple] = (cost, used)\n",
    "        self.total_whatif_calls += 1\n",
    "        self.total_whatif_time += end_time - start_time\n",
    "\n",
    "        return cost, used\n",
    "\n",
    "    # Ensure the indexes parameter is hashable\n",
    "    def _cached_get_cost_used(self, indexes):\n",
    "        return self._get_cost_used(tuple(indexes))\n",
    "\n",
    "    \n",
    "    # IBG construction\n",
    "    def construct_ibg(self, root, max_nodes=None):\n",
    "        # Obtain query optimizer's cost and used indexes\n",
    "        cost, used = self._cached_get_cost_used(root.indexes)\n",
    "        root.cost = cost\n",
    "        root.used = used\n",
    "        root.built = True\n",
    "        self.node_count += 1\n",
    "\n",
    "        num_levels = 0\n",
    "        queue = deque([root])\n",
    "        while queue:\n",
    "\n",
    "            if max_nodes is not None and self.node_count >= max_nodes:\n",
    "                break  # end if the maximum number of nodes is reached\n",
    "            \n",
    "            # Get the current level size\n",
    "            level_size = len(queue)\n",
    "            num_levels += 1\n",
    "\n",
    "            # Process all nodes at the current level\n",
    "            for _ in range(level_size):\n",
    "                Y = queue.popleft()\n",
    "                               \n",
    "                # Create children\n",
    "                for a in Y.used:\n",
    "                    # Create a new configuration with index a removed from Y\n",
    "                    X_indexes = [index for index in Y.indexes if index != a]\n",
    "                    X_id = self.get_configuration_id(X_indexes)\n",
    "                    \n",
    "                    # If X is not in the hash table, create a new node and add it to the queue\n",
    "                    if X_id not in self.nodes:\n",
    "                        self.node_count += 1\n",
    "                        print(f\"Creating node # {self.node_count}\", end=\"\\r\")\n",
    "         \n",
    "                        X = Node(X_id, X_indexes)\n",
    "                        # Obtain query optimizer's cost and used indexes\n",
    "                        cost, used = self._cached_get_cost_used(X.indexes)\n",
    "                        X.cost = cost\n",
    "                        X.used = used\n",
    "                        X.built = True\n",
    "                        X.parents.append(Y)\n",
    "                        self.nodes[X_id] = X\n",
    "                        Y.children.append(X)\n",
    "                        queue.append(X)\n",
    "\n",
    "                    else:\n",
    "                        X = self.nodes[X_id]\n",
    "                        Y.children.append(X)\n",
    "                        X.parents.append(Y)\n",
    "\n",
    "        print(f\"Number of levels in IBG: {num_levels}\")      \n",
    "\n",
    "\n",
    "    # use IBG to obtain estimated cost and used indexes for arbitrary subset of C\n",
    "    def get_cost_used(self, X):\n",
    "        # get id of the configuration\n",
    "        id = self.get_configuration_id(X)\n",
    "        # check if the configuration is in the IBG\n",
    "        if id in self.nodes:\n",
    "            cost, used = self.nodes[id].cost, self.nodes[id].used\n",
    "        \n",
    "        # if not in the IBG, traverse the IBG to find a covering node\n",
    "        else:\n",
    "            Y = self.find_covering_node(X)              \n",
    "            cost, used = Y.cost, Y.used\n",
    "\n",
    "        return cost, used    \n",
    "\n",
    "\n",
    "    # traverses the IBG to find a node that removes indexes not in X (i.e. a covering node for X)\n",
    "    def find_covering_node(self, X):\n",
    "        X_indexes = set([index.index_id for index in X])\n",
    "        Y = self.root\n",
    "        Y_indexes = set([index.index_id for index in Y.indexes])\n",
    "        # traverse IBG to find covering node\n",
    "        while len(Y.children) > 0:               \n",
    "            # traverse down to the child node that removes an index not in X\n",
    "            child_found = False\n",
    "            for child in Y.children:\n",
    "                child_indexes = set([index.index_id for index in child.indexes])\n",
    "                child_indexes_removed = Y_indexes - child_indexes\n",
    "                child_indexes_removed_not_in_X = child_indexes_removed - X_indexes\n",
    "        \n",
    "                # check if child removes an index not in X\n",
    "                if len(child_indexes_removed_not_in_X) > 0:\n",
    "                    Y = child\n",
    "                    Y_indexes = child_indexes\n",
    "                    child_found = True\n",
    "                    break\n",
    "\n",
    "            # if no children remove indexes not in X    \n",
    "            if not child_found:\n",
    "                break    \n",
    "    \n",
    "        return Y        \n",
    "\n",
    "    # compute benefit of an index for a given configuration \n",
    "    # input X is a list of index objects and 'a' is a single index object\n",
    "    # X must not contain 'a'\n",
    "    def compute_benefit(self, a, X):\n",
    "        if a in X:\n",
    "            # zero benefit if 'a' is already in X\n",
    "            #raise ValueError(\"Index 'a' is already in X\")\n",
    "            return 0\n",
    "        \n",
    "        # get cost  for X\n",
    "        cost_X = self.get_cost_used(X)[0]\n",
    "        # create a new configuration with index a added to X\n",
    "        X_a = X + [a]\n",
    "        # get cost for X + {a}\n",
    "        cost_X_a = self.get_cost_used(X_a)[0]\n",
    "        # compute benefit\n",
    "        benefit = cost_X - cost_X_a\n",
    "        return benefit \n",
    "\n",
    "\n",
    "    # compute maximum benefit of adding an index to any possibe configuration\n",
    "    def compute_max_benefit(self, a):\n",
    "        max_benefit = float('-inf')\n",
    "        for id, node in self.nodes.items():\n",
    "            #print(f\"Computing benefit for node: {[index.index_id for index in node.indexes]}\")\n",
    "            benefit = self.compute_benefit(a, node.indexes)\n",
    "            if benefit > max_benefit:\n",
    "                max_benefit = benefit\n",
    "\n",
    "        return max_benefit\n",
    "    \n",
    "    # compute the degree of interaction between two indexes a,b in configuration X \n",
    "    def compute_doi_configuration(self, a, b, X=[], normalize=True):\n",
    "        # X must not contain a or b\n",
    "        if a in X or b in X:\n",
    "            raise ValueError(\"a or b is already in X\")\n",
    "\n",
    "        doi = abs(self.compute_benefit(a, X) - self.compute_benefit(a, X + [b]))\n",
    "        if normalize:\n",
    "            doi /= self.get_cost_used(X + [a,b])[0]   \n",
    "        return doi\n",
    "   \n",
    "    \n",
    "    # Cache the results of find_covering_node and get_cost_used to avoid redundant calculations\n",
    "    @lru_cache(maxsize=None)\n",
    "    def cached_find_covering_node(self, indexes):\n",
    "        return self.find_covering_node(tuple(indexes))\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def cached_get_cost_used(self, indexes):\n",
    "        return self.get_cost_used(tuple(indexes))\n",
    "\n",
    "\n",
    "    # computes the degree of interaction between all pairs of indexes (a,b) in candidate set C\n",
    "    # Note: doi is symmetric, i.e. doi(a,b) = doi(b,a)\n",
    "\n",
    "    # simple version of compute_all_pair_doi, without parallelization\n",
    "    def compute_all_pair_doi_simple(self):\n",
    "        # hash table for storing doi values\n",
    "        doi = {}\n",
    "        # intialize doi values to zero\n",
    "        for i in range(len(self.C)):\n",
    "            for j in range(i+1, len(self.C)):\n",
    "                d = self.compute_doi_configuration(self.C[i], self.C[j])\n",
    "                doi[tuple(sorted((self.C[i].index_id, self.C[j].index_id)))] = d\n",
    "\n",
    "        return doi\n",
    "\n",
    "    # Naive version of compute_all_pair_doi, with random sampling of configurations\n",
    "    def compute_all_pair_doi_naive(self, num_samples=100):\n",
    "        doi = {}\n",
    "        \n",
    "        for i in range(len(self.C)):\n",
    "            for j in range(i + 1, len(self.C)):\n",
    "                doi[tuple(sorted((self.C[i].index_id, self.C[j].index_id)))] = 0\n",
    "        \n",
    "        # sample random configurations: X subset C (must include empty set configuration)\n",
    "        for i in tqdm(range(num_samples), desc=\"Sampling configurations\"):\n",
    "            if i == 0:\n",
    "                X = []\n",
    "            else:\n",
    "                X = random.sample(self.C, random.randint(1, len(self.C)))\n",
    "\n",
    "            # compute doi for all pairs (a, b) in U\\X \n",
    "            for i in range(len(self.C)):\n",
    "                for j in range(i+1, len(self.C)):\n",
    "                    a = self.C[i]\n",
    "                    b = self.C[j]\n",
    "                    if a not in X and b not in X:\n",
    "                        d = self.compute_doi_configuration(a, b, X)\n",
    "                        key = tuple(sorted((a.index_id, b.index_id)))\n",
    "                        doi[key] = max(doi[key], d)\n",
    "        \n",
    "        return doi    \n",
    "\n",
    "    # original version of compute_all_pair_doi, with optional max_nodes parameter for random sampling of nodes for efficient approximation\n",
    "    def compute_all_pair_doi(self, max_nodes=None):\n",
    "        # hash table for storing doi values\n",
    "        doi = {}\n",
    "        # intialize doi values to zero\n",
    "        for i in range(len(self.C)):\n",
    "            for j in range(i+1, len(self.C)):\n",
    "                doi[tuple(sorted((self.C[i].index_id, self.C[j].index_id)))] = 0\n",
    "\n",
    "        S_idxs = set([index.index_id for index in self.C])\n",
    "\n",
    "        # sample max_nodes number of nodes from the chunk\n",
    "        if max_nodes is not None:\n",
    "            nodes_sample = random.sample(self.nodes.values(), min(max_nodes, len(self.nodes)))\n",
    "        else:\n",
    "            nodes_sample = self.nodes.values()\n",
    "\n",
    "        # iterate over each IBG node\n",
    "        for Y in tqdm(nodes_sample, desc=\"Processing nodes\"):\n",
    "            \n",
    "            # remove Y.used from S\n",
    "            Y_idxs = set([index.index_id for index in Y.indexes])\n",
    "            used_Y = Y.used\n",
    "            Y_used_idxs = set([index.index_id for index in used_Y])\n",
    "            S_Y = list(S_idxs - Y_used_idxs)\n",
    "            # iterate over all pairs of indexes in S_Y\n",
    "            for i in range(len(S_Y)):\n",
    "                for j in range(i+1, len(S_Y)):\n",
    "                    a_idx = S_Y[i]\n",
    "                    b_idx = S_Y[j]\n",
    "                     \n",
    "                    # find Ya covering node in IBG\n",
    "                    Ya = (Y_idxs - {a_idx, b_idx}) | {a_idx}\n",
    "                    Ya = [self.idx2index[idx] for idx in Ya]\n",
    "                    Ya = self.cached_find_covering_node(tuple(Ya))\n",
    "                    # find Yab covering node in IBG\n",
    "                    Yab = (Y_idxs - {a_idx, b_idx}) | {a_idx, b_idx}\n",
    "                    Yab = [self.idx2index[idx] for idx in Yab]\n",
    "                    Yab = self.cached_find_covering_node(tuple(Yab))\n",
    "\n",
    "                    #used_Y = self.cached_get_cost_used(tuple(Y.indexes))[1]\n",
    "                    #used_Ya = self.cached_get_cost_used(tuple(Ya))[1]\n",
    "                    #used_Yab = self.cached_get_cost_used(tuple(Yab))[1]\n",
    "                    used_Ya = Ya.used\n",
    "                    used_Yab = Yab.used\n",
    "\n",
    "                    Uab = set([index.index_id for index in used_Y]) | set([index.index_id for index in used_Ya]) | set([index.index_id for index in used_Yab]) \n",
    "                    # find Yb_minus covering node in IBG \n",
    "                    Yb_minus = list((Uab - {a_idx, b_idx}) | {b_idx})\n",
    "                    Yb_minus = [self.idx2index[idx] for idx in Yb_minus]\n",
    "                    Yb_minus = self.cached_find_covering_node(tuple(Yb_minus))\n",
    "                    # find Yb_plus covering node in IBG\n",
    "                    Yb_plus = list((Y_idxs - {a_idx, b_idx}) | {b_idx})\n",
    "                    Yb_plus = [self.idx2index[idx] for idx in Yb_plus]\n",
    "                    Yb_plus = self.cached_find_covering_node(tuple(Yb_plus))\n",
    "\n",
    "                    # generate quadruples\n",
    "                    quadruples = [(Y.indexes, Ya.indexes, Yb_minus.indexes, Yab.indexes), (Y.indexes, Ya.indexes, Yb_plus.indexes, Yab.indexes)]\n",
    "\n",
    "                    # compute doi using the quadruples\n",
    "                    for Y_indexes, Ya_indexes, Yb_indexes, Yab_indexes in quadruples:\n",
    "                        cost_Y = self.cached_get_cost_used(tuple(Y_indexes))[0]\n",
    "                        cost_Ya = self.cached_get_cost_used(tuple(Ya_indexes))[0]\n",
    "                        cost_Yb = self.cached_get_cost_used(tuple(Yb_indexes))[0]\n",
    "                        cost_Yab = self.cached_get_cost_used(tuple(Yab_indexes))[0]\n",
    "                        # can ignore the normalization terms in denominator to get an absolute measure of doi\n",
    "                        d = abs(cost_Y - cost_Ya - cost_Yb + cost_Yab) / cost_Yab\n",
    "                        # save doi value for the pair\n",
    "                        key = tuple(sorted((a_idx, b_idx)))\n",
    "                        doi[key] = max(doi[key], d)\n",
    "                            \n",
    "        return doi\n",
    "\n",
    "\n",
    "    # parallelized version of compute_all_pair_doi\n",
    "    def compute_all_pair_doi_parallel(self, num_workers=16, max_nodes=None):\n",
    "        doi = {}\n",
    "        \n",
    "        for i in range(len(self.C)):\n",
    "            for j in range(i + 1, len(self.C)):\n",
    "                doi[tuple(sorted((self.C[i].index_id, self.C[j].index_id)))] = 0\n",
    "        \n",
    "        S_idxs = set([index.index_id for index in self.C])\n",
    "        \n",
    "        if max_nodes is not None:\n",
    "            nodes_list = random.sample(list(self.nodes.values()), min(max_nodes, len(self.nodes)))\n",
    "        else:    \n",
    "            nodes_list = list(self.nodes.values())\n",
    "        \n",
    "        chunk_size = max(1, len(nodes_list) // num_workers)\n",
    "\n",
    "        chunks = [nodes_list[i:i + chunk_size] for i in range(0, len(nodes_list), chunk_size)]\n",
    "        \n",
    "        args = [(chunk, self.C, self.idx2index, S_idxs, self.cached_find_covering_node, self.cached_get_cost_used) for chunk in chunks]\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            results = list(tqdm(executor.map(process_node_chunk, args), total=len(chunks), desc=\"Processing nodes in parallel\"))\n",
    "        \n",
    "        for result in results:\n",
    "            for key, value in result.items():\n",
    "                doi[key] = max(doi.get(key, 0), value)\n",
    "        \n",
    "        return doi\n",
    "    \n",
    "    \n",
    "    # get precomputed degree of interaction between a pair of indexes\n",
    "    def get_doi_pair(self, a, b):\n",
    "            return self.doi[tuple(sorted((a.index_id, b.index_id)))]\n",
    "\n",
    "\n",
    "    # function for printing the IBG, using BFS level order traversal\n",
    "    def print_ibg(self):\n",
    "        q = [self.root]\n",
    "        # traverse level by level, print all node ids in a level in a single line before moving to the next level\n",
    "        while len(q) > 0:\n",
    "            next_q = []\n",
    "            for node in q:\n",
    "                print(f\"{node.id} -> \", end=\"\")\n",
    "                for child in node.children:\n",
    "                    next_q.append(child)\n",
    "            print()\n",
    "            q = next_q  \n",
    "\n",
    "\n",
    "def process_node_chunk(args):\n",
    "    nodes_chunk, C, idx2index, S_idxs, cached_find_covering_node, cached_get_cost_used = args\n",
    "    doi_chunk = {}\n",
    "    \n",
    "    for Y in nodes_chunk:\n",
    "        Y_idxs = set([index.index_id for index in Y.indexes])\n",
    "        used_Y = Y.used\n",
    "        Y_used_idxs = set([index.index_id for index in used_Y])\n",
    "        S_Y = list(S_idxs - Y_used_idxs)\n",
    "        \n",
    "        for i in range(len(S_Y)):\n",
    "            for j in range(i + 1, len(S_Y)):\n",
    "                a_idx = S_Y[i]\n",
    "                b_idx = S_Y[j]\n",
    "                \n",
    "                Ya = (Y_idxs - {a_idx, b_idx}) | {a_idx}\n",
    "                Ya = [idx2index[idx] for idx in Ya]\n",
    "                Ya = cached_find_covering_node(tuple(Ya))\n",
    "                \n",
    "                Yab = (Y_idxs - {a_idx, b_idx}) | {a_idx, b_idx}\n",
    "                Yab = [idx2index[idx] for idx in Yab]\n",
    "                Yab = cached_find_covering_node(tuple(Yab))\n",
    "                \n",
    "                used_Ya = Ya.used\n",
    "                used_Yab = Yab.used\n",
    "                \n",
    "                Uab = set([index.index_id for index in used_Y]) | set([index.index_id for index in used_Ya]) | set([index.index_id for index in used_Yab])\n",
    "                \n",
    "                Yb_minus = list((Uab - {a_idx, b_idx}) | {b_idx})\n",
    "                Yb_minus = [idx2index[idx] for idx in Yb_minus]\n",
    "                Yb_minus = cached_find_covering_node(tuple(Yb_minus))\n",
    "                \n",
    "                Yb_plus = list((Y_idxs - {a_idx, b_idx}) | {b_idx})\n",
    "                Yb_plus = [idx2index[idx] for idx in Yb_plus]\n",
    "                Yb_plus = cached_find_covering_node(tuple(Yb_plus))\n",
    "                \n",
    "                quadruples = [(Y.indexes, Ya.indexes, Yb_minus.indexes, Yab.indexes), (Y.indexes, Ya.indexes, Yb_plus.indexes, Yab.indexes)]\n",
    "                \n",
    "                for Y_indexes, Ya_indexes, Yb_indexes, Yab_indexes in quadruples:\n",
    "                    cost_Y = cached_get_cost_used(tuple(Y_indexes))[0]\n",
    "                    cost_Ya = cached_get_cost_used(tuple(Ya_indexes))[0]\n",
    "                    cost_Yb = cached_get_cost_used(tuple(Yb_indexes))[0]\n",
    "                    cost_Yab = cached_get_cost_used(tuple(Yab_indexes))[0]\n",
    "                    \n",
    "                    d = abs(cost_Y - cost_Ya - cost_Yb + cost_Yab) / cost_Yab\n",
    "                    key = tuple(sorted((a_idx, b_idx)))\n",
    "                    doi_chunk[key] = max(doi_chunk.get(key, 0), d)\n",
    "    \n",
    "    return doi_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an SSB query generator object\n",
    "qg = QGEN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "template id: 1, query: \n",
      "                SELECT SUM(lo_extendedprice * lo_discount) AS revenue\n",
      "                FROM lineorder, dwdate\n",
      "                WHERE lo_orderdate = d_datekey\n",
      "                AND d_year = 1997\n",
      "                AND lo_discount BETWEEN 5 AND 7 \n",
      "                AND lo_quantity < 13;\n",
      "            , payload: {'lineorder': ['lo_extendedprice', 'lo_discount']}, predicates: {'lineorder': ['lo_orderdate', 'lo_discount', 'lo_quantity'], 'dwdate': ['d_datekey', 'd_year']}, order by: {}, group by: {}\n",
      "Number of candidate indexes: 19\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of levels in IBG: 4\n",
      "Number of nodes in IBG: 4, Total number of what-if calls: 4, Time spent on what-if calls: 0.027261018753051758\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 4/4 [00:00<00:00, 83055.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pair doi: {('ix_lineorder_lo_discount', 'ix_lineorder_lo_orderdate'): 0, ('ix_lineorder_lo_orderdate', 'ix_lineorder_lo_quantity'): 0, ('ix_lineorder_lo_orderdate', 'ix_lineorder_lo_orderdate_lo_discount'): 0, ('ix_lineorder_lo_orderdate', 'ix_lineorder_lo_orderdate_lo_quantity'): 0, ('ix_lineorder_lo_discount_lo_orderdate', 'ix_lineorder_lo_orderdate'): 0, ('ix_lineorder_lo_discount_lo_quantity', 'ix_lineorder_lo_orderdate'): 0, ('ix_lineorder_lo_orderdate', 'ix_lineorder_lo_quantity_lo_orderdate'): 0, ('ix_lineorder_lo_orderdate', 'ix_lineorder_lo_quantity_lo_discount'): 0, ('ix_lineorder_lo_orderdate', 'ix_lineorder_lo_orderdate_lo_discount_lo_quantity'): 0, ('ix_lineorder_lo_orderdate', 'ix_lineorder_lo_orderdate_lo_quantity_lo_discount'): 0, ('ix_lineorder_lo_discount_lo_orderdate_lo_quantity', 'ix_lineorder_lo_orderdate'): 0, ('ix_lineorder_lo_discount_lo_quantity_lo_orderdate', 'ix_lineorder_lo_orderdate'): 0, ('ix_lineorder_lo_orderdate', 'ix_lineorder_lo_quantity_lo_orderdate_lo_discount'): 0, ('ix_lineorder_lo_orderdate', 'ix_lineorder_lo_quantity_lo_discount_lo_orderdate'): 0, ('ix_dwdate_d_datekey', 'ix_lineorder_lo_orderdate'): 0, ('ix_dwdate_d_year', 'ix_lineorder_lo_orderdate'): 0, ('ix_dwdate_d_datekey_d_year', 'ix_lineorder_lo_orderdate'): 0, ('ix_dwdate_d_year_d_datekey', 'ix_lineorder_lo_orderdate'): 0, ('ix_lineorder_lo_discount', 'ix_lineorder_lo_quantity'): 0, ('ix_lineorder_lo_discount', 'ix_lineorder_lo_orderdate_lo_discount'): 0, ('ix_lineorder_lo_discount', 'ix_lineorder_lo_orderdate_lo_quantity'): 0, ('ix_lineorder_lo_discount', 'ix_lineorder_lo_discount_lo_orderdate'): 0, ('ix_lineorder_lo_discount', 'ix_lineorder_lo_discount_lo_quantity'): 0, ('ix_lineorder_lo_discount', 'ix_lineorder_lo_quantity_lo_orderdate'): 0, ('ix_lineorder_lo_discount', 'ix_lineorder_lo_quantity_lo_discount'): 0, ('ix_lineorder_lo_discount', 'ix_lineorder_lo_orderdate_lo_discount_lo_quantity'): 0, ('ix_lineorder_lo_discount', 'ix_lineorder_lo_orderdate_lo_quantity_lo_discount'): 0, ('ix_lineorder_lo_discount', 'ix_lineorder_lo_discount_lo_orderdate_lo_quantity'): 0, ('ix_lineorder_lo_discount', 'ix_lineorder_lo_discount_lo_quantity_lo_orderdate'): 0, ('ix_lineorder_lo_discount', 'ix_lineorder_lo_quantity_lo_orderdate_lo_discount'): 0, ('ix_lineorder_lo_discount', 'ix_lineorder_lo_quantity_lo_discount_lo_orderdate'): 0, ('ix_dwdate_d_datekey', 'ix_lineorder_lo_discount'): 0, ('ix_dwdate_d_year', 'ix_lineorder_lo_discount'): 0, ('ix_dwdate_d_datekey_d_year', 'ix_lineorder_lo_discount'): 0, ('ix_dwdate_d_year_d_datekey', 'ix_lineorder_lo_discount'): 0, ('ix_lineorder_lo_orderdate_lo_discount', 'ix_lineorder_lo_quantity'): 0, ('ix_lineorder_lo_orderdate_lo_quantity', 'ix_lineorder_lo_quantity'): 0, ('ix_lineorder_lo_discount_lo_orderdate', 'ix_lineorder_lo_quantity'): 0, ('ix_lineorder_lo_discount_lo_quantity', 'ix_lineorder_lo_quantity'): 0, ('ix_lineorder_lo_quantity', 'ix_lineorder_lo_quantity_lo_orderdate'): 0, ('ix_lineorder_lo_quantity', 'ix_lineorder_lo_quantity_lo_discount'): 0, ('ix_lineorder_lo_orderdate_lo_discount_lo_quantity', 'ix_lineorder_lo_quantity'): 0, ('ix_lineorder_lo_orderdate_lo_quantity_lo_discount', 'ix_lineorder_lo_quantity'): 0, ('ix_lineorder_lo_discount_lo_orderdate_lo_quantity', 'ix_lineorder_lo_quantity'): 0, ('ix_lineorder_lo_discount_lo_quantity_lo_orderdate', 'ix_lineorder_lo_quantity'): 0, ('ix_lineorder_lo_quantity', 'ix_lineorder_lo_quantity_lo_orderdate_lo_discount'): 0, ('ix_lineorder_lo_quantity', 'ix_lineorder_lo_quantity_lo_discount_lo_orderdate'): 0, ('ix_dwdate_d_datekey', 'ix_lineorder_lo_quantity'): 0, ('ix_dwdate_d_year', 'ix_lineorder_lo_quantity'): 0, ('ix_dwdate_d_datekey_d_year', 'ix_lineorder_lo_quantity'): 0, ('ix_dwdate_d_year_d_datekey', 'ix_lineorder_lo_quantity'): 0, ('ix_lineorder_lo_orderdate_lo_discount', 'ix_lineorder_lo_orderdate_lo_quantity'): 0, ('ix_lineorder_lo_discount_lo_orderdate', 'ix_lineorder_lo_orderdate_lo_discount'): 0, ('ix_lineorder_lo_discount_lo_quantity', 'ix_lineorder_lo_orderdate_lo_discount'): 0, ('ix_lineorder_lo_orderdate_lo_discount', 'ix_lineorder_lo_quantity_lo_orderdate'): 0, ('ix_lineorder_lo_orderdate_lo_discount', 'ix_lineorder_lo_quantity_lo_discount'): 0, ('ix_lineorder_lo_orderdate_lo_discount', 'ix_lineorder_lo_orderdate_lo_discount_lo_quantity'): 0, ('ix_lineorder_lo_orderdate_lo_discount', 'ix_lineorder_lo_orderdate_lo_quantity_lo_discount'): 0, ('ix_lineorder_lo_discount_lo_orderdate_lo_quantity', 'ix_lineorder_lo_orderdate_lo_discount'): 0, ('ix_lineorder_lo_discount_lo_quantity_lo_orderdate', 'ix_lineorder_lo_orderdate_lo_discount'): 0, ('ix_lineorder_lo_orderdate_lo_discount', 'ix_lineorder_lo_quantity_lo_orderdate_lo_discount'): 0, ('ix_lineorder_lo_orderdate_lo_discount', 'ix_lineorder_lo_quantity_lo_discount_lo_orderdate'): 0, ('ix_dwdate_d_datekey', 'ix_lineorder_lo_orderdate_lo_discount'): 0, ('ix_dwdate_d_year', 'ix_lineorder_lo_orderdate_lo_discount'): 0, ('ix_dwdate_d_datekey_d_year', 'ix_lineorder_lo_orderdate_lo_discount'): 0, ('ix_dwdate_d_year_d_datekey', 'ix_lineorder_lo_orderdate_lo_discount'): 0, ('ix_lineorder_lo_discount_lo_orderdate', 'ix_lineorder_lo_orderdate_lo_quantity'): 0, ('ix_lineorder_lo_discount_lo_quantity', 'ix_lineorder_lo_orderdate_lo_quantity'): 0, ('ix_lineorder_lo_orderdate_lo_quantity', 'ix_lineorder_lo_quantity_lo_orderdate'): 0, ('ix_lineorder_lo_orderdate_lo_quantity', 'ix_lineorder_lo_quantity_lo_discount'): 0, ('ix_lineorder_lo_orderdate_lo_discount_lo_quantity', 'ix_lineorder_lo_orderdate_lo_quantity'): 0, ('ix_lineorder_lo_orderdate_lo_quantity', 'ix_lineorder_lo_orderdate_lo_quantity_lo_discount'): 0, ('ix_lineorder_lo_discount_lo_orderdate_lo_quantity', 'ix_lineorder_lo_orderdate_lo_quantity'): 0, ('ix_lineorder_lo_discount_lo_quantity_lo_orderdate', 'ix_lineorder_lo_orderdate_lo_quantity'): 0, ('ix_lineorder_lo_orderdate_lo_quantity', 'ix_lineorder_lo_quantity_lo_orderdate_lo_discount'): 0, ('ix_lineorder_lo_orderdate_lo_quantity', 'ix_lineorder_lo_quantity_lo_discount_lo_orderdate'): 0, ('ix_dwdate_d_datekey', 'ix_lineorder_lo_orderdate_lo_quantity'): 0, ('ix_dwdate_d_year', 'ix_lineorder_lo_orderdate_lo_quantity'): 0, ('ix_dwdate_d_datekey_d_year', 'ix_lineorder_lo_orderdate_lo_quantity'): 0, ('ix_dwdate_d_year_d_datekey', 'ix_lineorder_lo_orderdate_lo_quantity'): 0, ('ix_lineorder_lo_discount_lo_orderdate', 'ix_lineorder_lo_discount_lo_quantity'): 0, ('ix_lineorder_lo_discount_lo_orderdate', 'ix_lineorder_lo_quantity_lo_orderdate'): 0, ('ix_lineorder_lo_discount_lo_orderdate', 'ix_lineorder_lo_quantity_lo_discount'): 0, ('ix_lineorder_lo_discount_lo_orderdate', 'ix_lineorder_lo_orderdate_lo_discount_lo_quantity'): 0, ('ix_lineorder_lo_discount_lo_orderdate', 'ix_lineorder_lo_orderdate_lo_quantity_lo_discount'): 0, ('ix_lineorder_lo_discount_lo_orderdate', 'ix_lineorder_lo_discount_lo_orderdate_lo_quantity'): 0, ('ix_lineorder_lo_discount_lo_orderdate', 'ix_lineorder_lo_discount_lo_quantity_lo_orderdate'): 0, ('ix_lineorder_lo_discount_lo_orderdate', 'ix_lineorder_lo_quantity_lo_orderdate_lo_discount'): 0, ('ix_lineorder_lo_discount_lo_orderdate', 'ix_lineorder_lo_quantity_lo_discount_lo_orderdate'): 0, ('ix_dwdate_d_datekey', 'ix_lineorder_lo_discount_lo_orderdate'): 0, ('ix_dwdate_d_year', 'ix_lineorder_lo_discount_lo_orderdate'): 0, ('ix_dwdate_d_datekey_d_year', 'ix_lineorder_lo_discount_lo_orderdate'): 0, ('ix_dwdate_d_year_d_datekey', 'ix_lineorder_lo_discount_lo_orderdate'): 0, ('ix_lineorder_lo_discount_lo_quantity', 'ix_lineorder_lo_quantity_lo_orderdate'): 0, ('ix_lineorder_lo_discount_lo_quantity', 'ix_lineorder_lo_quantity_lo_discount'): 0, ('ix_lineorder_lo_discount_lo_quantity', 'ix_lineorder_lo_orderdate_lo_discount_lo_quantity'): 0, ('ix_lineorder_lo_discount_lo_quantity', 'ix_lineorder_lo_orderdate_lo_quantity_lo_discount'): 0, ('ix_lineorder_lo_discount_lo_orderdate_lo_quantity', 'ix_lineorder_lo_discount_lo_quantity'): 0, ('ix_lineorder_lo_discount_lo_quantity', 'ix_lineorder_lo_discount_lo_quantity_lo_orderdate'): 0, ('ix_lineorder_lo_discount_lo_quantity', 'ix_lineorder_lo_quantity_lo_orderdate_lo_discount'): 0, ('ix_lineorder_lo_discount_lo_quantity', 'ix_lineorder_lo_quantity_lo_discount_lo_orderdate'): 0, ('ix_dwdate_d_datekey', 'ix_lineorder_lo_discount_lo_quantity'): 0, ('ix_dwdate_d_year', 'ix_lineorder_lo_discount_lo_quantity'): 0, ('ix_dwdate_d_datekey_d_year', 'ix_lineorder_lo_discount_lo_quantity'): 0, ('ix_dwdate_d_year_d_datekey', 'ix_lineorder_lo_discount_lo_quantity'): 0, ('ix_lineorder_lo_quantity_lo_discount', 'ix_lineorder_lo_quantity_lo_orderdate'): 0, ('ix_lineorder_lo_orderdate_lo_discount_lo_quantity', 'ix_lineorder_lo_quantity_lo_orderdate'): 0, ('ix_lineorder_lo_orderdate_lo_quantity_lo_discount', 'ix_lineorder_lo_quantity_lo_orderdate'): 0, ('ix_lineorder_lo_discount_lo_orderdate_lo_quantity', 'ix_lineorder_lo_quantity_lo_orderdate'): 0, ('ix_lineorder_lo_discount_lo_quantity_lo_orderdate', 'ix_lineorder_lo_quantity_lo_orderdate'): 0, ('ix_lineorder_lo_quantity_lo_orderdate', 'ix_lineorder_lo_quantity_lo_orderdate_lo_discount'): 0, ('ix_lineorder_lo_quantity_lo_discount_lo_orderdate', 'ix_lineorder_lo_quantity_lo_orderdate'): 0, ('ix_dwdate_d_datekey', 'ix_lineorder_lo_quantity_lo_orderdate'): 0, ('ix_dwdate_d_year', 'ix_lineorder_lo_quantity_lo_orderdate'): 0, ('ix_dwdate_d_datekey_d_year', 'ix_lineorder_lo_quantity_lo_orderdate'): 0, ('ix_dwdate_d_year_d_datekey', 'ix_lineorder_lo_quantity_lo_orderdate'): 0, ('ix_lineorder_lo_orderdate_lo_discount_lo_quantity', 'ix_lineorder_lo_quantity_lo_discount'): 0, ('ix_lineorder_lo_orderdate_lo_quantity_lo_discount', 'ix_lineorder_lo_quantity_lo_discount'): 0, ('ix_lineorder_lo_discount_lo_orderdate_lo_quantity', 'ix_lineorder_lo_quantity_lo_discount'): 0, ('ix_lineorder_lo_discount_lo_quantity_lo_orderdate', 'ix_lineorder_lo_quantity_lo_discount'): 0, ('ix_lineorder_lo_quantity_lo_discount', 'ix_lineorder_lo_quantity_lo_orderdate_lo_discount'): 0, ('ix_lineorder_lo_quantity_lo_discount', 'ix_lineorder_lo_quantity_lo_discount_lo_orderdate'): 0, ('ix_dwdate_d_datekey', 'ix_lineorder_lo_quantity_lo_discount'): 0, ('ix_dwdate_d_year', 'ix_lineorder_lo_quantity_lo_discount'): 0, ('ix_dwdate_d_datekey_d_year', 'ix_lineorder_lo_quantity_lo_discount'): 0, ('ix_dwdate_d_year_d_datekey', 'ix_lineorder_lo_quantity_lo_discount'): 0, ('ix_lineorder_lo_orderdate_lo_discount_lo_quantity', 'ix_lineorder_lo_orderdate_lo_quantity_lo_discount'): 0, ('ix_lineorder_lo_discount_lo_orderdate_lo_quantity', 'ix_lineorder_lo_orderdate_lo_discount_lo_quantity'): 0, ('ix_lineorder_lo_discount_lo_quantity_lo_orderdate', 'ix_lineorder_lo_orderdate_lo_discount_lo_quantity'): 0, ('ix_lineorder_lo_orderdate_lo_discount_lo_quantity', 'ix_lineorder_lo_quantity_lo_orderdate_lo_discount'): 0, ('ix_lineorder_lo_orderdate_lo_discount_lo_quantity', 'ix_lineorder_lo_quantity_lo_discount_lo_orderdate'): 0, ('ix_dwdate_d_datekey', 'ix_lineorder_lo_orderdate_lo_discount_lo_quantity'): 0, ('ix_dwdate_d_year', 'ix_lineorder_lo_orderdate_lo_discount_lo_quantity'): 0, ('ix_dwdate_d_datekey_d_year', 'ix_lineorder_lo_orderdate_lo_discount_lo_quantity'): 0, ('ix_dwdate_d_year_d_datekey', 'ix_lineorder_lo_orderdate_lo_discount_lo_quantity'): 0, ('ix_lineorder_lo_discount_lo_orderdate_lo_quantity', 'ix_lineorder_lo_orderdate_lo_quantity_lo_discount'): 0, ('ix_lineorder_lo_discount_lo_quantity_lo_orderdate', 'ix_lineorder_lo_orderdate_lo_quantity_lo_discount'): 0, ('ix_lineorder_lo_orderdate_lo_quantity_lo_discount', 'ix_lineorder_lo_quantity_lo_orderdate_lo_discount'): 0, ('ix_lineorder_lo_orderdate_lo_quantity_lo_discount', 'ix_lineorder_lo_quantity_lo_discount_lo_orderdate'): 0, ('ix_dwdate_d_datekey', 'ix_lineorder_lo_orderdate_lo_quantity_lo_discount'): 0, ('ix_dwdate_d_year', 'ix_lineorder_lo_orderdate_lo_quantity_lo_discount'): 0, ('ix_dwdate_d_datekey_d_year', 'ix_lineorder_lo_orderdate_lo_quantity_lo_discount'): 0, ('ix_dwdate_d_year_d_datekey', 'ix_lineorder_lo_orderdate_lo_quantity_lo_discount'): 0, ('ix_lineorder_lo_discount_lo_orderdate_lo_quantity', 'ix_lineorder_lo_discount_lo_quantity_lo_orderdate'): 0, ('ix_lineorder_lo_discount_lo_orderdate_lo_quantity', 'ix_lineorder_lo_quantity_lo_orderdate_lo_discount'): 0, ('ix_lineorder_lo_discount_lo_orderdate_lo_quantity', 'ix_lineorder_lo_quantity_lo_discount_lo_orderdate'): 0, ('ix_dwdate_d_datekey', 'ix_lineorder_lo_discount_lo_orderdate_lo_quantity'): 0, ('ix_dwdate_d_year', 'ix_lineorder_lo_discount_lo_orderdate_lo_quantity'): 0, ('ix_dwdate_d_datekey_d_year', 'ix_lineorder_lo_discount_lo_orderdate_lo_quantity'): 0, ('ix_dwdate_d_year_d_datekey', 'ix_lineorder_lo_discount_lo_orderdate_lo_quantity'): 0, ('ix_lineorder_lo_discount_lo_quantity_lo_orderdate', 'ix_lineorder_lo_quantity_lo_orderdate_lo_discount'): 0, ('ix_lineorder_lo_discount_lo_quantity_lo_orderdate', 'ix_lineorder_lo_quantity_lo_discount_lo_orderdate'): 0, ('ix_dwdate_d_datekey', 'ix_lineorder_lo_discount_lo_quantity_lo_orderdate'): 0, ('ix_dwdate_d_year', 'ix_lineorder_lo_discount_lo_quantity_lo_orderdate'): 0, ('ix_dwdate_d_datekey_d_year', 'ix_lineorder_lo_discount_lo_quantity_lo_orderdate'): 0, ('ix_dwdate_d_year_d_datekey', 'ix_lineorder_lo_discount_lo_quantity_lo_orderdate'): 0, ('ix_lineorder_lo_quantity_lo_discount_lo_orderdate', 'ix_lineorder_lo_quantity_lo_orderdate_lo_discount'): 0, ('ix_dwdate_d_datekey', 'ix_lineorder_lo_quantity_lo_orderdate_lo_discount'): 0, ('ix_dwdate_d_year', 'ix_lineorder_lo_quantity_lo_orderdate_lo_discount'): 0, ('ix_dwdate_d_datekey_d_year', 'ix_lineorder_lo_quantity_lo_orderdate_lo_discount'): 0, ('ix_dwdate_d_year_d_datekey', 'ix_lineorder_lo_quantity_lo_orderdate_lo_discount'): 0, ('ix_dwdate_d_datekey', 'ix_lineorder_lo_quantity_lo_discount_lo_orderdate'): 0, ('ix_dwdate_d_year', 'ix_lineorder_lo_quantity_lo_discount_lo_orderdate'): 0, ('ix_dwdate_d_datekey_d_year', 'ix_lineorder_lo_quantity_lo_discount_lo_orderdate'): 0, ('ix_dwdate_d_year_d_datekey', 'ix_lineorder_lo_quantity_lo_discount_lo_orderdate'): 0, ('ix_dwdate_d_datekey', 'ix_dwdate_d_year'): 0, ('ix_dwdate_d_datekey', 'ix_dwdate_d_datekey_d_year'): 0, ('ix_dwdate_d_datekey', 'ix_dwdate_d_year_d_datekey'): 0, ('ix_dwdate_d_datekey_d_year', 'ix_dwdate_d_year'): 1.4082553681306089e-05, ('ix_dwdate_d_year', 'ix_dwdate_d_year_d_datekey'): 4.100772795644783e-05, ('ix_dwdate_d_datekey_d_year', 'ix_dwdate_d_year_d_datekey'): 1.4082612881052915e-05}\n",
      "Time spent on computing all pair degree of interaction: 0.0074803829193115234\n",
      "0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18 -> \n",
      "0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17 -> \n",
      "0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_17 -> \n",
      "0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15 -> \n",
      "IBG     --> Cost: 1427297.95, Used indexes: ['ix_dwdate_d_year']\n",
      "What-if --> Cost: 1427297.95, Used indexes: ['ix_dwdate_d_year']\n",
      "\n",
      "Maximum benefit of adding index ix_lineorder_lo_orderdate: 0\n",
      "\n",
      "DOI between indexes ix_lineorder_lo_orderdate and ix_lineorder_lo_orderdate_lo_quantity : 0.0\n",
      "in configuration ['ix_lineorder_lo_discount', 'ix_lineorder_lo_quantity', 'ix_lineorder_lo_discount_lo_orderdate', 'ix_lineorder_lo_discount_lo_quantity', 'ix_lineorder_lo_quantity_lo_discount']\n",
      "\n",
      "DOI between indexes ix_lineorder_lo_orderdate and ix_lineorder_lo_orderdate_lo_quantity : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test IBG \n",
    "\n",
    "query = qg.generate_query(1)\n",
    "print(query)\n",
    "\n",
    "C = extract_query_indexes(qg.generate_query(1), include_cols=False)  \n",
    "\n",
    "ibg = IBG(query, C)\n",
    "\n",
    "ibg.print_ibg()\n",
    "\n",
    "# pick random subset of candidate indexes\n",
    "X = random.sample(ibg.C, 8)\n",
    "cost, used = ibg.get_cost_used(X)\n",
    "print(f\"IBG     --> Cost: {cost}, Used indexes: {[idx.index_id for idx in used]}\")\n",
    "\n",
    "cost, used = ibg._cached_get_cost_used(X)\n",
    "print(f\"What-if --> Cost: {cost}, Used indexes: {[idx.index_id for idx in used]}\")\n",
    "\n",
    "# pick two indexes and a configuration\n",
    "a = ibg.C[0]\n",
    "b = ibg.C[4] \n",
    "X = [ibg.C[1], ibg.C[2], ibg.C[5], ibg.C[6], ibg.C[8]]\n",
    "\n",
    "# compute maximum benefit of adding index 'a' \n",
    "max_benefit = ibg.compute_max_benefit(a)\n",
    "print(f\"\\nMaximum benefit of adding index {a.index_id}: {max_benefit}\")\n",
    "\n",
    "# compute degree of interaction between indexes 'a' and 'b' in configuration X\n",
    "doi = ibg.compute_doi_configuration(a, b, X)\n",
    "print(f\"\\nDOI between indexes {a.index_id} and {b.index_id} : {doi}\")\n",
    "print(f\"in configuration {[idx.index_id for idx in X]}\")\n",
    "\n",
    "# compute configuration independent degree of interaction between indexes 'a' and 'b'\n",
    "doi = ibg.get_doi_pair(a, b)\n",
    "print(f\"\\nDOI between indexes {a.index_id} and {b.index_id} : {doi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for key, value in ibg.doi.items():\n",
    "#    print(f\"doi({key[0]},   {key[1]}) = {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WFIT class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WFIT:\n",
    "\n",
    "    def __init__(self, S_0=[], max_key_columns=None, include_cols=False, max_U=30, idxCnt=50, stateCnt=200, histSize=100, rand_cnt=1):\n",
    "        # initial set of materialzed indexes\n",
    "        self.S_0 = S_0\n",
    "        # maximum number of key columns in an index\n",
    "        self.max_key_columns = max_key_columns\n",
    "        # allow include columns in indexes\n",
    "        self.include_cols = include_cols\n",
    "        # maximum number of candidate indexes for IBG \n",
    "        self.max_U = max_U\n",
    "        # parameter for maximum number of candidate indexes tracked \n",
    "        self.idxCnt = idxCnt\n",
    "        # parameter for maximum number of MTS states/configurations\n",
    "        self.stateCnt = stateCnt\n",
    "        # parameter for maximum number of historical index statistics kept\n",
    "        self.histSize = histSize\n",
    "        # parameter for number of randomized clustering iterations\n",
    "        self.rand_cnt = rand_cnt\n",
    "        # growing list of candidate indexes (initially contains S_0)\n",
    "        self.U = {index.index_id:index for index in S_0}\n",
    "        # index benefit and interaction statistics\n",
    "        self.idxStats = defaultdict(list)\n",
    "        self.intStats = defaultdict(list)\n",
    "        # list of currently monitored indexes\n",
    "        self.C = {index.index_id:index for index in S_0} \n",
    "        # list of currently materialized indexes\n",
    "        self.M = {index.index_id:index for index in S_0}  \n",
    "        # initialize stable partitions (each partition is a singleton set of indexes from S_0)\n",
    "        self.stable_partitions = [[index] for index in S_0] if S_0 else [[]]\n",
    "        self.n_pos = 0\n",
    "\n",
    "        print(f\"##################################################################\")\n",
    "        # initialize work function instance for each stable partition\n",
    "        self.W = self.initilize_WFA(self.stable_partitions)\n",
    "        # initialize current recommendations for each stable partition\n",
    "        self.current_recommendations = {i:indexes for i, indexes in enumerate(self.stable_partitions)}\n",
    "\n",
    "\n",
    "        print(f\"Initial set of materialized indexes: {[index.index_id for index in S_0]}\")\n",
    "        print(f\"Stable partitions: {[[index.index_id for index in P] for P in self.stable_partitions]}\")\n",
    "        print(f\"Initial work function instances: \")\n",
    "        for i, wf in self.W.items():\n",
    "            print(f\"\\tWFA Instance #{i}: {wf}\")\n",
    "\n",
    "        print(f\"\\nMaximum number of candidate indexes tracked: {idxCnt}\")\n",
    "        print(f\"Maximum number of MTS states/configurations: {stateCnt}\")\n",
    "        print(f\"Maximum number of historical index statistics kept: {histSize}\")\n",
    "        print(f\"Number of randomized clustering iterations: {rand_cnt}\")\n",
    "        print(f\"##################################################################\\n\")\n",
    "\n",
    "        # set random seed\n",
    "        random.seed(1234)\n",
    "\n",
    "        # track total cost \n",
    "        self.total_cost_wfit = 0\n",
    "        self.total_cost_simple = 0\n",
    "        self.total_no_index_cost = 0\n",
    "\n",
    "        # bulk drop all materialized indexes\n",
    "        conn = create_connection()\n",
    "        drop_all_indexes(conn)\n",
    "        close_connection(conn)\n",
    "\n",
    "    # initialize a WFA instance for each stable partition\n",
    "    def initilize_WFA(self, stable_partitions):\n",
    "        print(f\"Initializing WFA instances for {len(stable_partitions)} stable partitions...\")\n",
    "        W = {}\n",
    "        for i, P in enumerate(stable_partitions):\n",
    "            # initialize all MTS states, i.e. power set of indexes in the partition\n",
    "            states = [tuple(sorted(state, key=lambda x: x.index_id)) for state in powerset(P)]\n",
    "            # initialize work function instance for the partition\n",
    "            W[i] = {tuple(X):self.compute_transition_cost(self.S_0, X) for X in states}    \n",
    "\n",
    "        for i in W:\n",
    "            print(f\"WFA instance #{i}: {W[i]}\")\n",
    "\n",
    "        return W\n",
    "\n",
    "\n",
    "    # update WFIT step for next query in workload (this is the MAIN INTERFACE for generating an index configuration recommendation)\n",
    "    def process_WFIT(self, query_object, remove_stale_U=True, remove_stale_freq=1, verbose=False):\n",
    "        self.n_pos += 1        \n",
    "        previous_config = list(self.M.values())\n",
    " \n",
    "        # get estimated no index cost for the query\n",
    "        conn = create_connection()\n",
    "        self.total_no_index_cost += hypo_query_cost(conn, query_object, [], [])\n",
    "        close_connection(conn)\n",
    "\n",
    "        # generate new partitions \n",
    "        if verbose: print(f\"Generating new partitions for query #{self.n_pos}\")\n",
    "        start_time_1 = time.time()\n",
    "        new_partitions, need_to_repartition, ibg = self.choose_candidates(self.n_pos, query_object, verbose)\n",
    "        end_time_1 = time.time()\n",
    "\n",
    "        # repartition if necessary\n",
    "        start_time_2 = time.time()\n",
    "        if need_to_repartition:\n",
    "            if verbose: print(f\"Repartitioning...\")\n",
    "            self.repartition(new_partitions, verbose)\n",
    "        end_time_2 = time.time()\n",
    "        \n",
    "        # analyze the query\n",
    "        if verbose: print(f\"Analyzing query...\")\n",
    "        start_time_3 = time.time()\n",
    "        self.analyze_query(query_object, ibg, verbose)\n",
    "        end_time_3 = time.time()    \n",
    "\n",
    "        if verbose: \n",
    "            print(f\"New indexes added this round: {[index.index_id for index in (set(self.M.values()) - set(previous_config))]}\")\n",
    "            print(f\"Old indexes removed this round: {[index.index_id for index in (set(previous_config) - set(self.M.values()))]}\")\n",
    "            print(f\"Currently materialized indexes: {[index.index_id for index in self.M.values()]}\") \n",
    "\n",
    "        # remove stale indexes from U\n",
    "        if remove_stale_U and (self.n_pos % remove_stale_freq == 0):\n",
    "            if verbose: print(f\"Removing stale indexes from U...\")\n",
    "            self.remove_stale_indexes_U(verbose)\n",
    "\n",
    "        # simple recommendation, just the used indexes in the IBG root node\n",
    "        self.get_simple_recommendation_ibg(ibg)\n",
    "\n",
    "        # compute hypothetical speedup from switching to new configuration\n",
    "        new_config = list(self.M.values())\n",
    "        conn = create_connection()\n",
    "        speedup_wfit, query_execution_cost_wfit = hypo_query_speedup(conn, query_object, previous_config, new_config, [])\n",
    "        self.total_cost_wfit += query_execution_cost_wfit + sum([self.get_index_creation_cost(index) for index in (set(new_config) - set(previous_config))])    \n",
    "        # also compute speed up for the simple recommendation\n",
    "        speedup_simple, query_execution_cost_simple = hypo_query_speedup(conn, query_object, previous_config, list(ibg.root.used), [])\n",
    "        self.total_cost_simple += query_execution_cost_simple + sum([self.get_index_creation_cost(index) for index in (set(ibg.root.used) - set(previous_config))])\n",
    "        close_connection(conn)   \n",
    "\n",
    "        print(f\"*** Speedup WFIT: {speedup_wfit}, Speedup Simple: {speedup_simple}\")\n",
    "        print(f\"*** Total cost WFIT: {self.total_cost_wfit}, Total cost Simple: {self.total_cost_simple}, WFIT/Simple: {self.total_cost_wfit/self.total_cost_simple}\")\n",
    "        print(f\"*** Total cost without any indexes: {self.total_no_index_cost}\")\n",
    "\n",
    "        print(f\"Total recommendation time taken for query #{self.n_pos}: {end_time_3 - start_time_1} seconds\")\n",
    "        print(f\"(Partitioning: {end_time_1 - start_time_1} seconds, Repartitioning: {end_time_2 - start_time_2} seconds, Analyzing: {end_time_3 - start_time_3} seconds)\")\n",
    "\n",
    "\n",
    "    # Simple baseline recommendation: just the used indexes in the IBG root node, i.e. these are the indexes from \n",
    "    # the full set of candidate indexes which are used in the query plan\n",
    "    def get_simple_recommendation_ibg(self, ibg):\n",
    "        simple_recommendation = ibg.root.used  \n",
    "        wfit_recommendation = [index.index_id for i in self.current_recommendations for index in self.current_recommendations[i]]\n",
    "        print(f\"*** WFIT recommendation: {sorted(wfit_recommendation)}\")\n",
    "        print(f\"*** Simple recommendation: {sorted([index.index_id for index in simple_recommendation])}\") \n",
    "\n",
    "\n",
    "    # check for stale indexes in U and remove them\n",
    "    def remove_stale_indexes_U(self, verbose):\n",
    "        # find out which indexes have loweest benefit statistics\n",
    "        avg_benefit = {}\n",
    "        for index_id in self.U:\n",
    "            # compute average benefit of the index from all stats\n",
    "            avg_benefit[index_id] = sum([stat[1] for stat in self.idxStats[index_id]]) / len(self.idxStats[index_id])\n",
    "\n",
    "        # sort indexes by average benefit\n",
    "        sorted_indexes = sorted(avg_benefit, key=avg_benefit.get, reverse=True)\n",
    "\n",
    "        # mark all indexes with zero benefit and not in M and S_0 as stale\n",
    "        stale_indexes = set()\n",
    "        for index_id in sorted_indexes:\n",
    "            if avg_benefit[index_id] == 0 and index_id not in self.M and index_id not in self.S_0:\n",
    "                stale_indexes.add(index_id)\n",
    "\n",
    "        # remove stale indexes from U\n",
    "        print(f\"Number of indexes in U: {len(self.U)}\")\n",
    "        num_removed = 0\n",
    "        for index_id in stale_indexes:\n",
    "            #if verbose: print(f\"Removing stale index: {index_id}\")\n",
    "            del self.U[index_id]\n",
    "            #if verbose: print(f\"Number of indexes in U after removal: {len(self.U)}\")\n",
    "            num_removed += 1\n",
    "\n",
    "        # keep at most self.max_U of highest benefit indexes in U, make sure to keep all indexes in S_0 and M\n",
    "        if self.max_U is not None and len(self.U) > self.max_U:\n",
    "            for index_id in sorted_indexes[self.max_U:]:\n",
    "                if index_id not in self.M and index_id not in self.S_0 and index_id in self.U:\n",
    "                    del self.U[index_id]\n",
    "                    num_removed += 1\n",
    "\n",
    "        # remove stale indexes from stable partitions and C (not sure if this is necessary...)\n",
    "        \n",
    "        if verbose:\n",
    "            #print(f\"Average benefit of indexes:\")\n",
    "            #for index_id in sorted_indexes:\n",
    "            #    print(f\"\\tIndex {index_id}: {avg_benefit[index_id]}, Stale: {index_id in stale_indexes}\")\n",
    "                \n",
    "            print(f\"Number of indexes removed: {num_removed}, Number of indexes remaining: {len(self.U)}\")\n",
    "            #print(f\"Indexes in U: {self.U.keys()}\")\n",
    "                \n",
    "\n",
    "    # repartition the stable partitions based on the new partitions\n",
    "    def repartition(self, new_partitions, verbose):\n",
    "        # all indexes recommmendations across the WFA instances from previous round\n",
    "        S_curr = set(chain(*self.current_recommendations.values()))\n",
    "        C = set(self.C.values()) \n",
    "        S_0 = set(self.S_0)\n",
    "\n",
    "        # compute L2-norm of the work function across all partitions\n",
    "        #l2_norm_wf_old = 0\n",
    "        #for i, wf in self.W.items():\n",
    "        #    l2_norm_wf_old += sum([wf[X]**2 for X in wf])\n",
    "\n",
    "        \n",
    "        # re-initizlize WFA instances and recommendations for each new partition\n",
    "        if verbose: print(f\"Reinitializing WFA instances...\")\n",
    "        W = {}\n",
    "        recommendations = {}\n",
    "        for i, P in enumerate(new_partitions):\n",
    "            partition_all_configs = [tuple(sorted(state, key=lambda x: x.index_id)) for state in powerset(P)]\n",
    "            wf = {}\n",
    "            # initialize work function values for each state\n",
    "            print(f\"\\tNew partition # {i}\")\n",
    "            for X in partition_all_configs:\n",
    "                wf_x = 0\n",
    "                for j, wf_prev in self.W.items(): \n",
    "                    wf_x += wf_prev[tuple(sorted(set(X) & set(self.stable_partitions[j]), key=lambda x: x.index_id))]\n",
    "                \n",
    "                transition_cost_term = self.compute_transition_cost(S_0 & (set(P) - C), set(X) - C)\n",
    "                wf[X] = wf_x + transition_cost_term #- self.total_no_index_cost\n",
    "                print(f\"\\t\\t w[{tuple([index.index_id for index in X])}] --> {wf[X]}   ({wf_x} + {transition_cost_term})\")\n",
    "            \n",
    "            W[i] = wf\n",
    "            # initialize current state/recommended configuration of the WFA instance\n",
    "            recommendations[i] = list(set(P) & S_curr)\n",
    "\n",
    "        \"\"\"\n",
    "        # compute l2 norm of the work function across all partitions\n",
    "        l2_norm_wf_new = 0\n",
    "        for i, wf in W.items():\n",
    "            l2_norm_wf_new += sum([wf[X]**2 for X in wf])\n",
    "        # rescale work function values to maintain the same l2 norm (otherwise wf values will keep increasing\n",
    "        # due to the summation terms in repartitioning)\n",
    "        for i, wf in W.items():\n",
    "            for X in wf:\n",
    "                wf[X] = wf[X] * (l2_norm_wf_old / l2_norm_wf_new)    \n",
    "        \"\"\"\n",
    "\n",
    "        # replace current stable partitions, WFA instances and recommendations with the new ones\n",
    "        self.stable_partitions = new_partitions\n",
    "        self.W = W\n",
    "        self.current_recommendations = recommendations\n",
    "        if verbose: \n",
    "            print(f\"Replaced stable partitions, WFA instances and recommendations with new ones\")\n",
    "            print(f\"New WFA instances:\")\n",
    "            for i, wf in self.W.items():\n",
    "                print(f\"\\tWFA Instance #{i}:\")\n",
    "                for X, value in wf.items():\n",
    "                    print(f\"\\t\\tState: {tuple([index.index_id for index in X])}, Work function value: {value}\")\n",
    "\n",
    "        self.C = {}\n",
    "        for P in self.stable_partitions:\n",
    "            for index in P: \n",
    "                self.C[index.index_id] = index      \n",
    "\n",
    "\n",
    "    # update WFA instance on each stable partition and get index configuration recommendation\n",
    "    def analyze_query(self, query_object, ibg, verbose):\n",
    "        new_recommendations = {}\n",
    "        # update WFA instance for each stable partition\n",
    "        for i in self.W:\n",
    "            if verbose: print(f\"Updating WFA instance: {i}\")\n",
    "            self.W[i], new_recommendations[i]  = self.process_WFA(query_object, self.W[i], self.current_recommendations[i], ibg, verbose)\n",
    "\n",
    "            # materialize new recommendation\n",
    "            indexes_added = set(new_recommendations[i]) - set(self.current_recommendations[i])\n",
    "            indexes_removed = set(self.current_recommendations[i]) - set(new_recommendations[i])\n",
    "            if verbose: print(f\"\\tWFA Instance #{i}, Num States: {len(self.W[i])}, New Recommendation: {[index.index_id for index in new_recommendations[i]]} --> Indexes Added: {[index.index_id for index in indexes_added]}, Indexes Removed: {[index.index_id for index in indexes_removed]}\")\n",
    "            \n",
    "            for index in indexes_added:\n",
    "                self.M[index.index_id] = index\n",
    "            for index in indexes_removed:\n",
    "                del self.M[index.index_id]    \n",
    "                \n",
    "            self.current_recommendations[i] = new_recommendations[i]\n",
    "\n",
    "            # TODO: need to implement the following function in pg_utils\n",
    "            # ... materialize_configuration(connection, indexes_added, indexes_removed)\n",
    "            \n",
    "\n",
    "\n",
    "    # update a WFA instance for the given query    \n",
    "    def process_WFA(self, query_object, wf, S_current, ibg, verbose):\n",
    "        # update work function values for each state in the WFA instance\n",
    "        wf_new = {}\n",
    "        p = {}\n",
    "        for Y in wf.keys():\n",
    "            sorted_Y = tuple(sorted(Y, key=lambda x: x.index_id))\n",
    "            #print(f\"\\tComputing work function value for state: {tuple([index.index_id for index in sorted_Y])}, old value --> {wf[sorted_Y]}\")\n",
    "            # compute new work function value for state Y \n",
    "            min_wf_value = float('inf')\n",
    "            wf_X = {}\n",
    "            for X in wf.keys():\n",
    "                sorted_X = tuple(sorted(X, key=lambda x: x.index_id))\n",
    "                wf_term = wf[sorted_X]\n",
    "                query_cost_term = ibg.get_cost_used(list(sorted_X))[0]\n",
    "                transition_cost_term = self.compute_transition_cost(sorted_X, sorted_Y) \n",
    "                wf_value = wf_term + query_cost_term + transition_cost_term\n",
    "                #print(f'\\t\\tValue for X = {tuple([index.index_id for index in sorted_X])} -->  {wf_value}  ({wf_term} + {query_cost_term} + {transition_cost_term})')\n",
    "                \n",
    "                wf_X[sorted_X] = wf_value\n",
    "                \n",
    "                if wf_value < min_wf_value:\n",
    "                    min_wf_value = wf_value\n",
    "\n",
    "            wf_new[sorted_Y] = min_wf_value\n",
    "            min_p = []\n",
    "            for X in wf_X:\n",
    "                if wf_X[X] == min_wf_value:\n",
    "                    min_p.append(X)\n",
    "            p[sorted_Y] = min_p\n",
    "            #print(f\"\\tUpdated value: w[{tuple([index.index_id for index in sorted_Y])}] --> {wf_new[sorted_Y]}, p: {[[index.index_id for index in indexes] for indexes in p]}\")\n",
    "\n",
    "        # compute scores and find best state\n",
    "        best_score = float('inf')\n",
    "        best_state = None  \n",
    "        for Y in wf_new:\n",
    "            sorted_Y = tuple(sorted(Y, key=lambda x: x.index_id))\n",
    "            score = wf_new[sorted_Y] + self.compute_transition_cost(sorted_Y, S_current)  \n",
    "            if score <= best_score and sorted_Y in p[sorted_Y]:\n",
    "                best_score = score\n",
    "                best_state = sorted_Y  #min_p\n",
    "\n",
    "        if verbose:\n",
    "            #print(f\"\\tAll updated Work function values for WFA instance:\")\n",
    "            #for Y, value in wf_new.items():\n",
    "            #    print(f\"\\t\\tstate :{tuple([index.index_id for index in Y])} , w_value: {value}, p: {[[index.index_id for index in indexes] for indexes in p]}, score: {scores[Y]}\")\n",
    "\n",
    "            print(f\"\\tBest state: {tuple([index.index_id for index in best_state])}, Best score: {best_score}\")\n",
    "        \n",
    "        return wf_new, best_state\n",
    "\n",
    "    # compute index benefit graph for the given query and candidate indexes\n",
    "    def compute_IBG(self, query_object, candidate_indexes):\n",
    "        return IBG(query_object, candidate_indexes)\n",
    "    \n",
    "\n",
    "    # extract candidate indexes from given query\n",
    "    def extract_indexes(self, query_object):\n",
    "        return extract_query_indexes(query_object,  self.max_key_columns, self.include_cols)\n",
    "\n",
    "\n",
    "    # generate stable partitions/sets of indexes for next query in workload\n",
    "    def choose_candidates(self, n_pos, query_object, verbose):\n",
    "        # extract new candidate indexes from the query\n",
    "        new_indexes = self.extract_indexes(query_object)\n",
    "        # add new indexes to the list of all candidate indexes\n",
    "        num_new = 0\n",
    "        for index in new_indexes:\n",
    "            if index.index_id not in self.U:\n",
    "                self.U[index.index_id] = index\n",
    "                num_new += 1\n",
    "\n",
    "        #if len(self.U) > self.max_U:\n",
    "        #    raise ValueError(\"Number of candidate indexes exceeds the maximum limit. Aborting WFIT...\")\n",
    "\n",
    "\n",
    "        if verbose: \n",
    "            print(f\"Extracted {num_new} new indexes from query.\")\n",
    "            print(f\"Candidate indexes (including those currently materialized), |U| = {len(self.U)}\")\n",
    "            #print(f\"{[index.index_id for index in self.U.values()]}\")\n",
    "\n",
    "        # TODO: need mechanism to evict indexes from U that may have gone \"stale\" to prevent unbounded growth of U\n",
    "\n",
    "        \n",
    "        # compute index benefit graph for the query\n",
    "        if verbose: print(f\"Computing IBG...\")\n",
    "        ibg = self.compute_IBG(query_object, list(self.U.values()))\n",
    "\n",
    "        #if verbose: print(f\"Candidate index sizes in Mb: {[(index.index_id,index.size) for index in self.U.values()]}\")\n",
    "        \n",
    "        # update statistics for the candidate indexes (n_pos is the position of the query in the workload sequence)\n",
    "        if verbose: print(f\"Updating statistics...\")\n",
    "        self.update_stats(n_pos, ibg, verbose=False)\n",
    "\n",
    "        # non-materialized candidate indexes \n",
    "        X = [self.U[index_id] for index_id in self.U if index_id not in self.M]\n",
    "        num_indexes = self.idxCnt - len(self.M)\n",
    "\n",
    "        # determine new set of candidate indexes to monitor for upcoming workload queries\n",
    "        if verbose: print(f\"Choosing top {num_indexes} indexes from {len(X)} non-materialized candidate indexes\")\n",
    "        top_indexes = self.top_indexes(n_pos, X, num_indexes, verbose)\n",
    "        D = self.M | top_indexes\n",
    "        if verbose: print(f\"New set of indexes to monitor for upcoming workload, |D| = {len(D)}\")\n",
    "\n",
    "        # generate new partitions by clustering the new candidate set\n",
    "        if verbose: print(f\"Choosing new partitions...\")\n",
    "        new_partitions, need_to_repartition = self.choose_partition(n_pos, D, verbose)\n",
    "        if verbose:\n",
    "            print(f\"Old partitions:\")\n",
    "            for P in self.stable_partitions:\n",
    "                print(f\"\\t{[index.index_id for index in P]}\")\n",
    "            print(\"New partitions:\")\n",
    "            for P in new_partitions:\n",
    "                print(f\"\\t{[index.index_id for index in P]}\")    \n",
    "\n",
    "        return new_partitions, need_to_repartition, ibg\n",
    "    \n",
    "\n",
    "    # partition the new candidate set into clusters \n",
    "    # (need to optimize this function, currently it is a naive implementation)\n",
    "    def choose_partition(self, N_workload, D, verbose):\n",
    "        \n",
    "        # compute total loss, i.e. sum of doi across indexes from pairs of partitions\n",
    "        def compute_loss(P, current_doi):\n",
    "            loss = 0\n",
    "            for i in range(len(P)):\n",
    "                for j in range(i+1, len(P)):\n",
    "                    for a in P[i]:\n",
    "                        for b in P[j]:\n",
    "                            loss += current_doi[(a.index_id, b.index_id)]\n",
    "            return loss\n",
    "        \n",
    "        # compute current doi values for all pairs of indexes in U\n",
    "        current_doi = defaultdict(int)\n",
    "        for (a_idx, b_idx) in self.intStats.keys():\n",
    "            # take max over incremental averages (optimistic estimate)\n",
    "            current_doi[(a_idx, b_idx)] = 0\n",
    "            doi_total = 0\n",
    "            for (n, doi) in self.intStats[(a_idx, b_idx)]:\n",
    "                doi_total += doi\n",
    "                doi_avg = doi_total / (N_workload-n+1)\n",
    "                current_doi[(a_idx, b_idx)] = max(current_doi[(a_idx, b_idx)], doi_avg)\n",
    "            # save symmetric doi value\n",
    "            current_doi[(b_idx, a_idx)] = current_doi[(a_idx, b_idx)]    \n",
    "\n",
    "        #if verbose:\n",
    "        #    print(\"Current degree of interaction:\")\n",
    "        #    for pair, doi in current_doi.items():\n",
    "        #        print(f\"\\tPair {pair}: {doi}\")     \n",
    "\n",
    "        # from each current stable partition, remove indexes not in D\n",
    "        P = []\n",
    "        for partition in self.stable_partitions:\n",
    "            P.append([index for index in partition if index.index_id in D])\n",
    "\n",
    "        # add a singleton partition containing each new index in D not in C\n",
    "        need_to_repartition = False\n",
    "        for index_id, index in D.items():\n",
    "            if index_id not in self.C:\n",
    "                P.append([index])\n",
    "                need_to_repartition = True\n",
    "        \n",
    "        # set the new partition as baseline solution if feasible\n",
    "        total_configurations = sum([2**len(partition) for partition in P])\n",
    "        if total_configurations <= self.stateCnt:\n",
    "            bestSolution = P\n",
    "            bestLoss = compute_loss(P, current_doi)\n",
    "        else:\n",
    "            bestSolution = None\n",
    "            bestLoss = float('inf')    \n",
    "\n",
    "        # perform randomized clustering to find better solution\n",
    "        for i in range(self.rand_cnt):\n",
    "            # create partition of D in singletons\n",
    "            P = [[index] for index in D.values()]\n",
    "            partition2id = {tuple(partition):i for i, partition in enumerate(P)}\n",
    "            loss_cache = {}\n",
    "            \n",
    "            #if verbose:\n",
    "            #    print(f\"Parition to id map: {partition2id}\")\n",
    "\n",
    "            # merge singletons until only one partition remains\n",
    "            while True:\n",
    "                # find all feasible merge candidates pairs (i.e. pairs with loss > 0 and 2^(|Pi|+|Pj|) <= stateCnt)\n",
    "                E = []\n",
    "                E1 = []\n",
    "\n",
    "                # get loss for all pairs of partitions\n",
    "                total_configurations = sum([2**len(partition) for partition in P])\n",
    "                for i in range(len(P)):\n",
    "                    for j in range(i+1, len(P)):\n",
    "                        Pi_id = partition2id[tuple(P[i])]\n",
    "                        Pj_id = partition2id[tuple(P[j])]\n",
    "                        if (Pi_id, Pj_id) in loss_cache:\n",
    "                            loss = loss_cache[(Pi_id, Pj_id)]\n",
    "                        else:\n",
    "                            loss = compute_loss([P[i], P[j]], current_doi)\n",
    "                            loss_cache[(Pi_id, Pj_id)] = loss\n",
    "\n",
    "                        # only include feasible merge pairs, i.e. a pair which can be merged without the total number of configs exceeding stateCnt\n",
    "                        total_configrations_after_merge = total_configurations - 2**len(P[i]) - 2**len(P[j]) + 2**(len(P[i]) + len(P[j]))\n",
    "                        if loss > 0 and total_configrations_after_merge <= self.stateCnt:\n",
    "                            E.append((P[i], P[j], loss))    \n",
    "                            if len(P[i]) == 1 and len(P[j]) == 1:\n",
    "                                E1.append((P[i],P[j], loss))\n",
    "\n",
    "                #if verbose:    \n",
    "                    #print(f\"E pairs: {[[(index.index_id for index in Pi), (index.index_id for index in Pj), loss] for (Pi, Pj, loss) in E]}\")\n",
    "                    #print(f\"E1 pairs: {[[(index.index_id for index in Pi), (index.index_id for index in Pj), loss] for (Pi, Pj, loss) in E1]}\")\n",
    "\n",
    "                if len(E) == 0:\n",
    "                    break\n",
    "                \n",
    "                elif len(E1) > 0:\n",
    "                    # merge a random pair of singletons, sample randomly from E1 weighted by loss (i.e. high loss pairs more likely to be merged)\n",
    "                    Pi, Pj, loss = random.choices(E1, weights=[loss for (Pi, Pj, loss) in E1], k=1)[0]\n",
    "                    Pij_merged = Pi + Pj\n",
    "                    P.remove(Pi)\n",
    "                    P.remove(Pj)\n",
    "                    P.append(Pij_merged) \n",
    "                    E1.remove((Pi, Pj, loss))  \n",
    "                    partition2id[tuple(Pij_merged)] = len(partition2id) \n",
    "                    #if verbose: \n",
    "                    #    print(f\"Merged singleton partitions {[index.index_id for index in Pi]} and {[index.index_id for index in Pj]} with loss {loss}\")\n",
    "\n",
    "                else:\n",
    "                    # merge a random pair of partitions, sample randomly from E weighted by normalized loss  \n",
    "                    Pi, Pj, loss = random.choices(E, weights=[loss / (2**(len(Pi) + len(Pj)) - 2**len(Pi) - 2**len(Pj)) for (Pi, Pj, loss) in E], k=1)[0]\n",
    "                    Pij_merged = Pi + Pj\n",
    "                    P.remove(Pi)\n",
    "                    P.remove(Pj)\n",
    "                    P.append(Pij_merged)   \n",
    "                    E.remove((Pi, Pj, loss)) \n",
    "                    partition2id[tuple(Pij_merged)] = len(partition2id) \n",
    "                    #if verbose:\n",
    "                    #    print(f\"Merged partitions {[index.index_id for index in Pi]} and {[index.index_id for index in Pj]} with loss {loss}\")    \n",
    "\n",
    "            # check if the new solution is better than the current best solution\n",
    "            loss = compute_loss(P, current_doi)\n",
    "            if loss < bestLoss:\n",
    "                bestSolution = P\n",
    "                bestLoss = loss\n",
    "\n",
    "        return bestSolution, need_to_repartition\n",
    "\n",
    "\n",
    "    # update candidate index statistics\n",
    "    def update_stats(self, n, ibg, verbose):\n",
    "        # update index benefit statistics\n",
    "        if verbose: print(\"Updating index benefit statistics...\")\n",
    "        for index in self.U.values():\n",
    "            max_benefit = ibg.compute_max_benefit(index)\n",
    "            #if verbose: print(f\"\\tibg max benefit for index {index.index_id}: {max_benefit}\")\n",
    "            self.idxStats[index.index_id].append((n, max_benefit))\n",
    "            #if verbose: print(f\"\\tIndex {index.index_id}: {self.idxStats[index.index_id]}\")\n",
    "            # evict old stats if the size exceeds histSize\n",
    "            self.idxStats[index.index_id] = self.idxStats[index.index_id][-self.histSize:]\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Index benefit statistics:\")\n",
    "            for index_id, stats in self.idxStats.items():\n",
    "                print(f\"\\tIndex {index_id}: {stats}\")\n",
    "\n",
    "\n",
    "        # update index interaction statistics\n",
    "        if verbose: print(\"Updating index interaction statistics...\")\n",
    "        for (a_idx, b_idx) in ibg.doi.keys():\n",
    "            d = ibg.doi[(a_idx, b_idx)]\n",
    "            #if verbose: print(f\"\\tibg doi for pair ({a_idx}, {b_idx}) : {d}\")\n",
    "            if d > 0:\n",
    "                self.intStats[(a_idx, b_idx)].append((n, d))\n",
    "            #if verbose: print(f\"\\tPair ({a_idx}, {b_idx}): {self.intStats[(a_idx, b_idx)]}\")\n",
    "            # evict old stats if the size exceeds histSize\n",
    "            self.intStats[(a_idx, b_idx)] = self.intStats[(a_idx, b_idx)][-self.histSize:]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Index interaction statistics:\")\n",
    "            for pair, stats in self.intStats.items():\n",
    "                print(f\"\\tPair {pair}: {stats}\")\n",
    "\n",
    "\n",
    "    # choose top num_indexes indexes from X with highest potential benefit\n",
    "    def top_indexes(self, N_workload, X, num_indexes, verbose, positive_scores_only=False):\n",
    "        #if verbose:\n",
    "        #    print(f\"Non-materialized candidate indexes, X = {[index.index_id for index in X]}\")\n",
    "\n",
    "        # compute \"current benefit\" of each index in X (these are derived from statistics of observed benefits from recent queries)\n",
    "        score = {}\n",
    "        for index in X:\n",
    "            if len(self.idxStats[index.index_id]) == 0:\n",
    "                # zero current benefit if no statistics are available\n",
    "                current_benefit = 0\n",
    "            else:\n",
    "                # take the maximum over all incremental average benefits (optimistic estimate)\n",
    "                current_benefit = 0\n",
    "                b_total = 0\n",
    "                for (n, b) in self.idxStats[index.index_id]:\n",
    "                    b_total += b \n",
    "                    # incremental average benefit of index up to query n (higher weight/smaller denominator for more recent queries)\n",
    "                    benefit = b_total / (N_workload - n + 1)\n",
    "                    current_benefit = max(current_benefit, benefit)\n",
    "\n",
    "            # use current benefit to compute a score for the index\n",
    "            if index.index_id in self.C:\n",
    "                # if index already being monitored, then score is just current benefit\n",
    "                score[index.index_id] = current_benefit\n",
    "            else:\n",
    "                # if index not being monitored, then score is current benefit minus cost of creating the index\n",
    "                # (unmonitored indexes are penalized so that they are only chosen if they have high potential benefit, which helps keep C stable)\n",
    "                score[index.index_id] = current_benefit - self.get_index_creation_cost(index)\n",
    "\n",
    "        #if verbose:\n",
    "        #    print(\"Index scores:\")\n",
    "        #    for index_id, s in score.items():\n",
    "        #        print(f\"Index {index_id}: {s}\")\n",
    "\n",
    "        # get the top num_indexes indexes with highest scores (keep non-zero scores only)\n",
    "        if positive_scores_only:\n",
    "            top_indexes = [index_id for index_id, s in score.items() if s > 0]\n",
    "        else:\n",
    "            top_indexes = [index_id for index_id, s in score.items()]    \n",
    "        top_indexes = sorted(top_indexes, key=lambda x: score[x], reverse=True)[:num_indexes]\n",
    "        top_indexes = {index_id: self.U[index_id] for index_id in top_indexes}\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{len(top_indexes)} top indexes: {[index.index_id for index in top_indexes.values()]}\")\n",
    "\n",
    "        return top_indexes    \n",
    "\n",
    "\n",
    "    # return index creation cost (using estimated index size as proxy for cost)\n",
    "    def get_index_creation_cost(self, index):\n",
    "        # return estimated size of index\n",
    "        return index.size * 512  #* 1024 * 1024\n",
    "\n",
    "\n",
    "    # compute transition cost between two MTS states/configurations\n",
    "    def compute_transition_cost(self, S_old, S_new):\n",
    "        # find out which indexes are added\n",
    "        added_indexes = set(S_new) - set(S_old)\n",
    "        \n",
    "        # compute cost of creating the added indexes\n",
    "        transition_cost = sum([self.get_index_creation_cost(index) for index in added_indexes])\n",
    "        #print(f\"\\t\\t\\tComputing transition cost for state: {tuple([index.index_id for index in S_old])} --> {tuple([index.index_id for index in S_new])} = {transition_cost}\")\n",
    "        return transition_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test WFIT implementation on sample SSB workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# generate an SSB workload\n",
    "workload = [qg.generate_query(i) for i in range(14, 15)] * 10\n",
    "\n",
    "print(len(workload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "template id: 14, query: \n",
      "                SELECT lo_linenumber, lo_quantity, lo_orderdate  \n",
      "                FROM lineorder\n",
      "                WHERE lo_linenumber >= 2 AND lo_linenumber <= 3\n",
      "                AND lo_quantity = 19;\n",
      "            , payload: {'lineorder': ['lo_linenumber', 'lo_quantity', 'lo_orderdate']}, predicates: {'lineorder': ['lo_linenumber', 'lo_quantity']}, order by: {}, group by: {}\n"
     ]
    }
   ],
   "source": [
    "print(workload[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n",
      "Initializing WFA instances for 1 stable partitions...\n",
      "WFA instance #0: {(): 0}\n",
      "Initial set of materialized indexes: []\n",
      "Stable partitions: [[]]\n",
      "Initial work function instances: \n",
      "\tWFA Instance #0: {(): 0}\n",
      "\n",
      "Maximum number of candidate indexes tracked: 25\n",
      "Maximum number of MTS states/configurations: 1000\n",
      "Maximum number of historical index statistics kept: 100\n",
      "Number of randomized clustering iterations: 500\n",
      "##################################################################\n",
      "\n",
      "Processing query 1\n",
      "-----------------------------------\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Generating new partitions for query #1\n",
      "Extracted 4 new indexes from query.\n",
      "Candidate indexes (including those currently materialized), |U| = 4\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 4\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of levels in IBG: 2\n",
      "Number of nodes in IBG: 2, Total number of what-if calls: 2, Time spent on what-if calls: 0.008063316345214844\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 2/2 [00:00<00:00, 44858.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pair doi:\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity'): 0\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_linenumber_lo_quantity'): 0\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "('ix_lineorder_lo_linenumber_lo_quantity', 'ix_lineorder_lo_quantity'): 0\n",
      "('ix_lineorder_lo_quantity', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "('ix_lineorder_lo_linenumber_lo_quantity', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "Time spent on computing all pair degree of interaction: 0.002270936965942383\n",
      "Updating statistics...\n",
      "Choosing top 25 indexes from 4 non-materialized candidate indexes\n",
      "4 top indexes: ['ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity', 'ix_lineorder_lo_quantity_lo_linenumber', 'ix_lineorder_lo_linenumber_lo_quantity']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 4\n",
      "Choosing new partitions...\n",
      "Old partitions:\n",
      "\t[]\n",
      "New partitions:\n",
      "\t[]\n",
      "\t['ix_lineorder_lo_linenumber']\n",
      "\t['ix_lineorder_lo_quantity']\n",
      "\t['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "\t['ix_lineorder_lo_linenumber_lo_quantity']\n",
      "Repartitioning...\n",
      "Reinitializing WFA instances...\n",
      "\tNew partition # 0\n",
      "\t\t w[()] --> 0   (0 + 0)\n",
      "\tNew partition # 1\n",
      "\t\t w[()] --> 0   (0 + 0)\n",
      "\t\t w[('ix_lineorder_lo_linenumber',)] --> 680248.0   (0 + 680248.0)\n",
      "\tNew partition # 2\n",
      "\t\t w[()] --> 0   (0 + 0)\n",
      "\t\t w[('ix_lineorder_lo_quantity',)] --> 680248.0   (0 + 680248.0)\n",
      "\tNew partition # 3\n",
      "\t\t w[()] --> 0   (0 + 0)\n",
      "\t\t w[('ix_lineorder_lo_quantity_lo_linenumber',)] --> 850308.0   (0 + 850308.0)\n",
      "\tNew partition # 4\n",
      "\t\t w[()] --> 0   (0 + 0)\n",
      "\t\t w[('ix_lineorder_lo_linenumber_lo_quantity',)] --> 850308.0   (0 + 850308.0)\n",
      "Replaced stable partitions, WFA instances and recommendations with new ones\n",
      "New WFA instances:\n",
      "\tWFA Instance #0:\n",
      "\t\tState: (), Work function value: 0\n",
      "\tWFA Instance #1:\n",
      "\t\tState: (), Work function value: 0\n",
      "\t\tState: ('ix_lineorder_lo_linenumber',), Work function value: 680248.0\n",
      "\tWFA Instance #2:\n",
      "\t\tState: (), Work function value: 0\n",
      "\t\tState: ('ix_lineorder_lo_quantity',), Work function value: 680248.0\n",
      "\tWFA Instance #3:\n",
      "\t\tState: (), Work function value: 0\n",
      "\t\tState: ('ix_lineorder_lo_quantity_lo_linenumber',), Work function value: 850308.0\n",
      "\tWFA Instance #4:\n",
      "\t\tState: (), Work function value: 0\n",
      "\t\tState: ('ix_lineorder_lo_linenumber_lo_quantity',), Work function value: 850308.0\n",
      "Analyzing query...\n",
      "Updating WFA instance: 0\n",
      "\tBest state: (), Best score: 1478411.69\n",
      "\tWFA Instance #0, Num States: 1, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 1\n",
      "\tBest state: (), Best score: 1478411.69\n",
      "\tWFA Instance #1, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 2\n",
      "\tBest state: (), Best score: 1478411.69\n",
      "\tWFA Instance #2, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 3\n",
      "\tBest state: (), Best score: 1478411.69\n",
      "\tWFA Instance #3, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 4\n",
      "\tBest state: (), Best score: 1478411.69\n",
      "\tWFA Instance #4, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "New indexes added this round: []\n",
      "Old indexes removed this round: []\n",
      "Currently materialized indexes: []\n",
      "*** WFIT recommendation: []\n",
      "*** Simple recommendation: ['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "*** Speedup WFIT: 1.0, Speedup Simple: 1.0913492557007471\n",
      "*** Total cost WFIT: 1478411.69, Total cost Simple: 2204972.13, WFIT/Simple: 0.6704899666917785\n",
      "*** Total cost without any indexes: 1478411.69\n",
      "Total recommendation time taken for query #1: 0.01758742332458496 seconds\n",
      "(Partitioning: 0.01737833023071289 seconds, Repartitioning: 0.00010466575622558594 seconds, Analyzing: 0.00010323524475097656 seconds)\n",
      "\n",
      "\n",
      "\n",
      "Processing query 2\n",
      "-----------------------------------\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Generating new partitions for query #2\n",
      "Extracted 0 new indexes from query.\n",
      "Candidate indexes (including those currently materialized), |U| = 4\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 4\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of levels in IBG: 2\n",
      "Number of nodes in IBG: 2, Total number of what-if calls: 2, Time spent on what-if calls: 0.00771641731262207\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 2/2 [00:00<00:00, 35246.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pair doi:\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity'): 0\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_linenumber_lo_quantity'): 0\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "('ix_lineorder_lo_linenumber_lo_quantity', 'ix_lineorder_lo_quantity'): 0\n",
      "('ix_lineorder_lo_quantity', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "('ix_lineorder_lo_linenumber_lo_quantity', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "Time spent on computing all pair degree of interaction: 0.0021262168884277344\n",
      "Updating statistics...\n",
      "Choosing top 25 indexes from 4 non-materialized candidate indexes\n",
      "4 top indexes: ['ix_lineorder_lo_quantity_lo_linenumber', 'ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity', 'ix_lineorder_lo_linenumber_lo_quantity']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 4\n",
      "Choosing new partitions...\n",
      "Old partitions:\n",
      "\t[]\n",
      "\t['ix_lineorder_lo_linenumber']\n",
      "\t['ix_lineorder_lo_quantity']\n",
      "\t['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "\t['ix_lineorder_lo_linenumber_lo_quantity']\n",
      "New partitions:\n",
      "\t[]\n",
      "\t['ix_lineorder_lo_linenumber']\n",
      "\t['ix_lineorder_lo_quantity']\n",
      "\t['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "\t['ix_lineorder_lo_linenumber_lo_quantity']\n",
      "Analyzing query...\n",
      "Updating WFA instance: 0\n",
      "\tBest state: (), Best score: 2956823.38\n",
      "\tWFA Instance #0, Num States: 1, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 1\n",
      "\tBest state: (), Best score: 2956823.38\n",
      "\tWFA Instance #1, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 2\n",
      "\tBest state: (), Best score: 2956823.38\n",
      "\tWFA Instance #2, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 3\n",
      "\tBest state: (), Best score: 2956823.38\n",
      "\tWFA Instance #3, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 4\n",
      "\tBest state: (), Best score: 2956823.38\n",
      "\tWFA Instance #4, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "New indexes added this round: []\n",
      "Old indexes removed this round: []\n",
      "Currently materialized indexes: []\n",
      "*** WFIT recommendation: []\n",
      "*** Simple recommendation: ['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "*** Speedup WFIT: 1.0, Speedup Simple: 1.0913492557007471\n",
      "*** Total cost WFIT: 2956823.38, Total cost Simple: 4409944.26, WFIT/Simple: 0.6704899666917785\n",
      "*** Total cost without any indexes: 2956823.38\n",
      "Total recommendation time taken for query #2: 0.016427278518676758 seconds\n",
      "(Partitioning: 0.016303062438964844 seconds, Repartitioning: 0.0 seconds, Analyzing: 0.00012183189392089844 seconds)\n",
      "\n",
      "\n",
      "\n",
      "Processing query 3\n",
      "-----------------------------------\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Generating new partitions for query #3\n",
      "Extracted 0 new indexes from query.\n",
      "Candidate indexes (including those currently materialized), |U| = 4\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 4\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of levels in IBG: 2\n",
      "Number of nodes in IBG: 2, Total number of what-if calls: 2, Time spent on what-if calls: 0.008548259735107422\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 2/2 [00:00<00:00, 39568.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pair doi:\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity'): 0\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_linenumber_lo_quantity'): 0\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "('ix_lineorder_lo_linenumber_lo_quantity', 'ix_lineorder_lo_quantity'): 0\n",
      "('ix_lineorder_lo_quantity', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "('ix_lineorder_lo_linenumber_lo_quantity', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "Time spent on computing all pair degree of interaction: 0.0023310184478759766\n",
      "Updating statistics...\n",
      "Choosing top 25 indexes from 4 non-materialized candidate indexes\n",
      "4 top indexes: ['ix_lineorder_lo_quantity_lo_linenumber', 'ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity', 'ix_lineorder_lo_linenumber_lo_quantity']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 4\n",
      "Choosing new partitions...\n",
      "Old partitions:\n",
      "\t[]\n",
      "\t['ix_lineorder_lo_linenumber']\n",
      "\t['ix_lineorder_lo_quantity']\n",
      "\t['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "\t['ix_lineorder_lo_linenumber_lo_quantity']\n",
      "New partitions:\n",
      "\t[]\n",
      "\t['ix_lineorder_lo_linenumber']\n",
      "\t['ix_lineorder_lo_quantity']\n",
      "\t['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "\t['ix_lineorder_lo_linenumber_lo_quantity']\n",
      "Analyzing query...\n",
      "Updating WFA instance: 0\n",
      "\tBest state: (), Best score: 4435235.07\n",
      "\tWFA Instance #0, Num States: 1, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 1\n",
      "\tBest state: (), Best score: 4435235.07\n",
      "\tWFA Instance #1, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 2\n",
      "\tBest state: (), Best score: 4435235.07\n",
      "\tWFA Instance #2, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 3\n",
      "\tBest state: (), Best score: 4435235.07\n",
      "\tWFA Instance #3, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 4\n",
      "\tBest state: (), Best score: 4435235.07\n",
      "\tWFA Instance #4, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "New indexes added this round: []\n",
      "Old indexes removed this round: []\n",
      "Currently materialized indexes: []\n",
      "*** WFIT recommendation: []\n",
      "*** Simple recommendation: ['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "*** Speedup WFIT: 1.0, Speedup Simple: 1.0913492557007471\n",
      "*** Total cost WFIT: 4435235.07, Total cost Simple: 6614916.39, WFIT/Simple: 0.6704899666917786\n",
      "*** Total cost without any indexes: 4435235.07\n",
      "Total recommendation time taken for query #3: 0.017806529998779297 seconds\n",
      "(Partitioning: 0.017681360244750977 seconds, Repartitioning: 0.0 seconds, Analyzing: 0.00012230873107910156 seconds)\n",
      "\n",
      "\n",
      "\n",
      "Processing query 4\n",
      "-----------------------------------\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Generating new partitions for query #4\n",
      "Extracted 0 new indexes from query.\n",
      "Candidate indexes (including those currently materialized), |U| = 4\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 4\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of levels in IBG: 2\n",
      "Number of nodes in IBG: 2, Total number of what-if calls: 2, Time spent on what-if calls: 0.008642196655273438\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 2/2 [00:00<00:00, 27776.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pair doi:\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity'): 0\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_linenumber_lo_quantity'): 0\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "('ix_lineorder_lo_linenumber_lo_quantity', 'ix_lineorder_lo_quantity'): 0\n",
      "('ix_lineorder_lo_quantity', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "('ix_lineorder_lo_linenumber_lo_quantity', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "Time spent on computing all pair degree of interaction: 0.002872467041015625\n",
      "Updating statistics...\n",
      "Choosing top 25 indexes from 4 non-materialized candidate indexes\n",
      "4 top indexes: ['ix_lineorder_lo_quantity_lo_linenumber', 'ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity', 'ix_lineorder_lo_linenumber_lo_quantity']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 4\n",
      "Choosing new partitions...\n",
      "Old partitions:\n",
      "\t[]\n",
      "\t['ix_lineorder_lo_linenumber']\n",
      "\t['ix_lineorder_lo_quantity']\n",
      "\t['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "\t['ix_lineorder_lo_linenumber_lo_quantity']\n",
      "New partitions:\n",
      "\t[]\n",
      "\t['ix_lineorder_lo_linenumber']\n",
      "\t['ix_lineorder_lo_quantity']\n",
      "\t['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "\t['ix_lineorder_lo_linenumber_lo_quantity']\n",
      "Analyzing query...\n",
      "Updating WFA instance: 0\n",
      "\tBest state: (), Best score: 5913646.76\n",
      "\tWFA Instance #0, Num States: 1, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 1\n",
      "\tBest state: (), Best score: 5913646.76\n",
      "\tWFA Instance #1, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 2\n",
      "\tBest state: (), Best score: 5913646.76\n",
      "\tWFA Instance #2, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 3\n",
      "\tBest state: (), Best score: 5913646.76\n",
      "\tWFA Instance #3, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 4\n",
      "\tBest state: (), Best score: 5913646.76\n",
      "\tWFA Instance #4, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "New indexes added this round: []\n",
      "Old indexes removed this round: []\n",
      "Currently materialized indexes: []\n",
      "*** WFIT recommendation: []\n",
      "*** Simple recommendation: ['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "*** Speedup WFIT: 1.0, Speedup Simple: 1.0913492557007471\n",
      "*** Total cost WFIT: 5913646.76, Total cost Simple: 8819888.52, WFIT/Simple: 0.6704899666917785\n",
      "*** Total cost without any indexes: 5913646.76\n",
      "Total recommendation time taken for query #4: 0.01836395263671875 seconds\n",
      "(Partitioning: 0.018258094787597656 seconds, Repartitioning: 4.76837158203125e-07 seconds, Analyzing: 0.00010371208190917969 seconds)\n",
      "\n",
      "\n",
      "\n",
      "Processing query 5\n",
      "-----------------------------------\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Generating new partitions for query #5\n",
      "Extracted 0 new indexes from query.\n",
      "Candidate indexes (including those currently materialized), |U| = 4\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 4\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of levels in IBG: 2\n",
      "Number of nodes in IBG: 2, Total number of what-if calls: 2, Time spent on what-if calls: 0.008913755416870117\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 2/2 [00:00<00:00, 16480.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pair doi:\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity'): 0\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_linenumber_lo_quantity'): 0\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "('ix_lineorder_lo_linenumber_lo_quantity', 'ix_lineorder_lo_quantity'): 0\n",
      "('ix_lineorder_lo_quantity', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "('ix_lineorder_lo_linenumber_lo_quantity', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "Time spent on computing all pair degree of interaction: 0.002846240997314453\n",
      "Updating statistics...\n",
      "Choosing top 25 indexes from 4 non-materialized candidate indexes\n",
      "4 top indexes: ['ix_lineorder_lo_quantity_lo_linenumber', 'ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity', 'ix_lineorder_lo_linenumber_lo_quantity']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 4\n",
      "Choosing new partitions...\n",
      "Old partitions:\n",
      "\t[]\n",
      "\t['ix_lineorder_lo_linenumber']\n",
      "\t['ix_lineorder_lo_quantity']\n",
      "\t['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "\t['ix_lineorder_lo_linenumber_lo_quantity']\n",
      "New partitions:\n",
      "\t[]\n",
      "\t['ix_lineorder_lo_linenumber']\n",
      "\t['ix_lineorder_lo_quantity']\n",
      "\t['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "\t['ix_lineorder_lo_linenumber_lo_quantity']\n",
      "Analyzing query...\n",
      "Updating WFA instance: 0\n",
      "\tBest state: (), Best score: 7392058.449999999\n",
      "\tWFA Instance #0, Num States: 1, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 1\n",
      "\tBest state: (), Best score: 7392058.449999999\n",
      "\tWFA Instance #1, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 2\n",
      "\tBest state: (), Best score: 7392058.449999999\n",
      "\tWFA Instance #2, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 3\n",
      "\tBest state: (), Best score: 7392058.449999999\n",
      "\tWFA Instance #3, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 4\n",
      "\tBest state: (), Best score: 7392058.449999999\n",
      "\tWFA Instance #4, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "New indexes added this round: []\n",
      "Old indexes removed this round: []\n",
      "Currently materialized indexes: []\n",
      "*** WFIT recommendation: []\n",
      "*** Simple recommendation: ['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "*** Speedup WFIT: 1.0, Speedup Simple: 1.0913492557007471\n",
      "*** Total cost WFIT: 7392058.449999999, Total cost Simple: 11024860.649999999, WFIT/Simple: 0.6704899666917786\n",
      "*** Total cost without any indexes: 7392058.449999999\n",
      "Total recommendation time taken for query #5: 0.01984238624572754 seconds\n",
      "(Partitioning: 0.019722938537597656 seconds, Repartitioning: 0.0 seconds, Analyzing: 0.0001163482666015625 seconds)\n",
      "\n",
      "\n",
      "\n",
      "Processing query 6\n",
      "-----------------------------------\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Generating new partitions for query #6\n",
      "Extracted 0 new indexes from query.\n",
      "Candidate indexes (including those currently materialized), |U| = 4\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 4\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of levels in IBG: 2\n",
      "Number of nodes in IBG: 2, Total number of what-if calls: 2, Time spent on what-if calls: 0.008903026580810547\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 2/2 [00:00<00:00, 34663.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pair doi:\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity'): 0\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_linenumber_lo_quantity'): 0\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "('ix_lineorder_lo_linenumber_lo_quantity', 'ix_lineorder_lo_quantity'): 0\n",
      "('ix_lineorder_lo_quantity', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "('ix_lineorder_lo_linenumber_lo_quantity', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "Time spent on computing all pair degree of interaction: 0.002422332763671875\n",
      "Updating statistics...\n",
      "Choosing top 25 indexes from 4 non-materialized candidate indexes\n",
      "4 top indexes: ['ix_lineorder_lo_quantity_lo_linenumber', 'ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity', 'ix_lineorder_lo_linenumber_lo_quantity']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 4\n",
      "Choosing new partitions...\n",
      "Old partitions:\n",
      "\t[]\n",
      "\t['ix_lineorder_lo_linenumber']\n",
      "\t['ix_lineorder_lo_quantity']\n",
      "\t['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "\t['ix_lineorder_lo_linenumber_lo_quantity']\n",
      "New partitions:\n",
      "\t[]\n",
      "\t['ix_lineorder_lo_linenumber']\n",
      "\t['ix_lineorder_lo_quantity']\n",
      "\t['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "\t['ix_lineorder_lo_linenumber_lo_quantity']\n",
      "Analyzing query...\n",
      "Updating WFA instance: 0\n",
      "\tBest state: (), Best score: 8870470.139999999\n",
      "\tWFA Instance #0, Num States: 1, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 1\n",
      "\tBest state: (), Best score: 8870470.139999999\n",
      "\tWFA Instance #1, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 2\n",
      "\tBest state: (), Best score: 8870470.139999999\n",
      "\tWFA Instance #2, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 3\n",
      "\tBest state: (), Best score: 8870470.139999999\n",
      "\tWFA Instance #3, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 4\n",
      "\tBest state: (), Best score: 8870470.139999999\n",
      "\tWFA Instance #4, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "New indexes added this round: []\n",
      "Old indexes removed this round: []\n",
      "Currently materialized indexes: []\n",
      "*** WFIT recommendation: []\n",
      "*** Simple recommendation: ['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "*** Speedup WFIT: 1.0, Speedup Simple: 1.0913492557007471\n",
      "*** Total cost WFIT: 8870470.139999999, Total cost Simple: 13229832.779999997, WFIT/Simple: 0.6704899666917786\n",
      "*** Total cost without any indexes: 8870470.139999999\n",
      "Total recommendation time taken for query #6: 0.018468618392944336 seconds\n",
      "(Partitioning: 0.018354415893554688 seconds, Repartitioning: 2.384185791015625e-07 seconds, Analyzing: 0.00011181831359863281 seconds)\n",
      "\n",
      "\n",
      "\n",
      "Processing query 7\n",
      "-----------------------------------\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Generating new partitions for query #7\n",
      "Extracted 0 new indexes from query.\n",
      "Candidate indexes (including those currently materialized), |U| = 4\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 4\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of levels in IBG: 2\n",
      "Number of nodes in IBG: 2, Total number of what-if calls: 2, Time spent on what-if calls: 0.008655071258544922\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 2/2 [00:00<00:00, 34663.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pair doi:\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity'): 0\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_linenumber_lo_quantity'): 0\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "('ix_lineorder_lo_linenumber_lo_quantity', 'ix_lineorder_lo_quantity'): 0\n",
      "('ix_lineorder_lo_quantity', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "('ix_lineorder_lo_linenumber_lo_quantity', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "Time spent on computing all pair degree of interaction: 0.002573251724243164\n",
      "Updating statistics...\n",
      "Choosing top 25 indexes from 4 non-materialized candidate indexes\n",
      "4 top indexes: ['ix_lineorder_lo_quantity_lo_linenumber', 'ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity', 'ix_lineorder_lo_linenumber_lo_quantity']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 4\n",
      "Choosing new partitions...\n",
      "Old partitions:\n",
      "\t[]\n",
      "\t['ix_lineorder_lo_linenumber']\n",
      "\t['ix_lineorder_lo_quantity']\n",
      "\t['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "\t['ix_lineorder_lo_linenumber_lo_quantity']\n",
      "New partitions:\n",
      "\t[]\n",
      "\t['ix_lineorder_lo_linenumber']\n",
      "\t['ix_lineorder_lo_quantity']\n",
      "\t['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "\t['ix_lineorder_lo_linenumber_lo_quantity']\n",
      "Analyzing query...\n",
      "Updating WFA instance: 0\n",
      "\tBest state: (), Best score: 10348881.829999998\n",
      "\tWFA Instance #0, Num States: 1, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 1\n",
      "\tBest state: (), Best score: 10348881.829999998\n",
      "\tWFA Instance #1, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 2\n",
      "\tBest state: (), Best score: 10348881.829999998\n",
      "\tWFA Instance #2, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 3\n",
      "\tBest state: ('ix_lineorder_lo_quantity_lo_linenumber',), Best score: 10332956.91\n",
      "\tWFA Instance #3, Num States: 2, New Recommendation: ['ix_lineorder_lo_quantity_lo_linenumber'] --> Indexes Added: ['ix_lineorder_lo_quantity_lo_linenumber'], Indexes Removed: []\n",
      "Updating WFA instance: 4\n",
      "\tBest state: (), Best score: 10348881.829999998\n",
      "\tWFA Instance #4, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "New indexes added this round: ['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "Old indexes removed this round: []\n",
      "Currently materialized indexes: ['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "*** WFIT recommendation: ['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "*** Simple recommendation: ['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "*** Speedup WFIT: 1.0913492557007471, Speedup Simple: 1.0913492557007471\n",
      "*** Total cost WFIT: 11075442.27, Total cost Simple: 15434804.909999996, WFIT/Simple: 0.7175628285929532\n",
      "*** Total cost without any indexes: 10348881.829999998\n",
      "Total recommendation time taken for query #7: 0.01909947395324707 seconds\n",
      "(Partitioning: 0.018973588943481445 seconds, Repartitioning: 0.0 seconds, Analyzing: 0.0001232624053955078 seconds)\n",
      "\n",
      "\n",
      "\n",
      "Processing query 8\n",
      "-----------------------------------\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Generating new partitions for query #8\n",
      "Extracted 0 new indexes from query.\n",
      "Candidate indexes (including those currently materialized), |U| = 4\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 4\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3\n",
      "Constructing IBG...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No index scans were explicitly noted in the query plan.\n",
      "Number of levels in IBG: 2\n",
      "Number of nodes in IBG: 2, Total number of what-if calls: 2, Time spent on what-if calls: 0.008732795715332031\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 2/2 [00:00<00:00, 38479.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pair doi:\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity'): 0\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_linenumber_lo_quantity'): 0\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "('ix_lineorder_lo_linenumber_lo_quantity', 'ix_lineorder_lo_quantity'): 0\n",
      "('ix_lineorder_lo_quantity', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "('ix_lineorder_lo_linenumber_lo_quantity', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "Time spent on computing all pair degree of interaction: 0.002351045608520508\n",
      "Updating statistics...\n",
      "Choosing top 24 indexes from 3 non-materialized candidate indexes\n",
      "3 top indexes: ['ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity', 'ix_lineorder_lo_linenumber_lo_quantity']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 4\n",
      "Choosing new partitions...\n",
      "Old partitions:\n",
      "\t[]\n",
      "\t['ix_lineorder_lo_linenumber']\n",
      "\t['ix_lineorder_lo_quantity']\n",
      "\t['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "\t['ix_lineorder_lo_linenumber_lo_quantity']\n",
      "New partitions:\n",
      "\t[]\n",
      "\t['ix_lineorder_lo_linenumber']\n",
      "\t['ix_lineorder_lo_quantity']\n",
      "\t['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "\t['ix_lineorder_lo_linenumber_lo_quantity']\n",
      "Analyzing query...\n",
      "Updating WFA instance: 0\n",
      "\tBest state: (), Best score: 11827293.519999998\n",
      "\tWFA Instance #0, Num States: 1, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 1\n",
      "\tBest state: (), Best score: 11827293.519999998\n",
      "\tWFA Instance #1, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 2\n",
      "\tBest state: (), Best score: 11827293.519999998\n",
      "\tWFA Instance #2, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 3\n",
      "\tBest state: ('ix_lineorder_lo_quantity_lo_linenumber',), Best score: 11687621.04\n",
      "\tWFA Instance #3, Num States: 2, New Recommendation: ['ix_lineorder_lo_quantity_lo_linenumber'] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 4\n",
      "\tBest state: (), Best score: 11827293.519999998\n",
      "\tWFA Instance #4, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "New indexes added this round: []\n",
      "Old indexes removed this round: []\n",
      "Currently materialized indexes: ['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "*** WFIT recommendation: ['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "*** Simple recommendation: ['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "*** Speedup WFIT: 1.0, Speedup Simple: 1.0\n",
      "*** Total cost WFIT: 12430106.399999999, Total cost Simple: 16789469.039999995, WFIT/Simple: 0.740351369682147\n",
      "*** Total cost without any indexes: 11827293.519999998\n",
      "Total recommendation time taken for query #8: 0.01893901824951172 seconds\n",
      "(Partitioning: 0.018819808959960938 seconds, Repartitioning: 2.384185791015625e-07 seconds, Analyzing: 0.00011682510375976562 seconds)\n",
      "\n",
      "\n",
      "\n",
      "Processing query 9\n",
      "-----------------------------------\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Generating new partitions for query #9\n",
      "Extracted 0 new indexes from query.\n",
      "Candidate indexes (including those currently materialized), |U| = 4\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 4\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of levels in IBG: 2\n",
      "Number of nodes in IBG: 2, Total number of what-if calls: 2, Time spent on what-if calls: 0.009716510772705078\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 2/2 [00:00<00:00, 31184.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pair doi:\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity'): 0\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_linenumber_lo_quantity'): 0\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "('ix_lineorder_lo_linenumber_lo_quantity', 'ix_lineorder_lo_quantity'): 0\n",
      "('ix_lineorder_lo_quantity', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "('ix_lineorder_lo_linenumber_lo_quantity', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "Time spent on computing all pair degree of interaction: 0.0021562576293945312\n",
      "Updating statistics...\n",
      "Choosing top 24 indexes from 3 non-materialized candidate indexes\n",
      "3 top indexes: ['ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity', 'ix_lineorder_lo_linenumber_lo_quantity']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 4\n",
      "Choosing new partitions...\n",
      "Old partitions:\n",
      "\t[]\n",
      "\t['ix_lineorder_lo_linenumber']\n",
      "\t['ix_lineorder_lo_quantity']\n",
      "\t['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "\t['ix_lineorder_lo_linenumber_lo_quantity']\n",
      "New partitions:\n",
      "\t[]\n",
      "\t['ix_lineorder_lo_linenumber']\n",
      "\t['ix_lineorder_lo_quantity']\n",
      "\t['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "\t['ix_lineorder_lo_linenumber_lo_quantity']\n",
      "Analyzing query...\n",
      "Updating WFA instance: 0\n",
      "\tBest state: (), Best score: 13305705.209999997\n",
      "\tWFA Instance #0, Num States: 1, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 1\n",
      "\tBest state: (), Best score: 13305705.209999997\n",
      "\tWFA Instance #1, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 2\n",
      "\tBest state: (), Best score: 13305705.209999997\n",
      "\tWFA Instance #2, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 3\n",
      "\tBest state: ('ix_lineorder_lo_quantity_lo_linenumber',), Best score: 13042285.169999998\n",
      "\tWFA Instance #3, Num States: 2, New Recommendation: ['ix_lineorder_lo_quantity_lo_linenumber'] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 4\n",
      "\tBest state: (), Best score: 13305705.209999997\n",
      "\tWFA Instance #4, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "New indexes added this round: []\n",
      "Old indexes removed this round: []\n",
      "Currently materialized indexes: ['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "*** WFIT recommendation: ['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "*** Simple recommendation: ['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "*** Speedup WFIT: 1.0, Speedup Simple: 1.0\n",
      "*** Total cost WFIT: 13784770.529999997, Total cost Simple: 18144133.169999994, WFIT/Simple: 0.7597370676705633\n",
      "*** Total cost without any indexes: 13305705.209999997\n",
      "Total recommendation time taken for query #9: 0.02074432373046875 seconds\n",
      "(Partitioning: 0.020624399185180664 seconds, Repartitioning: 2.384185791015625e-07 seconds, Analyzing: 0.00011730194091796875 seconds)\n",
      "\n",
      "\n",
      "\n",
      "Processing query 10\n",
      "-----------------------------------\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Generating new partitions for query #10\n",
      "Extracted 0 new indexes from query.\n",
      "Candidate indexes (including those currently materialized), |U| = 4\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 4\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of levels in IBG: 2\n",
      "Number of nodes in IBG: 2, Total number of what-if calls: 2, Time spent on what-if calls: 0.008317232131958008\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 2/2 [00:00<00:00, 17697.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All pair doi:\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity'): 0\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_linenumber_lo_quantity'): 0\n",
      "('ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "('ix_lineorder_lo_linenumber_lo_quantity', 'ix_lineorder_lo_quantity'): 0\n",
      "('ix_lineorder_lo_quantity', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "('ix_lineorder_lo_linenumber_lo_quantity', 'ix_lineorder_lo_quantity_lo_linenumber'): 0\n",
      "Time spent on computing all pair degree of interaction: 0.0033686161041259766\n",
      "Updating statistics...\n",
      "Choosing top 24 indexes from 3 non-materialized candidate indexes\n",
      "3 top indexes: ['ix_lineorder_lo_linenumber', 'ix_lineorder_lo_quantity', 'ix_lineorder_lo_linenumber_lo_quantity']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 4\n",
      "Choosing new partitions...\n",
      "Old partitions:\n",
      "\t[]\n",
      "\t['ix_lineorder_lo_linenumber']\n",
      "\t['ix_lineorder_lo_quantity']\n",
      "\t['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "\t['ix_lineorder_lo_linenumber_lo_quantity']\n",
      "New partitions:\n",
      "\t[]\n",
      "\t['ix_lineorder_lo_linenumber']\n",
      "\t['ix_lineorder_lo_quantity']\n",
      "\t['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "\t['ix_lineorder_lo_linenumber_lo_quantity']\n",
      "Analyzing query...\n",
      "Updating WFA instance: 0\n",
      "\tBest state: (), Best score: 14784116.899999997\n",
      "\tWFA Instance #0, Num States: 1, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 1\n",
      "\tBest state: (), Best score: 14784116.899999997\n",
      "\tWFA Instance #1, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 2\n",
      "\tBest state: (), Best score: 14784116.899999997\n",
      "\tWFA Instance #2, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 3\n",
      "\tBest state: ('ix_lineorder_lo_quantity_lo_linenumber',), Best score: 14396949.299999997\n",
      "\tWFA Instance #3, Num States: 2, New Recommendation: ['ix_lineorder_lo_quantity_lo_linenumber'] --> Indexes Added: [], Indexes Removed: []\n",
      "Updating WFA instance: 4\n",
      "\tBest state: (), Best score: 14784116.899999997\n",
      "\tWFA Instance #4, Num States: 2, New Recommendation: [] --> Indexes Added: [], Indexes Removed: []\n",
      "New indexes added this round: []\n",
      "Old indexes removed this round: []\n",
      "Currently materialized indexes: ['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "*** WFIT recommendation: ['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "*** Simple recommendation: ['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "*** Speedup WFIT: 1.0, Speedup Simple: 1.0\n",
      "*** Total cost WFIT: 15139434.659999996, Total cost Simple: 19498797.299999993, WFIT/Simple: 0.7764291523764905\n",
      "*** Total cost without any indexes: 14784116.899999997\n",
      "Total recommendation time taken for query #10: 0.0198822021484375 seconds\n",
      "(Partitioning: 0.019763946533203125 seconds, Repartitioning: 0.0 seconds, Analyzing: 0.00011610984802246094 seconds)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instantiate WFIT\n",
    "C = extract_query_indexes(qg.generate_query(8), include_cols=False)  \n",
    "S_0 = []#C[0:1]\n",
    "#wfit = WFIT(S_0, idxCnt=20, stateCnt=1000, histSize=100, rand_cnt=10)\n",
    "wfit = WFIT(S_0, max_key_columns=3, include_cols=False, max_U=None, idxCnt=25, stateCnt=1000, rand_cnt=500)\n",
    "\n",
    "# process the workload\n",
    "for i, query in enumerate(workload):\n",
    "    print(f\"Processing query {i+1}\")\n",
    "    print('-----------------------------------')\n",
    "    wfit.process_WFIT(query, remove_stale_U=False, remove_stale_freq=1, verbose=True)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
