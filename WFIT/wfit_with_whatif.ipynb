{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WFIT Algorithm Implementation (Schnaitter 2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "import IPython\n",
    "notebook_path = IPython.get_ipython().starting_dir\n",
    "target_subdirectory_path = os.path.abspath(os.path.join(os.path.dirname(notebook_path), 'PostgreSQL'))\n",
    "sys.path.append(target_subdirectory_path)\n",
    "\n",
    "from pg_utils import *\n",
    "from ssb_qgen_class import *\n",
    "\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "import time\n",
    "import random\n",
    "from more_itertools import powerset\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from collections import deque\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index Benefit Graph (IBG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, id, indexes):\n",
    "        self.id = id\n",
    "        self.indexes = indexes\n",
    "        self.children = []\n",
    "        self.parents = []\n",
    "        self.built = False\n",
    "        self.cost = None\n",
    "        self.used = []\n",
    "\n",
    "\n",
    "# class for creating and storing the IBG\n",
    "class IBG:\n",
    "    # Class-level cache\n",
    "    #_class_cache = {}\n",
    "\n",
    "    def __init__(self, query_object, C, existing_indexes=[], ibg_max_nodes=100, normalize_doi=False, max_doi_iters_per_node=100):\n",
    "        self.q = query_object\n",
    "        self.C = C\n",
    "        self.existing_indexes = existing_indexes # indexes currently materialized in the database\n",
    "        self.normalize_doi = normalize_doi\n",
    "        print(f\"Number of candidate indexes: {len(self.C)}\")\n",
    "        #print(f\"Candidate indexes: {self.C}\")\n",
    "        \n",
    "        # create a connection session to the database\n",
    "        self.conn = create_connection()\n",
    "        # get hypothetical sizes of all the candidate indexes\n",
    "        print(\"Getting hypothetical sizes of candidate indexes...\")\n",
    "        self.get_hypo_sizes()\n",
    "        # hide existing indexes\n",
    "        bulk_hide_indexes(self.conn, self.existing_indexes)\n",
    "        \n",
    "        # map index_id to integer\n",
    "        self.idx2id = {index.index_id:i for i, index in enumerate(self.C)}\n",
    "        self.idx2index = {index.index_id:index for index in self.C}\n",
    "        #print(f\"Index id to integer mapping: {self.idx2id}\")\n",
    "        \n",
    "        # create a hash table for keeping track of all created nodes\n",
    "        self.nodes = {}\n",
    "        # create a root node\n",
    "        self.root = Node(self.get_configuration_id(self.C), self.C)\n",
    "        self.nodes[self.root.id] = self.root\n",
    "        print(f\"Created root node with id: {self.root.id}\")\n",
    "        \n",
    "        self.total_whatif_calls = 0\n",
    "        self.total_whatif_time = 0\n",
    "        self.node_count = 0\n",
    "\n",
    "        # start the IBG construction\n",
    "        print(\"Constructing IBG...\")\n",
    "        self.construct_ibg(self.root, max_nodes=ibg_max_nodes)\n",
    "        print(f\"Number of nodes in IBG: {len(self.nodes)}, Total number of what-if calls: {self.total_whatif_calls}, Time spent on what-if calls: {self.total_whatif_time}\")\n",
    "        # compute all pair degree of interaction\n",
    "        print(f\"Computing all pair degree of interaction...\")\n",
    "        start_time = time.time()\n",
    "        #self.doi = self.compute_all_pair_doi()\n",
    "        self.doi = self.compute_all_pair_doi_parallel(num_workers=4, max_nodes=50, max_iters_per_node=max_doi_iters_per_node)\n",
    "        #self.doi = self.compute_all_pair_doi_simple()\n",
    "        #self.doi = self.compute_all_pair_doi_naive(num_samples=256)\n",
    "        #print(f\"All pair doi:\")\n",
    "        #for key, value in self.doi.items():\n",
    "        #    print(f\"{key}: {value}\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"Time spent on computing all pair degree of interaction: {end_time - start_time}\")\n",
    "\n",
    "        # unhide existing indexes\n",
    "        bulk_unhide_indexes(self.conn, self.existing_indexes)\n",
    "        close_connection(self.conn)\n",
    "        self.conn = None\n",
    "\n",
    "    # assign unique string id to a configuration\n",
    "    def get_configuration_id(self, indexes):\n",
    "        # get sorted list of integer ids\n",
    "        ids = sorted([self.idx2id[idx.index_id] for idx in indexes])\n",
    "        return \"_\".join([str(i) for i in ids])\n",
    "    \n",
    "\n",
    "    # get hypothetical sizes of all the candidate indexes\n",
    "    def get_hypo_sizes(self):\n",
    "        hypo_indexes = bulk_create_hypothetical_indexes(self.conn, self.C, return_size=True)\n",
    "        for i in range(len(hypo_indexes)):\n",
    "            self.C[i].size = hypo_indexes[i][1]\n",
    "        \n",
    "    @lru_cache(maxsize=None)\n",
    "    def _get_cost_used(self, indexes):\n",
    "        # Convert indexes to a tuple to make it hashable\n",
    "        #indexes_tuple = tuple(sorted(indexes, key=lambda x: x.index_id))\n",
    "        # Check if the result is already in the class-level cache\n",
    "        #if indexes_tuple in self._class_cache:\n",
    "        #    return self._class_cache[indexes_tuple]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        if self.conn is None:\n",
    "            conn = create_connection()\n",
    "        else:\n",
    "            conn = self.conn    \n",
    "        # create hypothetical indexes\n",
    "        hypo_indexes = bulk_create_hypothetical_indexes(conn, indexes)\n",
    "        # map oid to index object\n",
    "        oid2index = {}\n",
    "        for i in range(len(hypo_indexes)):\n",
    "            oid2index[hypo_indexes[i]] = indexes[i]\n",
    "        # get cost and used indexes\n",
    "        cost, indexes_used = get_query_cost_estimate_hypo_indexes(conn, self.q.query_string, show_plan=False)\n",
    "        # map used index oids to index objects\n",
    "        used = [oid2index[oid] for oid, scan_type, scan_cost in indexes_used]\n",
    "        # drop hypothetical indexes\n",
    "        bulk_drop_hypothetical_indexes(conn)\n",
    "        if self.conn is None:\n",
    "            close_connection(conn)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Store the result in the class-level cache\n",
    "        #self._class_cache[indexes_tuple] = (cost, used)\n",
    "        self.total_whatif_calls += 1\n",
    "        self.total_whatif_time += end_time - start_time\n",
    "\n",
    "        #print(f\"Configuration: {[index.index_id for index in indexes]}, Cost: {cost}, Used indexes: {[index.index_id for index in used]}\")\n",
    "\n",
    "\n",
    "        return cost, used\n",
    "\n",
    "    # Ensure the indexes parameter is hashable\n",
    "    def _cached_get_cost_used(self, indexes):\n",
    "        return self._get_cost_used(tuple(indexes))\n",
    "\n",
    "    \n",
    "    # IBG construction\n",
    "    def construct_ibg(self, root, max_nodes=None):\n",
    "        # Obtain query optimizer's cost and used indexes\n",
    "        cost, used = self._cached_get_cost_used(root.indexes)\n",
    "        #cost, used = self._get_cost_used(root.indexes)\n",
    "        root.cost = cost\n",
    "        root.used = used\n",
    "        root.built = True\n",
    "        self.node_count += 1\n",
    "\n",
    "        num_levels = 0\n",
    "        queue = deque([root])\n",
    "        while queue:\n",
    "\n",
    "            if max_nodes is not None and self.node_count >= max_nodes:\n",
    "                break  # end if the maximum number of nodes is reached\n",
    "            \n",
    "            # Get the current level size\n",
    "            level_size = len(queue)\n",
    "            num_levels += 1\n",
    "\n",
    "            # Process all nodes at the current level\n",
    "            for _ in range(level_size):\n",
    "                Y = queue.popleft()\n",
    "                               \n",
    "                # Create children\n",
    "                for a in Y.used:\n",
    "                    # Create a new configuration with index a removed from Y\n",
    "                    X_indexes = [index for index in Y.indexes if index != a]\n",
    "                    X_id = self.get_configuration_id(X_indexes)\n",
    "                    \n",
    "                    # If X is not in the hash table, create a new node and add it to the queue\n",
    "                    if X_id not in self.nodes:\n",
    "                        self.node_count += 1\n",
    "                        print(f\"Creating node # {self.node_count}\", end=\"\\r\")\n",
    "         \n",
    "                        X = Node(X_id, X_indexes)\n",
    "                        # Obtain query optimizer's cost and used indexes\n",
    "                        cost, used = self._cached_get_cost_used(X.indexes)\n",
    "                        #cost, used = self._get_cost_used(X.indexes)\n",
    "                        X.cost = cost\n",
    "                        X.used = used\n",
    "                        X.built = True\n",
    "                        X.parents.append(Y)\n",
    "                        self.nodes[X_id] = X\n",
    "                        Y.children.append(X)\n",
    "                        queue.append(X)\n",
    "\n",
    "                    else:\n",
    "                        X = self.nodes[X_id]\n",
    "                        Y.children.append(X)\n",
    "                        X.parents.append(Y)\n",
    "\n",
    "        print(f\"Number of levels in IBG: {num_levels}\")      \n",
    "\n",
    "\n",
    "    # use IBG to obtain estimated cost and used indexes for arbitrary subset of C\n",
    "    def get_cost_used(self, X):\n",
    "        # get id of the configuration\n",
    "        id = self.get_configuration_id(X)\n",
    "        # check if the configuration is in the IBG\n",
    "        if id in self.nodes:\n",
    "            cost, used = self.nodes[id].cost, self.nodes[id].used\n",
    "        \n",
    "        # if not in the IBG, traverse the IBG to find a covering node\n",
    "        else:\n",
    "            Y = self.find_covering_node(X)              \n",
    "            cost, used = Y.cost, Y.used\n",
    "\n",
    "        return cost, used    \n",
    "\n",
    "\n",
    "    # traverses the IBG to find a node that removes indexes not in X (i.e. a covering node for X)\n",
    "    def find_covering_node(self, X):\n",
    "        X_indexes = set([index.index_id for index in X])\n",
    "        Y = self.root\n",
    "        Y_indexes = set([index.index_id for index in Y.indexes])\n",
    "        # traverse IBG to find covering node\n",
    "        while len(Y.children) > 0:               \n",
    "            # traverse down to the child node that removes an index not in X\n",
    "            child_found = False\n",
    "            for child in Y.children:\n",
    "                child_indexes = set([index.index_id for index in child.indexes])\n",
    "                child_indexes_removed = Y_indexes - child_indexes\n",
    "                child_indexes_removed_not_in_X = child_indexes_removed - X_indexes\n",
    "        \n",
    "                # check if child removes an index not in X\n",
    "                if len(child_indexes_removed_not_in_X) > 0:\n",
    "                    Y = child\n",
    "                    Y_indexes = child_indexes\n",
    "                    child_found = True\n",
    "                    break\n",
    "\n",
    "            # if no children remove indexes not in X    \n",
    "            if not child_found:\n",
    "                break    \n",
    "    \n",
    "        return Y        \n",
    "\n",
    "    # compute benefit of an index for a given configuration \n",
    "    # input X is a list of index objects and 'a' is a single index object\n",
    "    # X must not contain 'a'\n",
    "    def compute_benefit(self, a, X):\n",
    "        if a in X:\n",
    "            # zero benefit if 'a' is already in X\n",
    "            #raise ValueError(\"Index 'a' is already in X\")\n",
    "            return 0\n",
    "        \n",
    "        # get cost  for X\n",
    "        cost_X = self.get_cost_used(X)[0]\n",
    "        # create a new configuration with index a added to X\n",
    "        X_a = X + [a]\n",
    "        # get cost for X + {a}\n",
    "        cost_X_a = self.get_cost_used(X_a)[0]\n",
    "        # compute benefit\n",
    "        benefit = cost_X - cost_X_a\n",
    "        return benefit \n",
    "\n",
    "\n",
    "    # compute maximum benefit of adding an index to any possibe configuration\n",
    "    def compute_max_benefit(self, a):\n",
    "        max_benefit = float('-inf')\n",
    "        for id, node in self.nodes.items():\n",
    "            #print(f\"Computing benefit for node: {[index.index_id for index in node.indexes]}\")\n",
    "            benefit = self.compute_benefit(a, node.indexes)\n",
    "            if benefit > max_benefit:\n",
    "                max_benefit = benefit\n",
    "\n",
    "        return max_benefit\n",
    "    \n",
    "    # compute the degree of interaction between two indexes a,b in configuration X \n",
    "    def compute_doi_configuration(self, a, b, X=[]):\n",
    "        # X must not contain a or b\n",
    "        if a in X or b in X:\n",
    "            raise ValueError(\"a or b is already in X\")\n",
    "\n",
    "        doi = abs(self.compute_benefit(a, X) - self.compute_benefit(a, X + [b]))\n",
    "        if self.normalize_doi:\n",
    "            doi /= self.get_cost_used(X + [a,b])[0]   \n",
    "        return doi\n",
    "   \n",
    "    \n",
    "    # Cache the results of find_covering_node and get_cost_used to avoid redundant calculations\n",
    "    @lru_cache(maxsize=None)\n",
    "    def cached_find_covering_node(self, indexes):\n",
    "        return self.find_covering_node(tuple(indexes))\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def cached_get_cost_used(self, indexes):\n",
    "        return self.get_cost_used(tuple(indexes))\n",
    "\n",
    "\n",
    "    # computes the degree of interaction between all pairs of indexes (a,b) in candidate set C\n",
    "    # Note: doi is symmetric, i.e. doi(a,b) = doi(b,a)\n",
    "\n",
    "    # simple version of compute_all_pair_doi, without parallelization\n",
    "    def compute_all_pair_doi_simple(self):\n",
    "        # hash table for storing doi values\n",
    "        doi = {}\n",
    "        # intialize doi values to zero\n",
    "        for i in range(len(self.C)):\n",
    "            for j in range(i+1, len(self.C)):\n",
    "                d = self.compute_doi_configuration(self.C[i], self.C[j])\n",
    "                doi[tuple(sorted((self.C[i].index_id, self.C[j].index_id)))] = d\n",
    "\n",
    "        return doi\n",
    "\n",
    "    # Naive version of compute_all_pair_doi, with random sampling of configurations\n",
    "    def compute_all_pair_doi_naive(self, num_samples=100):\n",
    "        doi = {}\n",
    "        \n",
    "        for i in range(len(self.C)):\n",
    "            for j in range(i + 1, len(self.C)):\n",
    "                doi[tuple(sorted((self.C[i].index_id, self.C[j].index_id)))] = 0\n",
    "        \n",
    "        # sample random configurations: X subset C (must include empty set configuration)\n",
    "        for i in tqdm(range(num_samples), desc=\"Sampling configurations\"):\n",
    "            if i == 0:\n",
    "                X = []\n",
    "            else:\n",
    "                X = random.sample(self.C, random.randint(1, len(self.C)))\n",
    "\n",
    "            # compute doi for all pairs (a, b) in U\\X \n",
    "            for i in range(len(self.C)):\n",
    "                for j in range(i+1, len(self.C)):\n",
    "                    a = self.C[i]\n",
    "                    b = self.C[j]\n",
    "                    if a not in X and b not in X:\n",
    "                        d = self.compute_doi_configuration(a, b, X)\n",
    "                        key = tuple(sorted((a.index_id, b.index_id)))\n",
    "                        doi[key] = max(doi[key], d)\n",
    "        \n",
    "        return doi    \n",
    "\n",
    "    # original version of compute_all_pair_doi, with optional max_nodes parameter for random sampling of nodes for efficient approximation\n",
    "    def compute_all_pair_doi(self, max_nodes=None):\n",
    "        # hash table for storing doi values\n",
    "        doi = {}\n",
    "        # intialize doi values to zero\n",
    "        for i in range(len(self.C)):\n",
    "            for j in range(i+1, len(self.C)):\n",
    "                doi[tuple(sorted((self.C[i].index_id, self.C[j].index_id)))] = 0\n",
    "\n",
    "        S_idxs = set([index.index_id for index in self.C])\n",
    "\n",
    "        # sample max_nodes number of nodes from the chunk\n",
    "        if max_nodes is not None:\n",
    "            nodes_sample = random.sample(self.nodes.values(), min(max_nodes, len(self.nodes)))\n",
    "        else:\n",
    "            nodes_sample = self.nodes.values()\n",
    "\n",
    "        # iterate over each IBG node\n",
    "        for Y in tqdm(nodes_sample, desc=\"Processing nodes\"):\n",
    "            \n",
    "            # remove Y.used from S\n",
    "            Y_idxs = set([index.index_id for index in Y.indexes])\n",
    "            used_Y = Y.used\n",
    "            Y_used_idxs = set([index.index_id for index in used_Y])\n",
    "            S_Y = list(S_idxs - Y_used_idxs)\n",
    "            # iterate over all pairs of indexes in S_Y\n",
    "            for i in range(len(S_Y)):\n",
    "                for j in range(i+1, len(S_Y)):\n",
    "                    a_idx = S_Y[i]\n",
    "                    b_idx = S_Y[j]\n",
    "                     \n",
    "                    # find Ya covering node in IBG\n",
    "                    Ya = (Y_idxs - {a_idx, b_idx}) | {a_idx}\n",
    "                    Ya = [self.idx2index[idx] for idx in Ya]\n",
    "                    Ya = self.cached_find_covering_node(tuple(Ya))\n",
    "                    # find Yab covering node in IBG\n",
    "                    Yab = (Y_idxs - {a_idx, b_idx}) | {a_idx, b_idx}\n",
    "                    Yab = [self.idx2index[idx] for idx in Yab]\n",
    "                    Yab = self.cached_find_covering_node(tuple(Yab))\n",
    "\n",
    "                    #used_Y = self.cached_get_cost_used(tuple(Y.indexes))[1]\n",
    "                    #used_Ya = self.cached_get_cost_used(tuple(Ya))[1]\n",
    "                    #used_Yab = self.cached_get_cost_used(tuple(Yab))[1]\n",
    "                    used_Ya = Ya.used\n",
    "                    used_Yab = Yab.used\n",
    "\n",
    "                    Uab = set([index.index_id for index in used_Y]) | set([index.index_id for index in used_Ya]) | set([index.index_id for index in used_Yab]) \n",
    "                    # find Yb_minus covering node in IBG \n",
    "                    Yb_minus = list((Uab - {a_idx, b_idx}) | {b_idx})\n",
    "                    Yb_minus = [self.idx2index[idx] for idx in Yb_minus]\n",
    "                    Yb_minus = self.cached_find_covering_node(tuple(Yb_minus))\n",
    "                    # find Yb_plus covering node in IBG\n",
    "                    Yb_plus = list((Y_idxs - {a_idx, b_idx}) | {b_idx})\n",
    "                    Yb_plus = [self.idx2index[idx] for idx in Yb_plus]\n",
    "                    Yb_plus = self.cached_find_covering_node(tuple(Yb_plus))\n",
    "\n",
    "                    # generate quadruples\n",
    "                    quadruples = [(Y.indexes, Ya.indexes, Yb_minus.indexes, Yab.indexes), (Y.indexes, Ya.indexes, Yb_plus.indexes, Yab.indexes)]\n",
    "\n",
    "                    # compute doi using the quadruples\n",
    "                    for Y_indexes, Ya_indexes, Yb_indexes, Yab_indexes in quadruples:\n",
    "                        cost_Y = self.cached_get_cost_used(tuple(Y_indexes))[0]\n",
    "                        cost_Ya = self.cached_get_cost_used(tuple(Ya_indexes))[0]\n",
    "                        cost_Yb = self.cached_get_cost_used(tuple(Yb_indexes))[0]\n",
    "                        cost_Yab = self.cached_get_cost_used(tuple(Yab_indexes))[0]\n",
    "                        # can ignore the normalization terms in denominator to get an absolute measure of doi\n",
    "                        d = abs(cost_Y - cost_Ya - cost_Yb + cost_Yab) \n",
    "                        if self.normalize_doi: \n",
    "                            d /= cost_Yab\n",
    "                        # save doi value for the pair\n",
    "                        key = tuple(sorted((a_idx, b_idx)))\n",
    "                        doi[key] = max(doi[key], d)\n",
    "                            \n",
    "        return doi\n",
    "\n",
    "\n",
    "    # parallelized version of compute_all_pair_doi\n",
    "    def compute_all_pair_doi_parallel(self, num_workers=16, max_nodes=None, max_iters_per_node=None):\n",
    "        doi = {}\n",
    "        \n",
    "        for i in range(len(self.C)):\n",
    "            for j in range(i + 1, len(self.C)):\n",
    "                doi[tuple(sorted((self.C[i].index_id, self.C[j].index_id)))] = 0\n",
    "        \n",
    "        S_idxs = set([index.index_id for index in self.C])\n",
    "        \n",
    "        if max_nodes is not None:\n",
    "            nodes_list = random.sample(list(self.nodes.values()), min(max_nodes, len(self.nodes)))\n",
    "        else:    \n",
    "            nodes_list = list(self.nodes.values())\n",
    "        \n",
    "        chunk_size = max(1, len(nodes_list) // num_workers)\n",
    "\n",
    "        chunks = [nodes_list[i:i + chunk_size] for i in range(0, len(nodes_list), chunk_size)]\n",
    "        \n",
    "        args = [(chunk, self.C, self.idx2index, S_idxs, self.cached_find_covering_node, self.cached_get_cost_used, self.normalize_doi, max_iters_per_node) for chunk in chunks]\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            results = list(tqdm(executor.map(process_node_chunk, args), total=len(chunks), desc=\"Processing nodes in parallel\"))\n",
    "        \n",
    "        for result in results:\n",
    "            for key, value in result.items():\n",
    "                doi[key] = max(doi.get(key, 0), value)\n",
    "        \n",
    "        return doi\n",
    "    \n",
    "    \n",
    "    # get precomputed degree of interaction between a pair of indexes\n",
    "    def get_doi_pair(self, a, b):\n",
    "            return self.doi[tuple(sorted((a.index_id, b.index_id)))]\n",
    "\n",
    "\n",
    "    # function for printing the IBG, using BFS level order traversal\n",
    "    def print_ibg(self):\n",
    "        q = [self.root]\n",
    "        # traverse level by level, print all node ids in a level in a single line before moving to the next level\n",
    "        while len(q) > 0:\n",
    "            next_q = []\n",
    "            for node in q:\n",
    "                print(f\"{node.id} -> \", end=\"\")\n",
    "                for child in node.children:\n",
    "                    next_q.append(child)\n",
    "            print()\n",
    "            q = next_q  \n",
    "\n",
    "\n",
    "def process_node_chunk(args):\n",
    "    nodes_chunk, C, idx2index, S_idxs, cached_find_covering_node, cached_get_cost_used, normalize_doi, max_iters_per_node = args\n",
    "    doi_chunk = {}\n",
    "    \n",
    "    for Y in nodes_chunk:\n",
    "        Y_idxs = set([index.index_id for index in Y.indexes])\n",
    "        used_Y = Y.used\n",
    "        Y_used_idxs = set([index.index_id for index in used_Y])\n",
    "        S_Y = list(S_idxs - Y_used_idxs)\n",
    "        \n",
    "        iter_count = 0\n",
    "        for i in range(len(S_Y)):\n",
    "            for j in range(i + 1, len(S_Y)):\n",
    "                iter_count += 1\n",
    "                a_idx = S_Y[i]\n",
    "                b_idx = S_Y[j]\n",
    "                \n",
    "                Ya = (Y_idxs - {a_idx, b_idx}) | {a_idx}\n",
    "                Ya = [idx2index[idx] for idx in Ya]\n",
    "                Ya = cached_find_covering_node(tuple(Ya))\n",
    "                \n",
    "                Yab = (Y_idxs - {a_idx, b_idx}) | {a_idx, b_idx}\n",
    "                Yab = [idx2index[idx] for idx in Yab]\n",
    "                Yab = cached_find_covering_node(tuple(Yab))\n",
    "                \n",
    "                used_Ya = Ya.used\n",
    "                used_Yab = Yab.used\n",
    "                \n",
    "                Uab = set([index.index_id for index in used_Y]) | set([index.index_id for index in used_Ya]) | set([index.index_id for index in used_Yab])\n",
    "                \n",
    "                Yb_minus = list((Uab - {a_idx, b_idx}) | {b_idx})\n",
    "                Yb_minus = [idx2index[idx] for idx in Yb_minus]\n",
    "                Yb_minus = cached_find_covering_node(tuple(Yb_minus))\n",
    "                \n",
    "                Yb_plus = list((Y_idxs - {a_idx, b_idx}) | {b_idx})\n",
    "                Yb_plus = [idx2index[idx] for idx in Yb_plus]\n",
    "                Yb_plus = cached_find_covering_node(tuple(Yb_plus))\n",
    "                \n",
    "                quadruples = [(Y.indexes, Ya.indexes, Yb_minus.indexes, Yab.indexes), (Y.indexes, Ya.indexes, Yb_plus.indexes, Yab.indexes)]\n",
    "                \n",
    "                for Y_indexes, Ya_indexes, Yb_indexes, Yab_indexes in quadruples:\n",
    "                    cost_Y = cached_get_cost_used(tuple(Y_indexes))[0]\n",
    "                    cost_Ya = cached_get_cost_used(tuple(Ya_indexes))[0]\n",
    "                    cost_Yb = cached_get_cost_used(tuple(Yb_indexes))[0]\n",
    "                    cost_Yab = cached_get_cost_used(tuple(Yab_indexes))[0]\n",
    "                    \n",
    "                    d = abs(cost_Y - cost_Ya - cost_Yb + cost_Yab)\n",
    "                    #if normalize_doi: \n",
    "                    #        d /= cost_Yab\n",
    "                    key = tuple(sorted((a_idx, b_idx)))\n",
    "                    doi_chunk[key] = max(doi_chunk.get(key, 0), d)\n",
    "    \n",
    "                if iter_count >= max_iters_per_node:\n",
    "                    break\n",
    "            if iter_count >= max_iters_per_node:\n",
    "                break\n",
    "\n",
    "    return doi_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an SSB query generator object\n",
    "qg = QGEN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate indexes:\n",
      "0: Index name: ix_lineorder_lo_orderdate, Key cols: ('lo_orderdate',), Include cols: (), Current OID: None\n",
      "1: Index name: ix_lineorder_lo_discount, Key cols: ('lo_discount',), Include cols: (), Current OID: None\n",
      "2: Index name: ix_lineorder_lo_quantity, Key cols: ('lo_quantity',), Include cols: (), Current OID: None\n",
      "3: Index name: ix_lineorder_lo_orderdate_lo_discount, Key cols: ('lo_orderdate', 'lo_discount'), Include cols: (), Current OID: None\n",
      "4: Index name: ix_lineorder_lo_orderdate_lo_quantity, Key cols: ('lo_orderdate', 'lo_quantity'), Include cols: (), Current OID: None\n",
      "5: Index name: ix_lineorder_lo_discount_lo_orderdate, Key cols: ('lo_discount', 'lo_orderdate'), Include cols: (), Current OID: None\n",
      "6: Index name: ix_lineorder_lo_discount_lo_quantity, Key cols: ('lo_discount', 'lo_quantity'), Include cols: (), Current OID: None\n",
      "7: Index name: ix_lineorder_lo_quantity_lo_orderdate, Key cols: ('lo_quantity', 'lo_orderdate'), Include cols: (), Current OID: None\n",
      "8: Index name: ix_lineorder_lo_quantity_lo_discount, Key cols: ('lo_quantity', 'lo_discount'), Include cols: (), Current OID: None\n",
      "9: Index name: ix_lineorder_lo_orderdate_lo_discount_lo_quantity, Key cols: ('lo_orderdate', 'lo_discount', 'lo_quantity'), Include cols: (), Current OID: None\n",
      "10: Index name: ix_lineorder_lo_orderdate_lo_quantity_lo_discount, Key cols: ('lo_orderdate', 'lo_quantity', 'lo_discount'), Include cols: (), Current OID: None\n",
      "11: Index name: ix_lineorder_lo_discount_lo_orderdate_lo_quantity, Key cols: ('lo_discount', 'lo_orderdate', 'lo_quantity'), Include cols: (), Current OID: None\n",
      "12: Index name: ix_lineorder_lo_discount_lo_quantity_lo_orderdate, Key cols: ('lo_discount', 'lo_quantity', 'lo_orderdate'), Include cols: (), Current OID: None\n",
      "13: Index name: ix_lineorder_lo_quantity_lo_orderdate_lo_discount, Key cols: ('lo_quantity', 'lo_orderdate', 'lo_discount'), Include cols: (), Current OID: None\n",
      "14: Index name: ix_lineorder_lo_quantity_lo_discount_lo_orderdate, Key cols: ('lo_quantity', 'lo_discount', 'lo_orderdate'), Include cols: (), Current OID: None\n",
      "15: Index name: ix_dwdate_d_datekey, Key cols: ('d_datekey',), Include cols: (), Current OID: None\n",
      "16: Index name: ix_dwdate_d_yearmonthnum, Key cols: ('d_yearmonthnum',), Include cols: (), Current OID: None\n",
      "17: Index name: ix_dwdate_d_datekey_d_yearmonthnum, Key cols: ('d_datekey', 'd_yearmonthnum'), Include cols: (), Current OID: None\n",
      "18: Index name: ix_dwdate_d_yearmonthnum_d_datekey, Key cols: ('d_yearmonthnum', 'd_datekey'), Include cols: (), Current OID: None\n"
     ]
    }
   ],
   "source": [
    "query = qg.generate_query(2)\n",
    "#print(query)\n",
    "C = extract_query_indexes(query, include_cols=False)\n",
    "print(f\"Candidate indexes:\")\n",
    "for i, index in enumerate(C):\n",
    "    print(f\"{i}: {index}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'ix_lineorder_lo_quantity' on table 'lineorder' dropped successfully\n",
      "Index 'ix_lineorder_lo_quantity_lo_orderdate' on table 'lineorder' dropped successfully\n",
      "Index 'ix_part_p_category_p_partkey_p_brand' on table 'part' dropped successfully\n",
      "Index 'ix_part_p_category_p_brand_p_partkey' on table 'part' dropped successfully\n",
      "Index 'ix_lineorder_lo_quantity_lo_linenumber' on table 'lineorder' dropped successfully\n",
      "Index 'ix_lineorder_lo_linenumber_lo_quantity' on table 'lineorder' dropped successfully\n",
      "Index 'ix_lineorder_lo_orderdate_lo_discount_lo_quantity' on table 'lineorder' dropped successfully\n",
      "Index 'ix_lineorder_lo_orderdate_lo_discount' on table 'lineorder' dropped successfully\n",
      "Index 'ix_lineorder_lo_partkey' on table 'lineorder' dropped successfully\n",
      "Index 'ix_lineorder_lo_partkey_lo_orderdate' on table 'lineorder' dropped successfully\n",
      "Index 'ix_lineorder_lo_partkey_lo_suppkey' on table 'lineorder' dropped successfully\n",
      "Index 'ix_lineorder_lo_partkey_lo_orderdate_lo_suppkey' on table 'lineorder' dropped successfully\n",
      "Number of candidate indexes: 19\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of levels in IBG: 7\n",
      "Number of nodes in IBG: 16, Total number of what-if calls: 16, Time spent on what-if calls: 0.05283212661743164\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 4/4 [00:00<00:00, 69327.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent on computing all pair degree of interaction: 0.024313688278198242\n",
      "0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18 -> \n",
      "0_1_2_3_4_5_6_7_8_10_11_12_13_14_15_16_17_18 -> 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17 -> \n",
      "0_1_2_3_4_5_6_7_8_11_12_13_14_15_16_17_18 -> 0_1_2_3_4_5_6_7_8_10_11_12_13_14_15_16_17 -> 0_1_2_3_4_5_6_7_8_10_11_12_13_14_15_16_17 -> 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_17 -> \n",
      "0_1_2_4_5_6_7_8_11_12_13_14_15_16_17_18 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15_16_17 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15_16_17 -> 0_1_2_3_4_5_6_7_8_10_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15_16_17 -> 0_1_2_3_4_5_6_7_8_10_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_10_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15 -> \n",
      "0_1_2_4_5_6_7_8_11_12_13_14_15_16_17 -> 0_1_2_4_5_6_7_8_11_12_13_14_15_16_17 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_4_5_6_7_8_11_12_13_14_15_16_17 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_10_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15_16_17 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_10_11_12_13_14_15 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_10_11_12_13_14_15 -> 0_1_2_3_4_5_6_7_8_10_11_12_13_14_15 -> \n",
      "0_1_2_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15 -> \n",
      "0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# drop all existing indexes\n",
    "conn = create_connection()\n",
    "drop_all_indexes(conn)\n",
    "close_connection(conn)\n",
    "\n",
    "# test IBG \n",
    "ibg = IBG(query, C)\n",
    "ibg.print_ibg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max benefits:\n",
      "ix_lineorder_lo_orderdate_lo_discount_lo_quantity: 813408.31\n",
      "ix_lineorder_lo_orderdate_lo_quantity_lo_discount: 795873.5100000001\n",
      "ix_lineorder_lo_orderdate_lo_discount: 789687.56\n",
      "ix_dwdate_d_yearmonthnum_d_datekey: 74.40000000002328\n",
      "ix_dwdate_d_yearmonthnum: 70.40000000002328\n",
      "ix_dwdate_d_datekey_d_yearmonthnum: 23.449999999953434\n",
      "ix_lineorder_lo_orderdate: 0\n",
      "ix_lineorder_lo_discount: 0\n",
      "ix_lineorder_lo_quantity: 0\n",
      "ix_lineorder_lo_orderdate_lo_quantity: 0\n",
      "ix_lineorder_lo_discount_lo_orderdate: 0\n",
      "ix_lineorder_lo_discount_lo_quantity: 0\n",
      "ix_lineorder_lo_quantity_lo_orderdate: 0\n",
      "ix_lineorder_lo_quantity_lo_discount: 0\n",
      "ix_lineorder_lo_discount_lo_orderdate_lo_quantity: 0\n",
      "ix_lineorder_lo_discount_lo_quantity_lo_orderdate: 0\n",
      "ix_lineorder_lo_quantity_lo_orderdate_lo_discount: 0\n",
      "ix_lineorder_lo_quantity_lo_discount_lo_orderdate: 0\n",
      "ix_dwdate_d_datekey: 0\n"
     ]
    }
   ],
   "source": [
    "# compute the max benefit of each candidate index and print the sorted list\n",
    "max_benefits = [(index, ibg.compute_max_benefit(index)) for index in C]\n",
    "max_benefits = sorted(max_benefits, key=lambda x: x[1], reverse=True)\n",
    "print(f\"Max benefits:\")\n",
    "for index, benefit in max_benefits:\n",
    "    print(f\"{index.index_id}: {benefit}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IBG     --> Cost: 712281.04, Used indexes: ['ix_lineorder_lo_orderdate_lo_discount', 'ix_dwdate_d_yearmonthnum']\n",
      "What-if --> Cost: 712281.04, Used indexes: ['ix_lineorder_lo_orderdate_lo_discount', 'ix_dwdate_d_yearmonthnum']\n",
      "\n",
      "Maximum benefit of adding index ix_lineorder_lo_orderdate: 0\n",
      "\n",
      "DOI between indexes ix_lineorder_lo_orderdate and ix_lineorder_lo_orderdate_lo_quantity : 0.0\n",
      "in configuration ['ix_lineorder_lo_discount', 'ix_lineorder_lo_quantity', 'ix_lineorder_lo_discount_lo_orderdate', 'ix_lineorder_lo_discount_lo_quantity', 'ix_lineorder_lo_quantity_lo_discount']\n",
      "\n",
      "DOI between indexes ix_lineorder_lo_orderdate and ix_lineorder_lo_orderdate_lo_quantity : 0\n"
     ]
    }
   ],
   "source": [
    "# pick random subset of candidate indexes\n",
    "X = random.sample(ibg.C, 8)\n",
    "cost, used = ibg.get_cost_used(X)\n",
    "print(f\"IBG     --> Cost: {cost}, Used indexes: {[idx.index_id for idx in used]}\")\n",
    "\n",
    "cost, used = ibg._cached_get_cost_used(X)\n",
    "print(f\"What-if --> Cost: {cost}, Used indexes: {[idx.index_id for idx in used]}\")\n",
    "\n",
    "# pick two indexes and a configuration\n",
    "a = ibg.C[0]\n",
    "b = ibg.C[4] \n",
    "X = [ibg.C[1], ibg.C[2], ibg.C[5], ibg.C[6], ibg.C[8]]\n",
    "\n",
    "# compute maximum benefit of adding index 'a' \n",
    "max_benefit = ibg.compute_max_benefit(a)\n",
    "print(f\"\\nMaximum benefit of adding index {a.index_id}: {max_benefit}\")\n",
    "\n",
    "# compute degree of interaction between indexes 'a' and 'b' in configuration X\n",
    "doi = ibg.compute_doi_configuration(a, b, X)\n",
    "print(f\"\\nDOI between indexes {a.index_id} and {b.index_id} : {doi}\")\n",
    "print(f\"in configuration {[idx.index_id for idx in X]}\")\n",
    "\n",
    "# compute configuration independent degree of interaction between indexes 'a' and 'b'\n",
    "doi = ibg.get_doi_pair(a, b)\n",
    "print(f\"\\nDOI between indexes {a.index_id} and {b.index_id} : {doi}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for key, value in ibg.doi.items():\n",
    "#    print(f\"doi({key[0]},   {key[1]}) = {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load workload from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num rounds: 100\n",
      "Template sequence: [1, 9, 4, 14, 2, 6, 8, 3, 5, 13, 7, 12]\n"
     ]
    }
   ],
   "source": [
    "# load the workload from a file\n",
    "with open('ssb_static_workload_4.pkl', 'rb') as f:\n",
    "    workload_dict = pickle.load(f) \n",
    "\n",
    "workload_metadata = workload_dict['metadata']\n",
    "workload = workload_dict['workload']    \n",
    "\n",
    "print(f\"Num rounds: {workload_metadata['num_rounds']}\")\n",
    "print(f\"Template sequence: {workload_metadata['template_sequence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Greedy Baseline (no memory constraints)\n",
    "\n",
    "In this algorithm, for every new query, we extract all cadidate indexes $C$, then use HypoPG to find the subset $S \\subseteq C$ of indexes used by the query planner and materialize the indexes in $S$ which don't exist currently.  \n",
    "\n",
    "TODO: Put limit on maximum memory for indexes => use some form of bin packing to decide which indexes to keep in the configuration => need to allow index dropping as well as creation, maybe could drop/evict least recently used (LRU) indexes to make room for new indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HypoGreedy:\n",
    "\n",
    "    def __init__(self, config_memory_MB=2048, max_key_columns=3, include_cols=False):\n",
    "        self.currently_materialized_indexes = {}\n",
    "        self.total_whatif_calls = 0\n",
    "        self.total_whatif_time = 0\n",
    "        # memory budget for configuration\n",
    "        self.config_memory_MB = config_memory_MB\n",
    "        # maximum number of key columns in an index\n",
    "        self.max_key_columns = max_key_columns\n",
    "        # allow include columns in indexes\n",
    "        self.include_cols = include_cols\n",
    "        # track time \n",
    "        self.recommendation_time = []\n",
    "        self.materialization_time = []\n",
    "        self.execution_time = []\n",
    "        self.total_recommendation_time = 0\n",
    "        self.total_materialization_time = 0\n",
    "        self.total_execution_time_actual = 0\n",
    "        self.total_time = 0\n",
    "        self.current_round = 0\n",
    "\n",
    "        # index size cache\n",
    "        self.index_size = {}\n",
    "\n",
    "        # index_stats_cache\n",
    "        self.index_stats = {}\n",
    "\n",
    "        print(f\"*** Dropping all materialized indexes...\")\n",
    "        conn = create_connection()\n",
    "        drop_all_indexes(conn)\n",
    "        close_connection(conn)\n",
    "\n",
    "\n",
    "    # create statistics entry for a new index\n",
    "    def create_index_stats(self, index_id):\n",
    "        stats = {'when_selected':[], 'when_materialized':[], 'when_used':[]}\n",
    "        self.index_stats[index_id] = stats\n",
    "\n",
    "\n",
    "    # materialize new indexes and evict old indexes if necessary \n",
    "    def materialize_indexes(self, recommended_indexes):\n",
    "        # materialize new recommendation\n",
    "        indexes_added = [index for index in recommended_indexes if index.index_id not in self.currently_materialized_indexes]\n",
    "        # compute size of currently materialized indexes (use hypothetical sizes)\n",
    "        current_size = sum([self.index_size[index.index_id] for index in self.currently_materialized_indexes.values()])\n",
    "        print(f\"Size of current configuration: {current_size} MB, Space needed for new indexes: {sum([self.index_size[index.index_id] for index in indexes_added])} MB\")\n",
    "        # remove least recently used indexes to make space for new indexes if necessary\n",
    "        indexes_removed = []\n",
    "        while current_size + sum([self.index_size[index.index_id] for index in indexes_added]) > self.config_memory_MB:\n",
    "            if current_size == 0:\n",
    "                # this means the new indexes are too big to fit in the memory budget, need to pack a subset of them\n",
    "                print(f\"New indexes are too big to fit in the memory budget, packing a subset of them...\")\n",
    "                # sort the new indexes in descending order of size\n",
    "                indexes_added = sorted(indexes_added, key=lambda x: self.index_size[x.index_id], reverse=True)\n",
    "                # pack the indexes until the memory budget is reached\n",
    "                packed_indexes = []\n",
    "                for index in indexes_added:\n",
    "                    if current_size + self.index_size[index.index_id] <= self.config_memory_MB:\n",
    "                        packed_indexes.append(index)\n",
    "                        current_size += self.index_size[index.index_id]    \n",
    "                indexes_added = packed_indexes\n",
    "                break\n",
    "            \n",
    "            # find the least recently selected index\n",
    "            lru_index = min(self.currently_materialized_indexes.values(), key=lambda x: self.index_stats[x.index_id]['when_selected'][-1])\n",
    "            print(f\"Evicting index {lru_index.index_id} to make space for new indexes\")\n",
    "            # remove the index from the materialized indexes\n",
    "            indexes_removed.append(lru_index)\n",
    "            del self.currently_materialized_indexes[lru_index.index_id]\n",
    "            # update the current size\n",
    "            current_size -= self.index_size[lru_index.index_id]\n",
    "\n",
    "        for index in indexes_added:\n",
    "            self.currently_materialized_indexes[index.index_id] = index\n",
    "\n",
    "        print(f\"New indexes added this round: {[index.index_id for index in indexes_added]}\")\n",
    "        print(f\"Old indexes removed this round: {[index.index_id for index in indexes_removed]}\")\n",
    "\n",
    "        # materialize new configuration\n",
    "        conn = create_connection()\n",
    "        bulk_drop_indexes(conn, indexes_removed)\n",
    "        close_connection(conn)\n",
    "        print(f\"Materializing new indexes...\")\n",
    "        start_time = time.time()\n",
    "        conn = create_connection()\n",
    "        bulk_create_indexes(conn, indexes_added)\n",
    "        close_connection(conn)\n",
    "        end_time = time.time()\n",
    "        creation_time = end_time - start_time\n",
    "\n",
    "        # update index usage stats\n",
    "        for index in indexes_added:\n",
    "            self.index_stats[index.index_id]['when_materialized'].append(self.current_round)\n",
    "\n",
    "\n",
    "        print(f\"Currently materialized indexes: {list(self.currently_materialized_indexes.keys())}\")\n",
    "\n",
    "        return creation_time\n",
    "\n",
    "\n",
    "    # extract candidate indexes from given query\n",
    "    def extract_indexes(self, query_object):        \n",
    "        candidate_indexes = extract_query_indexes(query_object,  self.max_key_columns, self.include_cols)\n",
    "        new_indexes = [index for index in candidate_indexes if index.index_id not in self.index_size]\n",
    "        # get hypothetical idnex sizes\n",
    "        conn = create_connection()\n",
    "        new_index_sizes = get_hypothetical_index_sizes(conn, new_indexes)\n",
    "        close_connection(conn)\n",
    "        for index in new_indexes:\n",
    "            self.index_size[index.index_id] = new_index_sizes[index.index_id]\n",
    "\n",
    "        # create index stats for new indexes\n",
    "        for index in new_indexes:\n",
    "            self.create_index_stats(index.index_id)\n",
    "        # update index stats for candidate indexes\n",
    "        for index in candidate_indexes:\n",
    "            self.index_stats[index.index_id]['when_selected'].append(self.current_round)    \n",
    "\n",
    "        return candidate_indexes\n",
    "\n",
    "\n",
    "    # get hypothetical cost and used indexes for the query in the given configuration, recommend the used indexes\n",
    "    def get_recommendation(self, query_string, indexes):\n",
    "        start_time = time.time()\n",
    "        conn = create_connection()\n",
    "        # hide existing indexes\n",
    "        bulk_hide_indexes(conn, list(self.currently_materialized_indexes.values()))\n",
    "        # create hypothetical indexes\n",
    "        hypo_indexes = bulk_create_hypothetical_indexes(conn, indexes)\n",
    "        # map oid to index object\n",
    "        oid2index = {}\n",
    "        for i in range(len(hypo_indexes)):\n",
    "            oid2index[hypo_indexes[i]] = indexes[i]\n",
    "        # get cost and used indexes\n",
    "        cost, indexes_used = get_query_cost_estimate_hypo_indexes(conn, query_string, show_plan=False)\n",
    "        # map used index oids to index objects\n",
    "        used = [oid2index[oid] for oid, scan_type, scan_cost in indexes_used]\n",
    "        # drop hypothetical indexes\n",
    "        bulk_drop_hypothetical_indexes(conn)\n",
    "        # unhide existing indexes\n",
    "        bulk_unhide_indexes(conn, list(self.currently_materialized_indexes.values()))\n",
    "        close_connection(conn)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Store the result in the class-level cache\n",
    "        #self._class_cache[indexes_tuple] = (cost, used)\n",
    "        self.total_whatif_calls += 1\n",
    "        self.total_whatif_time += end_time - start_time\n",
    "\n",
    "        # remove duplicates\n",
    "        used = list({index.index_id:index for index in used}.values())\n",
    "\n",
    "        print(f\"Recommended indexes: {[index.index_id for index in used]}\")\n",
    "\n",
    "        return used\n",
    "    \n",
    "    # execute the query\n",
    "    def execute(self, query_object):\n",
    "        # restart the server before each query execution\n",
    "        restart_postgresql()\n",
    "        conn = create_connection()\n",
    "        execution_time, rows, table_access_info, index_access_info, bitmap_heapscan_info = execute_query(conn, query_object.query_string, with_explain=True, return_access_info=True)\n",
    "        close_connection(conn)\n",
    "\n",
    "        # update index usage stats\n",
    "        for index_id in index_access_info:\n",
    "            self.index_stats[index_id]['when_used'].append(self.current_round)    \n",
    "\n",
    "        print(f\"Execution time: {execution_time/1000} s\")\n",
    "        print(f\"Indexes accessed --> {list(index_access_info.keys())}\")\n",
    "        return execution_time\n",
    "\n",
    "\n",
    "    # process the query using greedy algorithm\n",
    "    def process_greedy(self, query_object):\n",
    "        self.current_round += 1\n",
    "        #print(f\"Round# {self.n_rounds}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        # extract candidate indexes\n",
    "        print(\"Extracting candidate indexes...\")\n",
    "        candidate_indexes = self.extract_indexes(query_object)\n",
    "\n",
    "        # get index recommendation\n",
    "        print(\"Getting index recommendation...\")\n",
    "        recommended_indexes = self.get_recommendation(query_object.query_string, candidate_indexes)\n",
    "        end_time = time.time()\n",
    "        recommendation_time = end_time - start_time\n",
    "        self.recommendation_time.append(recommendation_time)\n",
    "\n",
    "        # materialize indexes\n",
    "        print(\"Materializing indexes...\")\n",
    "        materialization_time = self.materialize_indexes(recommended_indexes)\n",
    "        self.materialization_time.append(materialization_time)\n",
    "\n",
    "        # execute query\n",
    "        print(\"Executing query...\")\n",
    "        execution_time = self.execute(query_object)\n",
    "        self.execution_time.append(execution_time)\n",
    "\n",
    "        self.total_recommendation_time += recommendation_time\n",
    "        self.total_materialization_time += materialization_time\n",
    "        self.total_execution_time_actual += execution_time\n",
    "        self.total_time += recommendation_time + materialization_time + (execution_time/100)\n",
    "\n",
    "        print(f\"\\nTotal recommendation time so far: {self.total_recommendation_time} s\")\n",
    "        print(f\"Total materialization time so far: {self.total_materialization_time} s\")\n",
    "        print(f\"Total execution time so far: {self.total_execution_time_actual/1000} s\")\n",
    "        print(f\"Total time spent so far: {self.total_time} s\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# instatiate HypoGreedy object\\nhg = HypoGreedy(config_memory_MB=1024*12)\\n\\n# run greedy algorithm on 100 queries\\nfor i, query_object in enumerate(workload[:50]):\\n    print(f\"\\nProcessing query # {i+1}\")\\n    hg.process_greedy(query_object)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# instatiate HypoGreedy object\n",
    "hg = HypoGreedy(config_memory_MB=1024*12)\n",
    "\n",
    "# run greedy algorithm on 100 queries\n",
    "for i, query_object in enumerate(workload[:50]):\n",
    "    print(f\"\\nProcessing query # {i+1}\")\n",
    "    hg.process_greedy(query_object)\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WFIT class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WFIT:\n",
    "\n",
    "    def __init__(self, S_0=[], max_key_columns=None, include_cols=False, max_U=30, ibg_max_nodes=100, idxCnt=25, stateCnt=500, histSize=100, rand_cnt=100, creation_cost_fudge_factor=2048):\n",
    "        # initial set of materialzed indexes\n",
    "        self.S_0 = S_0\n",
    "        # maximum number of key columns in an index\n",
    "        self.max_key_columns = max_key_columns\n",
    "        # allow include columns in indexes\n",
    "        self.include_cols = include_cols\n",
    "        # maximum number of candidate indexes for IBG \n",
    "        self.max_U = max_U\n",
    "        # maximum number of nodes in IBG\n",
    "        self.ibg_max_nodes = ibg_max_nodes\n",
    "        # parameter for maximum number of candidate indexes tracked \n",
    "        self.idxCnt = idxCnt\n",
    "        # parameter for maximum number of MTS states/configurations\n",
    "        self.stateCnt = stateCnt\n",
    "        # parameter for maximum number of historical index statistics kept\n",
    "        self.histSize = histSize\n",
    "        # parameter for number of randomized clustering iterations\n",
    "        self.rand_cnt = rand_cnt\n",
    "        # fudge factor for index creation cost\n",
    "        self.creation_cost_fudge_factor = creation_cost_fudge_factor\n",
    "        # growing list of candidate indexes (initially contains S_0)\n",
    "        self.U = {index.index_id:index for index in S_0}\n",
    "        # index benefit and interaction statistics\n",
    "        self.idxStats = defaultdict(list)\n",
    "        self.intStats = defaultdict(list)\n",
    "        # list of currently monitored indexes\n",
    "        self.C = {index.index_id:index for index in S_0} \n",
    "        # list of currently materialized indexes\n",
    "        self.M = {index.index_id:index for index in S_0}  \n",
    "        # initialize stable partitions (each partition is a singleton set of indexes from S_0)\n",
    "        self.stable_partitions = [[index] for index in S_0] if S_0 else [[]]\n",
    "        # keep track of candidate index sizes\n",
    "        self.index_size = {}\n",
    "        self.n_pos = 0\n",
    "\n",
    "        print(f\"##################################################################\")\n",
    "        # initialize work function instance for each stable partition\n",
    "        self.W = self.initilize_WFA(self.stable_partitions)\n",
    "        # initialize current recommendations for each stable partition\n",
    "        self.current_recommendations = {i:indexes for i, indexes in enumerate(self.stable_partitions)}\n",
    "\n",
    "\n",
    "        print(f\"Initial set of materialized indexes: {[index.index_id for index in S_0]}\")\n",
    "        print(f\"Stable partitions: {[[index.index_id for index in P] for P in self.stable_partitions]}\")\n",
    "        print(f\"Initial work function instances: \")\n",
    "        for i, wf in self.W.items():\n",
    "            print(f\"\\tWFA Instance #{i}: {wf}\")\n",
    "\n",
    "        print(f\"\\nMaximum number of candidate indexes tracked: {idxCnt}\")\n",
    "        print(f\"Maximum number of MTS states/configurations: {stateCnt}\")\n",
    "        print(f\"Maximum number of historical index statistics kept: {histSize}\")\n",
    "        print(f\"Number of randomized clustering iterations: {rand_cnt}\")\n",
    "\n",
    "        # bulk drop all materialized indexes\n",
    "        print(f\"*** Dropping all materialized indexes...\")\n",
    "        conn = create_connection()\n",
    "        drop_all_indexes(conn)\n",
    "        close_connection(conn)\n",
    "        print(f\"##################################################################\\n\")\n",
    "\n",
    "        # set random seed\n",
    "        random.seed(1234)\n",
    "        # track time \n",
    "        self.recommendation_time = []\n",
    "        self.materialization_time = []\n",
    "        self.execution_time = []\n",
    "        self.total_recommendation_time = 0\n",
    "        self.total_materialization_time = 0\n",
    "        self.total_execution_time_actual = 0\n",
    "        self.total_time_actual = 0\n",
    "        self.total_cost_wfit = 0\n",
    "        self.total_cost_simple = 0\n",
    "        self.total_no_index_cost = 0\n",
    "\n",
    "    # initialize a WFA instance for each stable partition\n",
    "    def initilize_WFA(self, stable_partitions):\n",
    "        print(f\"Initializing WFA instances for {len(stable_partitions)} stable partitions...\")\n",
    "        W = {}\n",
    "        for i, P in enumerate(stable_partitions):\n",
    "            # initialize all MTS states, i.e. power set of indexes in the partition\n",
    "            states = [tuple(sorted(state, key=lambda x: x.index_id)) for state in powerset(P)]\n",
    "            # initialize work function instance for the partition\n",
    "            W[i] = {tuple(X):self.compute_transition_cost(self.S_0, X) for X in states}    \n",
    "\n",
    "        for i in W:\n",
    "            print(f\"WFA instance #{i}: {W[i]}\")\n",
    "\n",
    "        return W\n",
    "\n",
    "\n",
    "    # update WFIT step for next query in workload (this is the MAIN INTERFACE for generating an index configuration recommendation)\n",
    "    def process_WFIT(self, query_object, remove_stale_U=True, remove_stale_freq=1, execute=True, materialize=True, verbose=False):\n",
    "        self.n_pos += 1        \n",
    "        previous_config = list(self.M.values())\n",
    " \n",
    "        # get estimated no index cost for the query\n",
    "        conn = create_connection()\n",
    "        self.total_no_index_cost += hypo_query_cost(conn, query_object, [], currently_materialized_indexes=list(self.M.values()))\n",
    "        close_connection(conn)\n",
    "\n",
    "        # generate new partitions \n",
    "        if verbose: print(f\"Generating new partitions for query #{self.n_pos}\")\n",
    "        start_time_1 = time.time()\n",
    "        new_partitions, need_to_repartition, ibg = self.choose_candidates(self.n_pos, query_object, verbose=verbose)\n",
    "        end_time_1 = time.time()\n",
    "\n",
    "        # repartition if necessary\n",
    "        start_time_2 = time.time()\n",
    "        if need_to_repartition:\n",
    "            if verbose: print(f\"Repartitioning...\")\n",
    "            self.repartition(new_partitions, verbose)\n",
    "        end_time_2 = time.time()\n",
    "        \n",
    "        # analyze the query and get recommendation\n",
    "        if verbose: print(f\"Analyzing query...\")\n",
    "        start_time_3 = time.time()\n",
    "        all_indexes_added, all_indexes_removed = self.analyze_query(query_object, ibg, verbose=False)\n",
    "        end_time_3 = time.time()    \n",
    "\n",
    "        # materialize new indexes\n",
    "        if materialize:\n",
    "            config_materialization_time = self.materialize_configuration(all_indexes_added, all_indexes_removed, verbose)\n",
    "        else:\n",
    "            config_materialization_time = 0\n",
    "\n",
    "        if verbose: \n",
    "            print(f\"{len(self.M)} currently materialized indexes: {[index.index_id for index in self.M.values()]}\") \n",
    "\n",
    "        # execute the query with the new configuration\n",
    "        if execute:\n",
    "            # restart the server before each query execution\n",
    "            restart_postgresql()\n",
    "            if verbose: print(f\"Executing query...\")\n",
    "            conn = create_connection()\n",
    "            execution_time, rows, table_access_info, index_access_info, bitmap_heapscan_info = execute_query(conn, query_object.query_string, with_explain=True, return_access_info=True)\n",
    "            close_connection(conn)\n",
    "            execution_time /= 1000\n",
    "            print(f\"Indexes accessed --> {list(index_access_info.keys())}\")\n",
    "        else:\n",
    "            execution_time = 0\n",
    "\n",
    "        self.total_recommendation_time += end_time_3 - start_time_1     \n",
    "        self.total_materialization_time += config_materialization_time\n",
    "        self.total_execution_time_actual += execution_time\n",
    "        self.total_time_actual += (end_time_3 - start_time_1) + config_materialization_time + execution_time   \n",
    "        self.recommendation_time.append(end_time_3 - start_time_1)\n",
    "        self.materialization_time.append(config_materialization_time)\n",
    "        self.execution_time.append(execution_time)\n",
    "        \n",
    "        # remove stale indexes from U\n",
    "        if remove_stale_U and (self.n_pos % remove_stale_freq == 0):\n",
    "            if verbose: print(f\"Removing stale indexes from U...\")\n",
    "            self.remove_stale_indexes_U(verbose)\n",
    "\n",
    "        # simple recommendation, just the used indexes in the IBG root node\n",
    "        self.get_simple_recommendation_ibg(ibg)\n",
    "\n",
    "        # compute hypothetical speedup from switching to new configuration\n",
    "        new_config = list(self.M.values())\n",
    "        if materialize:\n",
    "            materialized_indexes = self.M.values()\n",
    "        else:\n",
    "            materialized_indexes = []    \n",
    "        conn = create_connection()\n",
    "        speedup_wfit, query_execution_cost_wfit = hypo_query_speedup(conn, query_object, previous_config, new_config, materialized_indexes)\n",
    "        self.total_cost_wfit += float(query_execution_cost_wfit) + sum([float(self.get_index_creation_cost(index)) for index in (set(new_config) - set(previous_config))])  \n",
    "        # also compute speed up for the simple recommendation\n",
    "        speedup_simple, query_execution_cost_simple = hypo_query_speedup(conn, query_object, previous_config, list(ibg.root.used), materialized_indexes)\n",
    "        self.total_cost_simple += float(query_execution_cost_simple) + sum([float(self.get_index_creation_cost(index)) for index in (set(ibg.root.used) - set(previous_config))])\n",
    "        close_connection(conn)   \n",
    "\n",
    "\n",
    "        print(f\"*** Hypothetical Speedup --> WFIT: {speedup_wfit}, Simple: {speedup_simple}\")\n",
    "        print(f\"*** Hypothetical Total cost --> WFIT: {self.total_cost_wfit}, Simple: {self.total_cost_simple}, WFIT/Simple: {self.total_cost_wfit/self.total_cost_simple}\")\n",
    "        print(f\"*** Hypothetical Total Cost No-Index --> {self.total_no_index_cost}\")\n",
    "\n",
    "        print(f\"Total recommendation time taken for query #{self.n_pos}: {end_time_3 - start_time_1} seconds\")\n",
    "        print(f\"Actual execution time for query #{self.n_pos} --> {execution_time} seconds\")\n",
    "        print(f\"\\nTotal recommendation time so far --> {self.total_recommendation_time} seconds\")\n",
    "        print(f\"Total materialization time so far --> {self.total_materialization_time} seconds\")\n",
    "        print(f\"Total execution time so far --> {self.total_execution_time_actual} seconds\")\n",
    "        print(f\"Total time so far --> {self.total_time_actual} seconds\")\n",
    "        print(f\"(Partitioning: {end_time_1 - start_time_1} seconds, Repartitioning: {end_time_2 - start_time_2} seconds, Analyzing: {end_time_3 - start_time_3} seconds), Materializing config: {config_materialization_time} seconds, Executing query: {execution_time} seconds\")\n",
    "\n",
    "\n",
    "    # Simple baseline recommendation: just the used indexes in the IBG root node, i.e. these are the indexes from \n",
    "    # the full set of candidate indexes which are used in the query plan\n",
    "    def get_simple_recommendation_ibg(self, ibg):\n",
    "        simple_recommendation = ibg.root.used  \n",
    "        wfit_recommendation = [index.index_id for i in self.current_recommendations for index in self.current_recommendations[i]]\n",
    "        print(f\"*** WFIT recommendation: {sorted(wfit_recommendation)}\")\n",
    "        print(f\"*** Simple recommendation: {sorted([index.index_id for index in simple_recommendation])}\") \n",
    "\n",
    "\n",
    "    # check for stale indexes in U and remove them\n",
    "    def remove_stale_indexes_U(self, verbose):\n",
    "        # find out which indexes have loweest benefit statistics\n",
    "        avg_benefit = {}\n",
    "        for index_id in self.U:\n",
    "            # compute average benefit of the index from all stats\n",
    "            avg_benefit[index_id] = sum([stat[1] for stat in self.idxStats[index_id]]) / len(self.idxStats[index_id])\n",
    "\n",
    "        # sort indexes by average benefit\n",
    "        sorted_indexes = sorted(avg_benefit, key=avg_benefit.get, reverse=True)\n",
    "\n",
    "        # mark all indexes with zero benefit and not in M and S_0 as stale\n",
    "        stale_indexes = set()\n",
    "        for index_id in sorted_indexes:\n",
    "            if avg_benefit[index_id] == 0 and index_id not in self.M and index_id not in self.S_0:\n",
    "                stale_indexes.add(index_id)\n",
    "\n",
    "        # remove stale indexes from U\n",
    "        print(f\"Number of indexes in U: {len(self.U)}\")\n",
    "        num_removed = 0\n",
    "        for index_id in stale_indexes:\n",
    "            #if verbose: print(f\"Removing stale index: {index_id}\")\n",
    "            del self.U[index_id]\n",
    "            #if verbose: print(f\"Number of indexes in U after removal: {len(self.U)}\")\n",
    "            num_removed += 1\n",
    "\n",
    "        # keep at most self.max_U of highest benefit indexes in U, make sure to keep all indexes in S_0 and M\n",
    "        if self.max_U is not None and len(self.U) > self.max_U:\n",
    "            for index_id in sorted_indexes[self.max_U:]:\n",
    "                if index_id not in self.M and index_id not in self.S_0 and index_id in self.U:\n",
    "                    del self.U[index_id]\n",
    "                    num_removed += 1\n",
    "\n",
    "        # remove stale indexes from stable partitions and C (not sure if this is necessary...)\n",
    "        \n",
    "        if verbose:\n",
    "            #print(f\"Average benefit of indexes:\")\n",
    "            #for index_id in sorted_indexes:\n",
    "            #    print(f\"\\tIndex {index_id}: {avg_benefit[index_id]}, Stale: {index_id in stale_indexes}\")\n",
    "                \n",
    "            print(f\"Number of indexes removed: {num_removed}, Number of indexes remaining: {len(self.U)}\")\n",
    "            #print(f\"Indexes in U: {self.U.keys()}\")\n",
    "                \n",
    "\n",
    "    # repartition the stable partitions based on the new partitions\n",
    "    def repartition(self, new_partitions, verbose):\n",
    "        # all indexes recommmendations across the WFA instances from previous round\n",
    "        S_curr = set(chain(*self.current_recommendations.values()))\n",
    "        C = set(self.C.values()) \n",
    "        S_0 = set(self.S_0)\n",
    "\n",
    "        # compute L2-norm of the work function across all partitions\n",
    "        #l2_norm_wf_old = 0\n",
    "        #for i, wf in self.W.items():\n",
    "        #    l2_norm_wf_old += sum([wf[X]**2 for X in wf])\n",
    "\n",
    "        \n",
    "        # re-initizlize WFA instances and recommendations for each new partition\n",
    "        if verbose: print(f\"Reinitializing WFA instances...\")\n",
    "        W = {}\n",
    "        recommendations = {}\n",
    "        for i, P in enumerate(new_partitions):\n",
    "            partition_all_configs = [tuple(sorted(state, key=lambda x: x.index_id)) for state in powerset(P)]\n",
    "            wf = {}\n",
    "            # initialize work function values for each state\n",
    "            #print(f\"\\tNew partition # {i}\")\n",
    "            for X in partition_all_configs:\n",
    "                wf_x = 0\n",
    "                for j, wf_prev in self.W.items(): \n",
    "                    wf_x += wf_prev[tuple(sorted(set(X) & set(self.stable_partitions[j]), key=lambda x: x.index_id))]\n",
    "                \n",
    "                transition_cost_term = self.compute_transition_cost(S_0 & (set(P) - C), set(X) - C)\n",
    "                wf[X] = wf_x + transition_cost_term - self.total_no_index_cost\n",
    "                #print(f\"\\t\\t w[{tuple([index.index_id for index in X])}] --> {wf[X]}   ({wf_x} + {transition_cost_term})\")\n",
    "            \n",
    "            W[i] = wf\n",
    "            # initialize current state/recommended configuration of the WFA instance\n",
    "            recommendations[i] = list(set(P) & S_curr)\n",
    "\n",
    "        \"\"\"\n",
    "        # compute l2 norm of the work function across all partitions\n",
    "        l2_norm_wf_new = 0\n",
    "        for i, wf in W.items():\n",
    "            l2_norm_wf_new += sum([wf[X]**2 for X in wf])\n",
    "        # rescale work function values to maintain the same l2 norm (otherwise wf values will keep increasing\n",
    "        # due to the summation terms in repartitioning)\n",
    "        for i, wf in W.items():\n",
    "            for X in wf:\n",
    "                wf[X] = wf[X] * (l2_norm_wf_old / l2_norm_wf_new)    \n",
    "        \"\"\"\n",
    "\n",
    "        # replace current stable partitions, WFA instances and recommendations with the new ones\n",
    "        self.stable_partitions = new_partitions\n",
    "        self.W = W\n",
    "        self.current_recommendations = recommendations\n",
    "        \"\"\"\n",
    "        if verbose: \n",
    "            print(f\"Replaced stable partitions, WFA instances and recommendations with new ones\")\n",
    "            print(f\"New WFA instances:\")\n",
    "            for i, wf in self.W.items():\n",
    "                print(f\"\\tWFA Instance #{i}:\")\n",
    "                for X, value in wf.items():\n",
    "                    print(f\"\\t\\tState: {tuple([index.index_id for index in X])}, Work function value: {value}\")\n",
    "        \"\"\"\n",
    "        \n",
    "        self.C = {}\n",
    "        for P in self.stable_partitions:\n",
    "            for index in P: \n",
    "                self.C[index.index_id] = index      \n",
    "\n",
    "\n",
    "    # update WFA instance on each stable partition and get index configuration recommendation\n",
    "    def analyze_query(self, query_object, ibg, verbose):\n",
    "        new_recommendations = {}\n",
    "        # update WFA instance for each stable partition\n",
    "        all_indexes_added = []\n",
    "        all_indexes_removed = []\n",
    "        for i in self.W:\n",
    "            if verbose: print(f\"Updating WFA instance: {i}\")\n",
    "            self.W[i], new_recommendations[i]  = self.process_WFA(query_object, self.W[i], self.current_recommendations[i], ibg, verbose)\n",
    "\n",
    "            # materialize new recommendation\n",
    "            indexes_added = set(new_recommendations[i]) - set(self.current_recommendations[i])\n",
    "            indexes_removed = set(self.current_recommendations[i]) - set(new_recommendations[i])\n",
    "            if verbose: print(f\"\\tWFA Instance #{i}, Num States: {len(self.W[i])}, New Recommendation: {[index.index_id for index in new_recommendations[i]]} --> Indexes Added: {[index.index_id for index in indexes_added]}, Indexes Removed: {[index.index_id for index in indexes_removed]}\")\n",
    "            \n",
    "            for index in indexes_added:\n",
    "                self.M[index.index_id] = index\n",
    "            for index in indexes_removed:\n",
    "                del self.M[index.index_id]    \n",
    "                \n",
    "            self.current_recommendations[i] = new_recommendations[i]\n",
    "\n",
    "            all_indexes_added += list(indexes_added)\n",
    "            all_indexes_removed += list(indexes_removed)\n",
    "\n",
    "        return all_indexes_added, all_indexes_removed\n",
    "\n",
    "\n",
    "    # materialize new configuration\n",
    "    def materialize_configuration(self, all_indexes_added, all_indexes_removed, verbose):\n",
    "        # materialize new configuration\n",
    "        if verbose: \n",
    "            print(f\"New indexes added this round: {[index.index_id for index in all_indexes_added]}\")\n",
    "            print(f\"Old indexes removed this round: {[index.index_id for index in all_indexes_removed]}\")\n",
    "\n",
    "            print(f\"Materializing new configuration...\")\n",
    "        start_time = time.time()\n",
    "        conn = create_connection()\n",
    "        bulk_drop_indexes(conn, all_indexes_removed)\n",
    "        bulk_create_indexes(conn, all_indexes_added)\n",
    "        close_connection(conn)\n",
    "        end_time = time.time()\n",
    "        creation_time = end_time - start_time\n",
    "        \n",
    "        return creation_time    \n",
    "\n",
    "\n",
    "    # update a WFA instance for the given query    \n",
    "    def process_WFA(self, query_object, wf, S_current, ibg, verbose):\n",
    "        # update work function values for each state in the WFA instance\n",
    "        wf_new = {}\n",
    "        p = {}\n",
    "        for Y in wf.keys():\n",
    "            sorted_Y = tuple(sorted(Y, key=lambda x: x.index_id))\n",
    "            #print(f\"\\tComputing work function value for state: {tuple([index.index_id for index in sorted_Y])}, old value --> {wf[sorted_Y]}\")\n",
    "            # compute new work function value for state Y \n",
    "            min_wf_value = float('inf')\n",
    "            wf_X = {}\n",
    "            for X in wf.keys():\n",
    "                sorted_X = tuple(sorted(X, key=lambda x: x.index_id))\n",
    "                wf_term = wf[sorted_X]\n",
    "                query_cost_term = ibg.get_cost_used(list(sorted_X))[0]\n",
    "                transition_cost_term = self.compute_transition_cost(sorted_X, sorted_Y) \n",
    "                wf_value = wf_term + query_cost_term + transition_cost_term\n",
    "                #print(f'\\t\\tValue for X = {tuple([index.index_id for index in sorted_X])} -->  {wf_value}  ({wf_term} + {query_cost_term} + {transition_cost_term})')\n",
    "                \n",
    "                wf_X[sorted_X] = wf_value\n",
    "                # keep track of minimum work function value for the state\n",
    "                if wf_value < min_wf_value:\n",
    "                    min_wf_value = wf_value\n",
    "\n",
    "            wf_new[sorted_Y] = min_wf_value\n",
    "            min_p = []\n",
    "            for X in wf_X:\n",
    "                if wf_X[X] == min_wf_value:\n",
    "                    min_p.append(X)\n",
    "            p[sorted_Y] = min_p\n",
    "            #print(f\"\\tUpdated value: w[{tuple([index.index_id for index in sorted_Y])}] --> {wf_new[sorted_Y]}, p: {[[index.index_id for index in indexes] for indexes in p]}\")\n",
    "\n",
    "        # compute scores and find best state\n",
    "        best_score = float('inf')\n",
    "        best_state = None  \n",
    "        for Y in wf_new:\n",
    "            sorted_Y = tuple(sorted(Y, key=lambda x: x.index_id))\n",
    "            score = wf_new[sorted_Y] + self.compute_transition_cost(sorted_Y, S_current)  \n",
    "            if score < best_score and sorted_Y in p[sorted_Y]:\n",
    "                best_score = score\n",
    "                best_state = sorted_Y  #min_p\n",
    "\n",
    "        if verbose:\n",
    "            #print(f\"\\tAll updated Work function values for WFA instance:\")\n",
    "            #for Y, value in wf_new.items():\n",
    "            #    print(f\"\\t\\tstate :{tuple([index.index_id for index in Y])} , w_value: {value}, p: {[[index.index_id for index in indexes] for indexes in p]}, score: {scores[Y]}\")\n",
    "\n",
    "            print(f\"\\tBest state: {tuple([index.index_id for index in best_state])}, Best score: {best_score}\")\n",
    "        \n",
    "        return wf_new, best_state\n",
    "\n",
    "    \n",
    "    # compute index benefit graph for the given query and candidate indexes\n",
    "    def compute_IBG(self, query_object, candidate_indexes):\n",
    "        return IBG(query_object, candidate_indexes, existing_indexes=list(self.M.values()), ibg_max_nodes=self.ibg_max_nodes)\n",
    "    \n",
    "\n",
    "    # extract candidate indexes from given query\n",
    "    def extract_indexes(self, query_object, max_size_mb=4096):        \n",
    "        candidate_indexes = extract_query_indexes(query_object,  self.max_key_columns, self.include_cols)\n",
    "        new_indexes = [index for index in candidate_indexes if index.index_id not in self.index_size]\n",
    "        # get hypothetical idnex sizes\n",
    "        conn = create_connection()\n",
    "        new_index_sizes = get_hypothetical_index_sizes(conn, new_indexes)\n",
    "        close_connection(conn)\n",
    "        for index in new_indexes:\n",
    "            self.index_size[index.index_id] = new_index_sizes[index.index_id]\n",
    "\n",
    "        # filter out indexes that exceed the maximum size\n",
    "        candidate_indexes = [index for index in candidate_indexes if self.index_size[index.index_id] <= max_size_mb]\n",
    "\n",
    "        return candidate_indexes\n",
    "        \n",
    "\n",
    "    # generate stable partitions/sets of indexes for next query in workload\n",
    "    def choose_candidates(self, n_pos, query_object, verbose):\n",
    "        # extract candidate indexes from the query\n",
    "        candidate_indexes = self.extract_indexes(query_object)\n",
    "        # add new candidate indexes to the list of all candidate indexes\n",
    "        num_new = 0\n",
    "        for index in candidate_indexes:\n",
    "            if index.index_id not in self.U:\n",
    "                self.U[index.index_id] = index\n",
    "                num_new += 1\n",
    "\n",
    "        #if len(self.U) > self.max_U:\n",
    "        #    raise ValueError(\"Number of candidate indexes exceeds the maximum limit. Aborting WFIT...\")\n",
    "\n",
    "        if verbose: \n",
    "            print(f\"Extracted {num_new} new indexes from query.\")\n",
    "            print(f\"Candidate indexes (including those currently materialized), |U| = {len(self.U)}\")\n",
    "            #print(f\"{[index.index_id for index in self.U.values()]}\")\n",
    "\n",
    "        # TODO: need mechanism to evict indexes from U that may have gone \"stale\" to prevent unbounded growth of U\n",
    "\n",
    "        \n",
    "        # compute index benefit graph for the query\n",
    "        if verbose: print(f\"Computing IBG...\")\n",
    "        ibg = self.compute_IBG(query_object, list(self.U.values()))\n",
    "\n",
    "        #if verbose: print(f\"Candidate index sizes in Mb: {[(index.index_id,index.size) for index in self.U.values()]}\")\n",
    "        \n",
    "        # update statistics for the candidate indexes (n_pos is the position of the query in the workload sequence)\n",
    "        if verbose: print(f\"Updating statistics...\")\n",
    "        self.update_stats(n_pos, ibg, verbose=False)\n",
    "\n",
    "        # non-materialized candidate indexes \n",
    "        X = [self.U[index_id] for index_id in self.U if index_id not in self.M]\n",
    "        num_indexes = self.idxCnt - len(self.M)\n",
    "\n",
    "        # determine new set of candidate indexes to monitor for upcoming workload queries\n",
    "        if verbose: print(f\"Choosing top {num_indexes} indexes from {len(X)} non-materialized candidate indexes\")\n",
    "        top_indexes = self.top_indexes(n_pos, X, num_indexes, verbose)\n",
    "        D = self.M | top_indexes\n",
    "        if verbose: print(f\"New set of indexes to monitor for upcoming workload, |D| = {len(D)}\")\n",
    "\n",
    "        # generate new partitions by clustering the new candidate set\n",
    "        if verbose: print(f\"Choosing new partitions...\")\n",
    "        new_partitions, need_to_repartition = self.choose_partition(n_pos, D, verbose)\n",
    "        if verbose:\n",
    "            print(f\"Old partitions:\")\n",
    "            for P in self.stable_partitions:\n",
    "                print(f\"\\t{[index.index_id for index in P]}\")\n",
    "            print(\"New partitions:\")\n",
    "            for P in new_partitions:\n",
    "                print(f\"\\t{[index.index_id for index in P]}\")    \n",
    "\n",
    "        return new_partitions, need_to_repartition, ibg\n",
    "    \n",
    "\n",
    "    # partition the new candidate set into clusters \n",
    "    # (need to optimize this function, currently it is a naive implementation)\n",
    "    def choose_partition(self, N_workload, D, verbose):\n",
    "        \n",
    "        # compute total loss, i.e. sum of doi across indexes from pairs of partitions\n",
    "        def compute_loss(P, current_doi):\n",
    "            loss = 0\n",
    "            for i in range(len(P)):\n",
    "                for j in range(i+1, len(P)):\n",
    "                    for a in P[i]:\n",
    "                        for b in P[j]:\n",
    "                            loss += current_doi[(a.index_id, b.index_id)]\n",
    "            return loss\n",
    "        \n",
    "        # compute current doi values for all pairs of indexes in U\n",
    "        current_doi = defaultdict(int)\n",
    "        for (a_idx, b_idx) in self.intStats.keys():\n",
    "            # take max over incremental averages (optimistic estimate)\n",
    "            current_doi[(a_idx, b_idx)] = 0\n",
    "            doi_total = 0\n",
    "            for (n, doi) in self.intStats[(a_idx, b_idx)]:\n",
    "                doi_total += doi\n",
    "                doi_avg = doi_total / (N_workload-n+1)\n",
    "                current_doi[(a_idx, b_idx)] = max(current_doi[(a_idx, b_idx)], doi_avg)\n",
    "            # save symmetric doi value\n",
    "            current_doi[(b_idx, a_idx)] = current_doi[(a_idx, b_idx)]    \n",
    "\n",
    "        #if verbose:\n",
    "        #    print(\"Current degree of interaction:\")\n",
    "        #    for pair, doi in current_doi.items():\n",
    "        #        print(f\"\\tPair {pair}: {doi}\")     \n",
    "\n",
    "        # from each current stable partition, remove indexes not in D\n",
    "        P = []\n",
    "        for partition in self.stable_partitions:\n",
    "            #P.append([index for index in partition if index.index_id in D])\n",
    "            P.append([index for index in partition])\n",
    "\n",
    "        # add a singleton partition containing each new index in D not in C\n",
    "        for index_id, index in D.items():\n",
    "            if index_id not in self.C:\n",
    "                P.append([index])\n",
    "        \n",
    "        # set the new partition as baseline solution if feasible\n",
    "        total_configurations = sum([2**len(partition) for partition in P])\n",
    "        if total_configurations <= self.stateCnt:\n",
    "            bestSolution = P\n",
    "            bestLoss = compute_loss(P, current_doi)\n",
    "        else:\n",
    "            bestSolution = None\n",
    "            bestLoss = float('inf')    \n",
    "\n",
    "        # perform randomized clustering to find better solution\n",
    "        for i in range(self.rand_cnt):\n",
    "            # create partition of D in singletons\n",
    "            P = [[index] for index in D.values()]\n",
    "            partition2id = {tuple(partition):i for i, partition in enumerate(P)}\n",
    "            loss_cache = {}\n",
    "            \n",
    "            #if verbose:\n",
    "            #    print(f\"Parition to id map: {partition2id}\")\n",
    "\n",
    "            # first merge all singletons, then merge pairs of partitions (randomized merge)\n",
    "            # stopping condition: no feasible merge pairs left (i.e. any merge would exceed stateCnt)\n",
    "            while True:\n",
    "                # find all feasible merge candidates pairs (i.e. pairs with loss > 0 and 2^(|Pi|+|Pj|) <= stateCnt)\n",
    "                E = []\n",
    "                E1 = []\n",
    "\n",
    "                # get loss for all pairs of partitions\n",
    "                total_configurations = sum([2**len(partition) for partition in P])\n",
    "                for i in range(len(P)):\n",
    "                    for j in range(i+1, len(P)):\n",
    "                        Pi_id = partition2id[tuple(P[i])]\n",
    "                        Pj_id = partition2id[tuple(P[j])]\n",
    "                        if (Pi_id, Pj_id) in loss_cache:\n",
    "                            loss = loss_cache[(Pi_id, Pj_id)]\n",
    "                        else:\n",
    "                            loss = compute_loss([P[i], P[j]], current_doi)\n",
    "                            loss_cache[(Pi_id, Pj_id)] = loss\n",
    "\n",
    "                        # only include feasible merge pairs, i.e. a pair which can be merged without the total number of configs exceeding stateCnt\n",
    "                        total_configrations_after_merge = total_configurations - 2**len(P[i]) - 2**len(P[j]) + 2**(len(P[i]) + len(P[j]))\n",
    "                        if loss > 0 and total_configrations_after_merge <= self.stateCnt:\n",
    "                            E.append((P[i], P[j], loss))    \n",
    "                            if len(P[i]) == 1 and len(P[j]) == 1:\n",
    "                                E1.append((P[i],P[j], loss))\n",
    "\n",
    "                #if verbose:    \n",
    "                    #print(f\"E pairs: {[[(index.index_id for index in Pi), (index.index_id for index in Pj), loss] for (Pi, Pj, loss) in E]}\")\n",
    "                    #print(f\"E1 pairs: {[[(index.index_id for index in Pi), (index.index_id for index in Pj), loss] for (Pi, Pj, loss) in E1]}\")\n",
    "\n",
    "                if len(E) == 0:\n",
    "                    break\n",
    "                \n",
    "                elif len(E1) > 0:\n",
    "                    # merge a random pair of singletons, sample randomly from E1 weighted by loss (i.e. high loss pairs more likely to be merged)\n",
    "                    Pi, Pj, loss = random.choices(E1, weights=[loss for (Pi, Pj, loss) in E1], k=1)[0]\n",
    "                    Pij_merged = Pi + Pj\n",
    "                    P.remove(Pi)\n",
    "                    P.remove(Pj)\n",
    "                    P.append(Pij_merged) \n",
    "                    E1.remove((Pi, Pj, loss))  \n",
    "                    partition2id[tuple(Pij_merged)] = len(partition2id) \n",
    "                    #if verbose: \n",
    "                    #    print(f\"Merged singleton partitions {[index.index_id for index in Pi]} and {[index.index_id for index in Pj]} with loss {loss}\")\n",
    "\n",
    "                else:\n",
    "                    # merge a random pair of partitions, sample randomly from E weighted by normalized loss  \n",
    "                    Pi, Pj, loss = random.choices(E, weights=[loss / (2**(len(Pi) + len(Pj)) - 2**len(Pi) - 2**len(Pj)) for (Pi, Pj, loss) in E], k=1)[0]\n",
    "                    Pij_merged = Pi + Pj\n",
    "                    P.remove(Pi)\n",
    "                    P.remove(Pj)\n",
    "                    P.append(Pij_merged)   \n",
    "                    E.remove((Pi, Pj, loss)) \n",
    "                    partition2id[tuple(Pij_merged)] = len(partition2id) \n",
    "                    #if verbose:\n",
    "                    #    print(f\"Merged partitions {[index.index_id for index in Pi]} and {[index.index_id for index in Pj]} with loss {loss}\")    \n",
    "\n",
    "            # check if the new solution is better than the current best solution\n",
    "            loss = compute_loss(P, current_doi)\n",
    "            if loss < bestLoss:\n",
    "                bestSolution = P\n",
    "                bestLoss = loss\n",
    "\n",
    "        # check if old partitions are different from new partitions\n",
    "        need_to_repartition = False\n",
    "        if bestSolution != self.stable_partitions:\n",
    "            need_to_repartition = True\n",
    "\n",
    "        return bestSolution, need_to_repartition\n",
    "\n",
    "\n",
    "    # update candidate index statistics\n",
    "    def update_stats(self, n, ibg, verbose):\n",
    "        # update index benefit statistics\n",
    "        if verbose: print(\"Updating index benefit statistics...\")\n",
    "        for index in self.U.values():\n",
    "            max_benefit = ibg.compute_max_benefit(index)\n",
    "            #if verbose: print(f\"\\tibg max benefit for index {index.index_id}: {max_benefit}\")\n",
    "            self.idxStats[index.index_id].append((n, max_benefit))\n",
    "            #if verbose: print(f\"\\tIndex {index.index_id}: {self.idxStats[index.index_id]}\")\n",
    "            # evict old stats if the size exceeds histSize\n",
    "            self.idxStats[index.index_id] = self.idxStats[index.index_id][-self.histSize:]\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Index benefit statistics:\")\n",
    "            for index_id, stats in self.idxStats.items():\n",
    "                print(f\"\\tIndex {index_id}: {stats}\")\n",
    "\n",
    "\n",
    "        # update index interaction statistics\n",
    "        if verbose: print(\"Updating index interaction statistics...\")\n",
    "        for (a_idx, b_idx) in ibg.doi.keys():\n",
    "            d = ibg.doi[(a_idx, b_idx)]\n",
    "            #if verbose: print(f\"\\tibg doi for pair ({a_idx}, {b_idx}) : {d}\")\n",
    "            if d > 0:\n",
    "                self.intStats[(a_idx, b_idx)].append((n, d))\n",
    "            #if verbose: print(f\"\\tPair ({a_idx}, {b_idx}): {self.intStats[(a_idx, b_idx)]}\")\n",
    "            # evict old stats if the size exceeds histSize\n",
    "            self.intStats[(a_idx, b_idx)] = self.intStats[(a_idx, b_idx)][-self.histSize:]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Index interaction statistics:\")\n",
    "            for pair, stats in self.intStats.items():\n",
    "                print(f\"\\tPair {pair}: {stats}\")\n",
    "\n",
    "\n",
    "    # choose top num_indexes indexes from X with highest potential benefit\n",
    "    def top_indexes(self, N_workload, X, num_indexes, verbose, positive_scores_only=False):\n",
    "        #if verbose:\n",
    "        #    print(f\"Non-materialized candidate indexes, X = {[index.index_id for index in X]}\")\n",
    "\n",
    "        # compute \"current benefit\" of each index in X (these are derived from statistics of observed benefits from recent queries)\n",
    "        score = {}\n",
    "        for index in X:\n",
    "            if len(self.idxStats[index.index_id]) == 0:\n",
    "                # zero current benefit if no statistics are available\n",
    "                current_benefit = 0\n",
    "            else:\n",
    "                # take the maximum over all incremental average benefits (optimistic estimate)\n",
    "                current_benefit = 0\n",
    "                b_total = 0\n",
    "                for (n, b) in self.idxStats[index.index_id]:\n",
    "                    b_total += b \n",
    "                    # incremental average benefit of index up to query n (higher weight/smaller denominator for more recent queries)\n",
    "                    benefit = b_total / (N_workload - n + 1)\n",
    "                    current_benefit = max(current_benefit, benefit)\n",
    "\n",
    "            # use current benefit to compute a score for the index\n",
    "            if index.index_id in self.C:\n",
    "                # if index already being monitored, then score is just current benefit\n",
    "                score[index.index_id] = current_benefit\n",
    "            else:\n",
    "                # if index not being monitored, then score is current benefit minus cost of creating the index\n",
    "                # (unmonitored indexes are penalized so that they are only chosen if they have high potential benefit, which helps keep C stable)\n",
    "                score[index.index_id] = current_benefit - self.get_index_creation_cost(index)\n",
    "\n",
    "        #if verbose:\n",
    "        #    print(\"Index scores:\")\n",
    "        #    for index_id, s in score.items():\n",
    "        #        print(f\"Index {index_id}: {s}\")\n",
    "\n",
    "        # get the top num_indexes indexes with highest scores (keep non-zero scores only)\n",
    "        if positive_scores_only:\n",
    "            top_indexes = [index_id for index_id, s in score.items() if s > 0]\n",
    "        else:\n",
    "            top_indexes = [index_id for index_id, s in score.items()]    \n",
    "        top_indexes = sorted(top_indexes, key=lambda x: score[x], reverse=True)[:num_indexes]\n",
    "        top_indexes = {index_id: self.U[index_id] for index_id in top_indexes}\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{len(top_indexes)} top indexes: {[index.index_id for index in top_indexes.values()]}\")\n",
    "\n",
    "        return top_indexes    \n",
    "\n",
    "\n",
    "    # return index creation cost (using estimated index size as proxy for cost)\n",
    "    def get_index_creation_cost(self, index):\n",
    "        # return estimated size of index\n",
    "        return index.size * self.creation_cost_fudge_factor  \n",
    "\n",
    "\n",
    "    # compute transition cost between two MTS states/configurations\n",
    "    def compute_transition_cost(self, S_old, S_new):\n",
    "        # find out which indexes are added\n",
    "        added_indexes = set(S_new) - set(S_old)\n",
    "        \n",
    "        # compute cost of creating the added indexes\n",
    "        transition_cost = sum([self.get_index_creation_cost(index) for index in added_indexes])\n",
    "        #print(f\"\\t\\t\\tComputing transition cost for state: {tuple([index.index_id for index in S_old])} --> {tuple([index.index_id for index in S_new])} = {transition_cost}\")\n",
    "        return transition_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test WFIT implementation on sample SSB workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# generate an SSB workload\\nworkload = [qg.generate_query(i) for i in ([1,2,3]*10)]\\n\\nprint(len(workload))'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# generate an SSB workload\n",
    "workload = [qg.generate_query(i) for i in ([1,2,3]*10)]\n",
    "\n",
    "print(len(workload))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workload 3 Experiment --> `wfit = WFIT(S_0, max_key_columns=3, include_cols=False, max_U=None, idxCnt=25, stateCnt=500, rand_cnt=100, creation_cost_fudge_factor=2048)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n",
      "Initializing WFA instances for 1 stable partitions...\n",
      "WFA instance #0: {(): 0}\n",
      "Initial set of materialized indexes: []\n",
      "Stable partitions: [[]]\n",
      "Initial work function instances: \n",
      "\tWFA Instance #0: {(): 0}\n",
      "\n",
      "Maximum number of candidate indexes tracked: 40\n",
      "Maximum number of MTS states/configurations: 500\n",
      "Maximum number of historical index statistics kept: 100\n",
      "Number of randomized clustering iterations: 100\n",
      "*** Dropping all materialized indexes...\n",
      "##################################################################\n",
      "\n",
      "Processing query 1\n",
      "-----------------------------------\n",
      "Generating new partitions for query #1\n",
      "Extracted 19 new indexes from query.\n",
      "Candidate indexes (including those currently materialized), |U| = 19\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 19\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18\n",
      "Constructing IBG...\n",
      "Number of levels in IBG: 16\n",
      "Number of nodes in IBG: 69, Total number of what-if calls: 69, Time spent on what-if calls: 0.12910938262939453\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 5/5 [00:00<00:00, 53911.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent on computing all pair degree of interaction: 0.06028246879577637\n",
      "Updating statistics...\n",
      "Choosing top 40 indexes from 19 non-materialized candidate indexes\n",
      "19 top indexes: ['ix_lineorder_lo_quantity', 'ix_lineorder_lo_quantity_lo_orderdate', 'ix_lineorder_lo_quantity_lo_discount', 'ix_lineorder_lo_orderdate_lo_quantity', 'ix_lineorder_lo_quantity_lo_orderdate_lo_discount', 'ix_lineorder_lo_quantity_lo_discount_lo_orderdate', 'ix_lineorder_lo_orderdate_lo_quantity_lo_discount', 'ix_lineorder_lo_orderdate_lo_discount_lo_quantity', 'ix_lineorder_lo_discount_lo_quantity', 'ix_lineorder_lo_discount_lo_orderdate_lo_quantity', 'ix_lineorder_lo_discount_lo_quantity_lo_orderdate', 'ix_dwdate_d_datekey_d_year', 'ix_dwdate_d_year_d_datekey', 'ix_dwdate_d_year', 'ix_dwdate_d_datekey', 'ix_lineorder_lo_orderdate', 'ix_lineorder_lo_discount', 'ix_lineorder_lo_orderdate_lo_discount', 'ix_lineorder_lo_discount_lo_orderdate']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 19\n",
      "Choosing new partitions...\n",
      "Old partitions:\n",
      "\t[]\n",
      "New partitions:\n",
      "\t['ix_lineorder_lo_quantity_lo_discount_lo_orderdate', 'ix_lineorder_lo_orderdate_lo_quantity_lo_discount', 'ix_lineorder_lo_quantity', 'ix_lineorder_lo_orderdate_lo_quantity', 'ix_lineorder_lo_quantity_lo_discount', 'ix_lineorder_lo_discount_lo_orderdate_lo_quantity', 'ix_lineorder_lo_quantity_lo_orderdate', 'ix_lineorder_lo_quantity_lo_orderdate_lo_discount']\n",
      "\t['ix_dwdate_d_year', 'ix_lineorder_lo_discount', 'ix_lineorder_lo_orderdate', 'ix_lineorder_lo_orderdate_lo_discount']\n",
      "\t['ix_dwdate_d_datekey', 'ix_lineorder_lo_discount_lo_quantity', 'ix_lineorder_lo_orderdate_lo_discount_lo_quantity', 'ix_lineorder_lo_discount_lo_quantity_lo_orderdate', 'ix_dwdate_d_datekey_d_year', 'ix_dwdate_d_year_d_datekey', 'ix_lineorder_lo_discount_lo_orderdate']\n",
      "Repartitioning...\n",
      "Reinitializing WFA instances...\n",
      "Analyzing query...\n",
      "New indexes added this round: ['ix_lineorder_lo_quantity', 'ix_lineorder_lo_orderdate_lo_discount_lo_quantity', 'ix_dwdate_d_year_d_datekey']\n",
      "Old indexes removed this round: []\n",
      "Materializing new configuration...\n",
      "Successfully created index: 'ix_lineorder_lo_quantity', size: 396.4765625000000000 MB, creation time: 23042.98 ms\n"
     ]
    }
   ],
   "source": [
    "# instantiate WFIT\n",
    "#C = extract_query_indexes(qg.generate_query(8), include_cols=False)  \n",
    "S_0 = []#C[0:1]\n",
    "wfit = WFIT(S_0, max_key_columns=3, include_cols=False, max_U=None, ibg_max_nodes=100, idxCnt=40, stateCnt=500, rand_cnt=100, creation_cost_fudge_factor=128)\n",
    "\n",
    "# process the workload\n",
    "for i, query in enumerate(workload[:12]):\n",
    "    print(f\"Processing query {i+1}\")\n",
    "    print('-----------------------------------')\n",
    "    wfit.process_WFIT(query, remove_stale_U=False, remove_stale_freq=1, execute=True, materialize=True, verbose=True)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_workload_noIndex(workload, drop_indexes=False):\n",
    "    if drop_indexes:\n",
    "        print(f\"*** Dropping all existing indexes...\")\n",
    "        # drop all existing indexes\n",
    "        conn = create_connection()\n",
    "        drop_all_indexes(conn)\n",
    "        close_connection(conn)\n",
    "\n",
    "    print(f\"Executing workload without any indexes...\")\n",
    "    # execute workload without any indexes\n",
    "    total_time = 0\n",
    "    for i, query_object in enumerate(workload):\n",
    "        # restart the server before each query execution\n",
    "        restart_postgresql()\n",
    "        conn = create_connection()\n",
    "        execution_time, rows = execute_query(conn, query_object.query_string, with_explain=True, print_results=False)\n",
    "        close_connection(conn)\n",
    "        execution_time /= 1000\n",
    "        total_time += execution_time\n",
    "        print(f\"\\tExecution time for query# {i+1}: {execution_time} s, Total time so far: {total_time} s\")\n",
    "\n",
    "    print(f\"Total execution time for workload without any indexes: {total_time} seconds\")\n",
    "    \n",
    "    return total_time    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Dropping all existing indexes...\n",
      "Index 'ix_part_p_category_p_partkey_p_brand' on table 'part' dropped successfully\n",
      "Index 'ix_lineorder_lo_quantity_lo_linenumber' on table 'lineorder' dropped successfully\n",
      "Index 'ix_lineorder_lo_linenumber_lo_quantity' on table 'lineorder' dropped successfully\n",
      "Index 'ix_lineorder_lo_orderdate_lo_discount_lo_quantity' on table 'lineorder' dropped successfully\n",
      "Index 'ix_lineorder_lo_orderdate_lo_discount' on table 'lineorder' dropped successfully\n",
      "Index 'ix_lineorder_lo_partkey' on table 'lineorder' dropped successfully\n",
      "Index 'ix_lineorder_lo_partkey_lo_orderdate' on table 'lineorder' dropped successfully\n",
      "Index 'ix_lineorder_lo_partkey_lo_suppkey' on table 'lineorder' dropped successfully\n",
      "Index 'ix_lineorder_lo_partkey_lo_orderdate_lo_suppkey' on table 'lineorder' dropped successfully\n",
      "Index 'ix_lineorder_lo_orderdate_lo_suppkey' on table 'lineorder' dropped successfully\n",
      "Index 'ix_lineorder_lo_orderdate_lo_suppkey_lo_custkey' on table 'lineorder' dropped successfully\n",
      "Index 'ix_lineorder_lo_partkey_lo_suppkey_lo_orderdate' on table 'lineorder' dropped successfully\n",
      "Index 'ix_lineorder_lo_orderdate_lo_custkey' on table 'lineorder' dropped successfully\n",
      "Index 'ix_lineorder_lo_orderdate_lo_partkey' on table 'lineorder' dropped successfully\n",
      "Index 'ix_lineorder_lo_quantity' on table 'lineorder' dropped successfully\n",
      "Index 'ix_lineorder_lo_discount_lo_quantity' on table 'lineorder' dropped successfully\n",
      "Index 'ix_lineorder_lo_orderdate_lo_quantity' on table 'lineorder' dropped successfully\n",
      "Executing workload without any indexes...\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 1: 5.82409 s, Total time so far: 5.82409 s\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 2: 4.837686 s, Total time so far: 10.661776 s\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 3: 5.4044799999999995 s, Total time so far: 16.066256 s\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 4: 4.371049 s, Total time so far: 20.437305 s\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 5: 5.5552269999999995 s, Total time so far: 25.992531999999997 s\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 6: 4.729567 s, Total time so far: 30.722098999999996 s\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 7: 4.744377 s, Total time so far: 35.466476 s\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 8: 5.545227 s, Total time so far: 41.011703 s\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 9: 8.012891999999999 s, Total time so far: 49.024595 s\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 10: 5.261717 s, Total time so far: 54.286311999999995 s\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 11: 5.573984 s, Total time so far: 59.860296 s\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 12: 6.223953000000001 s, Total time so far: 66.084249 s\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 13: 5.475138 s, Total time so far: 71.559387 s\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 14: 4.674843 s, Total time so far: 76.23423 s\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 15: 5.420895000000001 s, Total time so far: 81.655125 s\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 16: 4.473659 s, Total time so far: 86.128784 s\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 17: 5.637719 s, Total time so far: 91.766503 s\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 18: 4.92525 s, Total time so far: 96.691753 s\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 19: 5.175405 s, Total time so far: 101.867158 s\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 20: 5.492453 s, Total time so far: 107.359611 s\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 21: 5.996241 s, Total time so far: 113.355852 s\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 22: 4.9580280000000005 s, Total time so far: 118.31388 s\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 23: 5.472671 s, Total time so far: 123.786551 s\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 24: 5.957424 s, Total time so far: 129.743975 s\n",
      "Total execution time for workload without any indexes: 129.743975 seconds\n"
     ]
    }
   ],
   "source": [
    "# execute workload without any indexes\n",
    "total_time_noIndex = execute_workload_noIndex(workload[:24], drop_indexes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
