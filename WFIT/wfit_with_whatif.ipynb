{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WFIT Algorithm Implementation (Schnaitter 2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "import IPython\n",
    "notebook_path = IPython.get_ipython().starting_dir\n",
    "target_subdirectory_path = os.path.abspath(os.path.join(os.path.dirname(notebook_path), 'PostgreSQL'))\n",
    "sys.path.append(target_subdirectory_path)\n",
    "\n",
    "from pg_utils import *\n",
    "from ssb_qgen_class import *\n",
    "\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "import time\n",
    "import random\n",
    "from more_itertools import powerset\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "from collections import deque\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index Benefit Graph (IBG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, id, indexes):\n",
    "        self.id = id\n",
    "        self.indexes = indexes\n",
    "        self.children = []\n",
    "        self.parents = []\n",
    "        self.built = False\n",
    "        self.cost = None\n",
    "        self.used = []\n",
    "\n",
    "\n",
    "# class for creating and storing the IBG\n",
    "class IBG:\n",
    "    # Class-level cache\n",
    "    #_class_cache = {}\n",
    "\n",
    "    def __init__(self, query_object, C, existing_indexes=[], max_nodes=1000, normalize_doi=False):\n",
    "        self.q = query_object\n",
    "        self.C = C\n",
    "        self.existing_indexes = existing_indexes # indexes currently materialized in the database\n",
    "        self.normalize_doi = normalize_doi\n",
    "        print(f\"Number of candidate indexes: {len(self.C)}\")\n",
    "        #print(f\"Candidate indexes: {self.C}\")\n",
    "        \n",
    "        # create a connection session to the database\n",
    "        self.conn = create_connection()\n",
    "        # get hypothetical sizes of all the candidate indexes\n",
    "        print(\"Getting hypothetical sizes of candidate indexes...\")\n",
    "        self.get_hypo_sizes()\n",
    "        # hide existing indexes\n",
    "        bulk_hide_indexes(self.conn, self.existing_indexes)\n",
    "        \n",
    "        # map index_id to integer\n",
    "        self.idx2id = {index.index_id:i for i, index in enumerate(self.C)}\n",
    "        self.idx2index = {index.index_id:index for index in self.C}\n",
    "        #print(f\"Index id to integer mapping: {self.idx2id}\")\n",
    "        \n",
    "        # create a hash table for keeping track of all created nodes\n",
    "        self.nodes = {}\n",
    "        # create a root node\n",
    "        self.root = Node(self.get_configuration_id(self.C), self.C)\n",
    "        self.nodes[self.root.id] = self.root\n",
    "        print(f\"Created root node with id: {self.root.id}\")\n",
    "        \n",
    "        self.total_whatif_calls = 0\n",
    "        self.total_whatif_time = 0\n",
    "        self.node_count = 0\n",
    "\n",
    "        # start the IBG construction\n",
    "        print(\"Constructing IBG...\")\n",
    "        self.construct_ibg(self.root, max_nodes=max_nodes)\n",
    "        print(f\"Number of nodes in IBG: {len(self.nodes)}, Total number of what-if calls: {self.total_whatif_calls}, Time spent on what-if calls: {self.total_whatif_time}\")\n",
    "        # compute all pair degree of interaction\n",
    "        print(f\"Computing all pair degree of interaction...\")\n",
    "        start_time = time.time()\n",
    "        #self.doi = self.compute_all_pair_doi()\n",
    "        self.doi = self.compute_all_pair_doi_parallel(num_workers=4, max_nodes=100)\n",
    "        #self.doi = self.compute_all_pair_doi_simple()\n",
    "        #self.doi = self.compute_all_pair_doi_naive(num_samples=256)\n",
    "        #print(f\"All pair doi:\")\n",
    "        #for key, value in self.doi.items():\n",
    "        #    print(f\"{key}: {value}\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"Time spent on computing all pair degree of interaction: {end_time - start_time}\")\n",
    "\n",
    "        # unhide existing indexes\n",
    "        bulk_unhide_indexes(self.conn, self.existing_indexes)\n",
    "        close_connection(self.conn)\n",
    "        self.conn = None\n",
    "\n",
    "    # assign unique string id to a configuration\n",
    "    def get_configuration_id(self, indexes):\n",
    "        # get sorted list of integer ids\n",
    "        ids = sorted([self.idx2id[idx.index_id] for idx in indexes])\n",
    "        return \"_\".join([str(i) for i in ids])\n",
    "    \n",
    "\n",
    "    # get hypothetical sizes of all the candidate indexes\n",
    "    def get_hypo_sizes(self):\n",
    "        hypo_indexes = bulk_create_hypothetical_indexes(self.conn, self.C, return_size=True)\n",
    "        for i in range(len(hypo_indexes)):\n",
    "            self.C[i].size = hypo_indexes[i][1]\n",
    "        \n",
    "    @lru_cache(maxsize=None)\n",
    "    def _get_cost_used(self, indexes):\n",
    "        # Convert indexes to a tuple to make it hashable\n",
    "        #indexes_tuple = tuple(sorted(indexes, key=lambda x: x.index_id))\n",
    "        # Check if the result is already in the class-level cache\n",
    "        #if indexes_tuple in self._class_cache:\n",
    "        #    return self._class_cache[indexes_tuple]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        if self.conn is None:\n",
    "            conn = create_connection()\n",
    "        else:\n",
    "            conn = self.conn    \n",
    "        # create hypothetical indexes\n",
    "        hypo_indexes = bulk_create_hypothetical_indexes(conn, indexes)\n",
    "        # map oid to index object\n",
    "        oid2index = {}\n",
    "        for i in range(len(hypo_indexes)):\n",
    "            oid2index[hypo_indexes[i]] = indexes[i]\n",
    "        # get cost and used indexes\n",
    "        cost, indexes_used = get_query_cost_estimate_hypo_indexes(conn, self.q.query_string, show_plan=False)\n",
    "        # map used index oids to index objects\n",
    "        used = [oid2index[oid] for oid, scan_type, scan_cost in indexes_used]\n",
    "        # drop hypothetical indexes\n",
    "        bulk_drop_hypothetical_indexes(conn)\n",
    "        if self.conn is None:\n",
    "            close_connection(conn)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Store the result in the class-level cache\n",
    "        #self._class_cache[indexes_tuple] = (cost, used)\n",
    "        self.total_whatif_calls += 1\n",
    "        self.total_whatif_time += end_time - start_time\n",
    "\n",
    "        #print(f\"Configuration: {[index.index_id for index in indexes]}, Cost: {cost}, Used indexes: {[index.index_id for index in used]}\")\n",
    "\n",
    "\n",
    "        return cost, used\n",
    "\n",
    "    # Ensure the indexes parameter is hashable\n",
    "    def _cached_get_cost_used(self, indexes):\n",
    "        return self._get_cost_used(tuple(indexes))\n",
    "\n",
    "    \n",
    "    # IBG construction\n",
    "    def construct_ibg(self, root, max_nodes=None):\n",
    "        # Obtain query optimizer's cost and used indexes\n",
    "        cost, used = self._cached_get_cost_used(root.indexes)\n",
    "        #cost, used = self._get_cost_used(root.indexes)\n",
    "        root.cost = cost\n",
    "        root.used = used\n",
    "        root.built = True\n",
    "        self.node_count += 1\n",
    "\n",
    "        num_levels = 0\n",
    "        queue = deque([root])\n",
    "        while queue:\n",
    "\n",
    "            if max_nodes is not None and self.node_count >= max_nodes:\n",
    "                break  # end if the maximum number of nodes is reached\n",
    "            \n",
    "            # Get the current level size\n",
    "            level_size = len(queue)\n",
    "            num_levels += 1\n",
    "\n",
    "            # Process all nodes at the current level\n",
    "            for _ in range(level_size):\n",
    "                Y = queue.popleft()\n",
    "                               \n",
    "                # Create children\n",
    "                for a in Y.used:\n",
    "                    # Create a new configuration with index a removed from Y\n",
    "                    X_indexes = [index for index in Y.indexes if index != a]\n",
    "                    X_id = self.get_configuration_id(X_indexes)\n",
    "                    \n",
    "                    # If X is not in the hash table, create a new node and add it to the queue\n",
    "                    if X_id not in self.nodes:\n",
    "                        self.node_count += 1\n",
    "                        print(f\"Creating node # {self.node_count}\", end=\"\\r\")\n",
    "         \n",
    "                        X = Node(X_id, X_indexes)\n",
    "                        # Obtain query optimizer's cost and used indexes\n",
    "                        cost, used = self._cached_get_cost_used(X.indexes)\n",
    "                        #cost, used = self._get_cost_used(X.indexes)\n",
    "                        X.cost = cost\n",
    "                        X.used = used\n",
    "                        X.built = True\n",
    "                        X.parents.append(Y)\n",
    "                        self.nodes[X_id] = X\n",
    "                        Y.children.append(X)\n",
    "                        queue.append(X)\n",
    "\n",
    "                    else:\n",
    "                        X = self.nodes[X_id]\n",
    "                        Y.children.append(X)\n",
    "                        X.parents.append(Y)\n",
    "\n",
    "        print(f\"Number of levels in IBG: {num_levels}\")      \n",
    "\n",
    "\n",
    "    # use IBG to obtain estimated cost and used indexes for arbitrary subset of C\n",
    "    def get_cost_used(self, X):\n",
    "        # get id of the configuration\n",
    "        id = self.get_configuration_id(X)\n",
    "        # check if the configuration is in the IBG\n",
    "        if id in self.nodes:\n",
    "            cost, used = self.nodes[id].cost, self.nodes[id].used\n",
    "        \n",
    "        # if not in the IBG, traverse the IBG to find a covering node\n",
    "        else:\n",
    "            Y = self.find_covering_node(X)              \n",
    "            cost, used = Y.cost, Y.used\n",
    "\n",
    "        return cost, used    \n",
    "\n",
    "\n",
    "    # traverses the IBG to find a node that removes indexes not in X (i.e. a covering node for X)\n",
    "    def find_covering_node(self, X):\n",
    "        X_indexes = set([index.index_id for index in X])\n",
    "        Y = self.root\n",
    "        Y_indexes = set([index.index_id for index in Y.indexes])\n",
    "        # traverse IBG to find covering node\n",
    "        while len(Y.children) > 0:               \n",
    "            # traverse down to the child node that removes an index not in X\n",
    "            child_found = False\n",
    "            for child in Y.children:\n",
    "                child_indexes = set([index.index_id for index in child.indexes])\n",
    "                child_indexes_removed = Y_indexes - child_indexes\n",
    "                child_indexes_removed_not_in_X = child_indexes_removed - X_indexes\n",
    "        \n",
    "                # check if child removes an index not in X\n",
    "                if len(child_indexes_removed_not_in_X) > 0:\n",
    "                    Y = child\n",
    "                    Y_indexes = child_indexes\n",
    "                    child_found = True\n",
    "                    break\n",
    "\n",
    "            # if no children remove indexes not in X    \n",
    "            if not child_found:\n",
    "                break    \n",
    "    \n",
    "        return Y        \n",
    "\n",
    "    # compute benefit of an index for a given configuration \n",
    "    # input X is a list of index objects and 'a' is a single index object\n",
    "    # X must not contain 'a'\n",
    "    def compute_benefit(self, a, X):\n",
    "        if a in X:\n",
    "            # zero benefit if 'a' is already in X\n",
    "            #raise ValueError(\"Index 'a' is already in X\")\n",
    "            return 0\n",
    "        \n",
    "        # get cost  for X\n",
    "        cost_X = self.get_cost_used(X)[0]\n",
    "        # create a new configuration with index a added to X\n",
    "        X_a = X + [a]\n",
    "        # get cost for X + {a}\n",
    "        cost_X_a = self.get_cost_used(X_a)[0]\n",
    "        # compute benefit\n",
    "        benefit = cost_X - cost_X_a\n",
    "        return benefit \n",
    "\n",
    "\n",
    "    # compute maximum benefit of adding an index to any possibe configuration\n",
    "    def compute_max_benefit(self, a):\n",
    "        max_benefit = float('-inf')\n",
    "        for id, node in self.nodes.items():\n",
    "            #print(f\"Computing benefit for node: {[index.index_id for index in node.indexes]}\")\n",
    "            benefit = self.compute_benefit(a, node.indexes)\n",
    "            if benefit > max_benefit:\n",
    "                max_benefit = benefit\n",
    "\n",
    "        return max_benefit\n",
    "    \n",
    "    # compute the degree of interaction between two indexes a,b in configuration X \n",
    "    def compute_doi_configuration(self, a, b, X=[]):\n",
    "        # X must not contain a or b\n",
    "        if a in X or b in X:\n",
    "            raise ValueError(\"a or b is already in X\")\n",
    "\n",
    "        doi = abs(self.compute_benefit(a, X) - self.compute_benefit(a, X + [b]))\n",
    "        if self.normalize_doi:\n",
    "            doi /= self.get_cost_used(X + [a,b])[0]   \n",
    "        return doi\n",
    "   \n",
    "    \n",
    "    # Cache the results of find_covering_node and get_cost_used to avoid redundant calculations\n",
    "    @lru_cache(maxsize=None)\n",
    "    def cached_find_covering_node(self, indexes):\n",
    "        return self.find_covering_node(tuple(indexes))\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def cached_get_cost_used(self, indexes):\n",
    "        return self.get_cost_used(tuple(indexes))\n",
    "\n",
    "\n",
    "    # computes the degree of interaction between all pairs of indexes (a,b) in candidate set C\n",
    "    # Note: doi is symmetric, i.e. doi(a,b) = doi(b,a)\n",
    "\n",
    "    # simple version of compute_all_pair_doi, without parallelization\n",
    "    def compute_all_pair_doi_simple(self):\n",
    "        # hash table for storing doi values\n",
    "        doi = {}\n",
    "        # intialize doi values to zero\n",
    "        for i in range(len(self.C)):\n",
    "            for j in range(i+1, len(self.C)):\n",
    "                d = self.compute_doi_configuration(self.C[i], self.C[j])\n",
    "                doi[tuple(sorted((self.C[i].index_id, self.C[j].index_id)))] = d\n",
    "\n",
    "        return doi\n",
    "\n",
    "    # Naive version of compute_all_pair_doi, with random sampling of configurations\n",
    "    def compute_all_pair_doi_naive(self, num_samples=100):\n",
    "        doi = {}\n",
    "        \n",
    "        for i in range(len(self.C)):\n",
    "            for j in range(i + 1, len(self.C)):\n",
    "                doi[tuple(sorted((self.C[i].index_id, self.C[j].index_id)))] = 0\n",
    "        \n",
    "        # sample random configurations: X subset C (must include empty set configuration)\n",
    "        for i in tqdm(range(num_samples), desc=\"Sampling configurations\"):\n",
    "            if i == 0:\n",
    "                X = []\n",
    "            else:\n",
    "                X = random.sample(self.C, random.randint(1, len(self.C)))\n",
    "\n",
    "            # compute doi for all pairs (a, b) in U\\X \n",
    "            for i in range(len(self.C)):\n",
    "                for j in range(i+1, len(self.C)):\n",
    "                    a = self.C[i]\n",
    "                    b = self.C[j]\n",
    "                    if a not in X and b not in X:\n",
    "                        d = self.compute_doi_configuration(a, b, X)\n",
    "                        key = tuple(sorted((a.index_id, b.index_id)))\n",
    "                        doi[key] = max(doi[key], d)\n",
    "        \n",
    "        return doi    \n",
    "\n",
    "    # original version of compute_all_pair_doi, with optional max_nodes parameter for random sampling of nodes for efficient approximation\n",
    "    def compute_all_pair_doi(self, max_nodes=None):\n",
    "        # hash table for storing doi values\n",
    "        doi = {}\n",
    "        # intialize doi values to zero\n",
    "        for i in range(len(self.C)):\n",
    "            for j in range(i+1, len(self.C)):\n",
    "                doi[tuple(sorted((self.C[i].index_id, self.C[j].index_id)))] = 0\n",
    "\n",
    "        S_idxs = set([index.index_id for index in self.C])\n",
    "\n",
    "        # sample max_nodes number of nodes from the chunk\n",
    "        if max_nodes is not None:\n",
    "            nodes_sample = random.sample(self.nodes.values(), min(max_nodes, len(self.nodes)))\n",
    "        else:\n",
    "            nodes_sample = self.nodes.values()\n",
    "\n",
    "        # iterate over each IBG node\n",
    "        for Y in tqdm(nodes_sample, desc=\"Processing nodes\"):\n",
    "            \n",
    "            # remove Y.used from S\n",
    "            Y_idxs = set([index.index_id for index in Y.indexes])\n",
    "            used_Y = Y.used\n",
    "            Y_used_idxs = set([index.index_id for index in used_Y])\n",
    "            S_Y = list(S_idxs - Y_used_idxs)\n",
    "            # iterate over all pairs of indexes in S_Y\n",
    "            for i in range(len(S_Y)):\n",
    "                for j in range(i+1, len(S_Y)):\n",
    "                    a_idx = S_Y[i]\n",
    "                    b_idx = S_Y[j]\n",
    "                     \n",
    "                    # find Ya covering node in IBG\n",
    "                    Ya = (Y_idxs - {a_idx, b_idx}) | {a_idx}\n",
    "                    Ya = [self.idx2index[idx] for idx in Ya]\n",
    "                    Ya = self.cached_find_covering_node(tuple(Ya))\n",
    "                    # find Yab covering node in IBG\n",
    "                    Yab = (Y_idxs - {a_idx, b_idx}) | {a_idx, b_idx}\n",
    "                    Yab = [self.idx2index[idx] for idx in Yab]\n",
    "                    Yab = self.cached_find_covering_node(tuple(Yab))\n",
    "\n",
    "                    #used_Y = self.cached_get_cost_used(tuple(Y.indexes))[1]\n",
    "                    #used_Ya = self.cached_get_cost_used(tuple(Ya))[1]\n",
    "                    #used_Yab = self.cached_get_cost_used(tuple(Yab))[1]\n",
    "                    used_Ya = Ya.used\n",
    "                    used_Yab = Yab.used\n",
    "\n",
    "                    Uab = set([index.index_id for index in used_Y]) | set([index.index_id for index in used_Ya]) | set([index.index_id for index in used_Yab]) \n",
    "                    # find Yb_minus covering node in IBG \n",
    "                    Yb_minus = list((Uab - {a_idx, b_idx}) | {b_idx})\n",
    "                    Yb_minus = [self.idx2index[idx] for idx in Yb_minus]\n",
    "                    Yb_minus = self.cached_find_covering_node(tuple(Yb_minus))\n",
    "                    # find Yb_plus covering node in IBG\n",
    "                    Yb_plus = list((Y_idxs - {a_idx, b_idx}) | {b_idx})\n",
    "                    Yb_plus = [self.idx2index[idx] for idx in Yb_plus]\n",
    "                    Yb_plus = self.cached_find_covering_node(tuple(Yb_plus))\n",
    "\n",
    "                    # generate quadruples\n",
    "                    quadruples = [(Y.indexes, Ya.indexes, Yb_minus.indexes, Yab.indexes), (Y.indexes, Ya.indexes, Yb_plus.indexes, Yab.indexes)]\n",
    "\n",
    "                    # compute doi using the quadruples\n",
    "                    for Y_indexes, Ya_indexes, Yb_indexes, Yab_indexes in quadruples:\n",
    "                        cost_Y = self.cached_get_cost_used(tuple(Y_indexes))[0]\n",
    "                        cost_Ya = self.cached_get_cost_used(tuple(Ya_indexes))[0]\n",
    "                        cost_Yb = self.cached_get_cost_used(tuple(Yb_indexes))[0]\n",
    "                        cost_Yab = self.cached_get_cost_used(tuple(Yab_indexes))[0]\n",
    "                        # can ignore the normalization terms in denominator to get an absolute measure of doi\n",
    "                        d = abs(cost_Y - cost_Ya - cost_Yb + cost_Yab) \n",
    "                        if self.normalize_doi: \n",
    "                            d /= cost_Yab\n",
    "                        # save doi value for the pair\n",
    "                        key = tuple(sorted((a_idx, b_idx)))\n",
    "                        doi[key] = max(doi[key], d)\n",
    "                            \n",
    "        return doi\n",
    "\n",
    "\n",
    "    # parallelized version of compute_all_pair_doi\n",
    "    def compute_all_pair_doi_parallel(self, num_workers=16, max_nodes=None):\n",
    "        doi = {}\n",
    "        \n",
    "        for i in range(len(self.C)):\n",
    "            for j in range(i + 1, len(self.C)):\n",
    "                doi[tuple(sorted((self.C[i].index_id, self.C[j].index_id)))] = 0\n",
    "        \n",
    "        S_idxs = set([index.index_id for index in self.C])\n",
    "        \n",
    "        if max_nodes is not None:\n",
    "            nodes_list = random.sample(list(self.nodes.values()), min(max_nodes, len(self.nodes)))\n",
    "        else:    \n",
    "            nodes_list = list(self.nodes.values())\n",
    "        \n",
    "        chunk_size = max(1, len(nodes_list) // num_workers)\n",
    "\n",
    "        chunks = [nodes_list[i:i + chunk_size] for i in range(0, len(nodes_list), chunk_size)]\n",
    "        \n",
    "        args = [(chunk, self.C, self.idx2index, S_idxs, self.cached_find_covering_node, self.cached_get_cost_used, self.normalize_doi) for chunk in chunks]\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            results = list(tqdm(executor.map(process_node_chunk, args), total=len(chunks), desc=\"Processing nodes in parallel\"))\n",
    "        \n",
    "        for result in results:\n",
    "            for key, value in result.items():\n",
    "                doi[key] = max(doi.get(key, 0), value)\n",
    "        \n",
    "        return doi\n",
    "    \n",
    "    \n",
    "    # get precomputed degree of interaction between a pair of indexes\n",
    "    def get_doi_pair(self, a, b):\n",
    "            return self.doi[tuple(sorted((a.index_id, b.index_id)))]\n",
    "\n",
    "\n",
    "    # function for printing the IBG, using BFS level order traversal\n",
    "    def print_ibg(self):\n",
    "        q = [self.root]\n",
    "        # traverse level by level, print all node ids in a level in a single line before moving to the next level\n",
    "        while len(q) > 0:\n",
    "            next_q = []\n",
    "            for node in q:\n",
    "                print(f\"{node.id} -> \", end=\"\")\n",
    "                for child in node.children:\n",
    "                    next_q.append(child)\n",
    "            print()\n",
    "            q = next_q  \n",
    "\n",
    "\n",
    "def process_node_chunk(args):\n",
    "    nodes_chunk, C, idx2index, S_idxs, cached_find_covering_node, cached_get_cost_used, normalize_doi = args\n",
    "    doi_chunk = {}\n",
    "    \n",
    "    for Y in nodes_chunk:\n",
    "        Y_idxs = set([index.index_id for index in Y.indexes])\n",
    "        used_Y = Y.used\n",
    "        Y_used_idxs = set([index.index_id for index in used_Y])\n",
    "        S_Y = list(S_idxs - Y_used_idxs)\n",
    "        \n",
    "        for i in range(len(S_Y)):\n",
    "            for j in range(i + 1, len(S_Y)):\n",
    "                a_idx = S_Y[i]\n",
    "                b_idx = S_Y[j]\n",
    "                \n",
    "                Ya = (Y_idxs - {a_idx, b_idx}) | {a_idx}\n",
    "                Ya = [idx2index[idx] for idx in Ya]\n",
    "                Ya = cached_find_covering_node(tuple(Ya))\n",
    "                \n",
    "                Yab = (Y_idxs - {a_idx, b_idx}) | {a_idx, b_idx}\n",
    "                Yab = [idx2index[idx] for idx in Yab]\n",
    "                Yab = cached_find_covering_node(tuple(Yab))\n",
    "                \n",
    "                used_Ya = Ya.used\n",
    "                used_Yab = Yab.used\n",
    "                \n",
    "                Uab = set([index.index_id for index in used_Y]) | set([index.index_id for index in used_Ya]) | set([index.index_id for index in used_Yab])\n",
    "                \n",
    "                Yb_minus = list((Uab - {a_idx, b_idx}) | {b_idx})\n",
    "                Yb_minus = [idx2index[idx] for idx in Yb_minus]\n",
    "                Yb_minus = cached_find_covering_node(tuple(Yb_minus))\n",
    "                \n",
    "                Yb_plus = list((Y_idxs - {a_idx, b_idx}) | {b_idx})\n",
    "                Yb_plus = [idx2index[idx] for idx in Yb_plus]\n",
    "                Yb_plus = cached_find_covering_node(tuple(Yb_plus))\n",
    "                \n",
    "                quadruples = [(Y.indexes, Ya.indexes, Yb_minus.indexes, Yab.indexes), (Y.indexes, Ya.indexes, Yb_plus.indexes, Yab.indexes)]\n",
    "                \n",
    "                for Y_indexes, Ya_indexes, Yb_indexes, Yab_indexes in quadruples:\n",
    "                    cost_Y = cached_get_cost_used(tuple(Y_indexes))[0]\n",
    "                    cost_Ya = cached_get_cost_used(tuple(Ya_indexes))[0]\n",
    "                    cost_Yb = cached_get_cost_used(tuple(Yb_indexes))[0]\n",
    "                    cost_Yab = cached_get_cost_used(tuple(Yab_indexes))[0]\n",
    "                    \n",
    "                    d = abs(cost_Y - cost_Ya - cost_Yb + cost_Yab)\n",
    "                    if normalize_doi: \n",
    "                            d /= cost_Yab\n",
    "                    key = tuple(sorted((a_idx, b_idx)))\n",
    "                    doi_chunk[key] = max(doi_chunk.get(key, 0), d)\n",
    "    \n",
    "    return doi_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an SSB query generator object\n",
    "qg = QGEN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate indexes:\n",
      "0: Index name: ix_lineorder_lo_orderdate, Key cols: ('lo_orderdate',), Include cols: (), Current OID: None\n",
      "1: Index name: ix_lineorder_lo_discount, Key cols: ('lo_discount',), Include cols: (), Current OID: None\n",
      "2: Index name: ix_lineorder_lo_quantity, Key cols: ('lo_quantity',), Include cols: (), Current OID: None\n",
      "3: Index name: ix_lineorder_lo_orderdate_lo_discount, Key cols: ('lo_orderdate', 'lo_discount'), Include cols: (), Current OID: None\n",
      "4: Index name: ix_lineorder_lo_orderdate_lo_quantity, Key cols: ('lo_orderdate', 'lo_quantity'), Include cols: (), Current OID: None\n",
      "5: Index name: ix_lineorder_lo_discount_lo_orderdate, Key cols: ('lo_discount', 'lo_orderdate'), Include cols: (), Current OID: None\n",
      "6: Index name: ix_lineorder_lo_discount_lo_quantity, Key cols: ('lo_discount', 'lo_quantity'), Include cols: (), Current OID: None\n",
      "7: Index name: ix_lineorder_lo_quantity_lo_orderdate, Key cols: ('lo_quantity', 'lo_orderdate'), Include cols: (), Current OID: None\n",
      "8: Index name: ix_lineorder_lo_quantity_lo_discount, Key cols: ('lo_quantity', 'lo_discount'), Include cols: (), Current OID: None\n",
      "9: Index name: ix_lineorder_lo_orderdate_lo_discount_lo_quantity, Key cols: ('lo_orderdate', 'lo_discount', 'lo_quantity'), Include cols: (), Current OID: None\n",
      "10: Index name: ix_lineorder_lo_orderdate_lo_quantity_lo_discount, Key cols: ('lo_orderdate', 'lo_quantity', 'lo_discount'), Include cols: (), Current OID: None\n",
      "11: Index name: ix_lineorder_lo_discount_lo_orderdate_lo_quantity, Key cols: ('lo_discount', 'lo_orderdate', 'lo_quantity'), Include cols: (), Current OID: None\n",
      "12: Index name: ix_lineorder_lo_discount_lo_quantity_lo_orderdate, Key cols: ('lo_discount', 'lo_quantity', 'lo_orderdate'), Include cols: (), Current OID: None\n",
      "13: Index name: ix_lineorder_lo_quantity_lo_orderdate_lo_discount, Key cols: ('lo_quantity', 'lo_orderdate', 'lo_discount'), Include cols: (), Current OID: None\n",
      "14: Index name: ix_lineorder_lo_quantity_lo_discount_lo_orderdate, Key cols: ('lo_quantity', 'lo_discount', 'lo_orderdate'), Include cols: (), Current OID: None\n",
      "15: Index name: ix_dwdate_d_datekey, Key cols: ('d_datekey',), Include cols: (), Current OID: None\n",
      "16: Index name: ix_dwdate_d_yearmonthnum, Key cols: ('d_yearmonthnum',), Include cols: (), Current OID: None\n",
      "17: Index name: ix_dwdate_d_datekey_d_yearmonthnum, Key cols: ('d_datekey', 'd_yearmonthnum'), Include cols: (), Current OID: None\n",
      "18: Index name: ix_dwdate_d_yearmonthnum_d_datekey, Key cols: ('d_yearmonthnum', 'd_datekey'), Include cols: (), Current OID: None\n"
     ]
    }
   ],
   "source": [
    "query = qg.generate_query(2)\n",
    "#print(query)\n",
    "C = extract_query_indexes(query, include_cols=False)\n",
    "print(f\"Candidate indexes:\")\n",
    "for i, index in enumerate(C):\n",
    "    print(f\"{i}: {index}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidate indexes: 19\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of levels in IBG: 7\n",
      "Number of nodes in IBG: 16, Total number of what-if calls: 16, Time spent on what-if calls: 0.04270315170288086\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 4/4 [00:00<00:00, 53092.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent on computing all pair degree of interaction: 0.020785093307495117\n",
      "0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18 -> \n",
      "0_1_2_3_4_5_6_7_8_10_11_12_13_14_15_16_17_18 -> 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17 -> \n",
      "0_1_2_3_4_5_6_7_8_11_12_13_14_15_16_17_18 -> 0_1_2_3_4_5_6_7_8_10_11_12_13_14_15_16_17 -> 0_1_2_3_4_5_6_7_8_10_11_12_13_14_15_16_17 -> 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_17 -> \n",
      "0_1_2_4_5_6_7_8_11_12_13_14_15_16_17_18 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15_16_17 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15_16_17 -> 0_1_2_3_4_5_6_7_8_10_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15_16_17 -> 0_1_2_3_4_5_6_7_8_10_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_10_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15 -> \n",
      "0_1_2_4_5_6_7_8_11_12_13_14_15_16_17 -> 0_1_2_4_5_6_7_8_11_12_13_14_15_16_17 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_4_5_6_7_8_11_12_13_14_15_16_17 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_10_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15_16_17 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_10_11_12_13_14_15 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_10_11_12_13_14_15 -> 0_1_2_3_4_5_6_7_8_10_11_12_13_14_15 -> \n",
      "0_1_2_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15_17 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_3_4_5_6_7_8_11_12_13_14_15 -> \n",
      "0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> 0_1_2_4_5_6_7_8_11_12_13_14_15 -> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# drop all existing indexes\n",
    "conn = create_connection()\n",
    "drop_all_indexes(conn)\n",
    "close_connection(conn)\n",
    "\n",
    "# test IBG \n",
    "ibg = IBG(query, C)\n",
    "ibg.print_ibg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max benefits:\n",
      "ix_lineorder_lo_orderdate_lo_discount_lo_quantity: 813408.31\n",
      "ix_lineorder_lo_orderdate_lo_quantity_lo_discount: 795873.5100000001\n",
      "ix_lineorder_lo_orderdate_lo_discount: 789687.56\n",
      "ix_dwdate_d_yearmonthnum_d_datekey: 74.40000000002328\n",
      "ix_dwdate_d_yearmonthnum: 70.40000000002328\n",
      "ix_dwdate_d_datekey_d_yearmonthnum: 23.449999999953434\n",
      "ix_lineorder_lo_orderdate: 0\n",
      "ix_lineorder_lo_discount: 0\n",
      "ix_lineorder_lo_quantity: 0\n",
      "ix_lineorder_lo_orderdate_lo_quantity: 0\n",
      "ix_lineorder_lo_discount_lo_orderdate: 0\n",
      "ix_lineorder_lo_discount_lo_quantity: 0\n",
      "ix_lineorder_lo_quantity_lo_orderdate: 0\n",
      "ix_lineorder_lo_quantity_lo_discount: 0\n",
      "ix_lineorder_lo_discount_lo_orderdate_lo_quantity: 0\n",
      "ix_lineorder_lo_discount_lo_quantity_lo_orderdate: 0\n",
      "ix_lineorder_lo_quantity_lo_orderdate_lo_discount: 0\n",
      "ix_lineorder_lo_quantity_lo_discount_lo_orderdate: 0\n",
      "ix_dwdate_d_datekey: 0\n"
     ]
    }
   ],
   "source": [
    "# compute the max benefit of each candidate index and print the sorted list\n",
    "max_benefits = [(index, ibg.compute_max_benefit(index)) for index in C]\n",
    "max_benefits = sorted(max_benefits, key=lambda x: x[1], reverse=True)\n",
    "print(f\"Max benefits:\")\n",
    "for index, benefit in max_benefits:\n",
    "    print(f\"{index.index_id}: {benefit}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IBG     --> Cost: 688556.29, Used indexes: ['ix_lineorder_lo_orderdate_lo_discount_lo_quantity', 'ix_dwdate_d_yearmonthnum_d_datekey']\n",
      "What-if --> Cost: 688556.29, Used indexes: ['ix_lineorder_lo_orderdate_lo_discount_lo_quantity', 'ix_dwdate_d_yearmonthnum_d_datekey']\n",
      "\n",
      "Maximum benefit of adding index ix_lineorder_lo_orderdate: 0\n",
      "\n",
      "DOI between indexes ix_lineorder_lo_orderdate and ix_lineorder_lo_orderdate_lo_quantity : 0.0\n",
      "in configuration ['ix_lineorder_lo_discount', 'ix_lineorder_lo_quantity', 'ix_lineorder_lo_discount_lo_orderdate', 'ix_lineorder_lo_discount_lo_quantity', 'ix_lineorder_lo_quantity_lo_discount']\n",
      "\n",
      "DOI between indexes ix_lineorder_lo_orderdate and ix_lineorder_lo_orderdate_lo_quantity : 0\n"
     ]
    }
   ],
   "source": [
    "# pick random subset of candidate indexes\n",
    "X = random.sample(ibg.C, 8)\n",
    "cost, used = ibg.get_cost_used(X)\n",
    "print(f\"IBG     --> Cost: {cost}, Used indexes: {[idx.index_id for idx in used]}\")\n",
    "\n",
    "cost, used = ibg._cached_get_cost_used(X)\n",
    "print(f\"What-if --> Cost: {cost}, Used indexes: {[idx.index_id for idx in used]}\")\n",
    "\n",
    "# pick two indexes and a configuration\n",
    "a = ibg.C[0]\n",
    "b = ibg.C[4] \n",
    "X = [ibg.C[1], ibg.C[2], ibg.C[5], ibg.C[6], ibg.C[8]]\n",
    "\n",
    "# compute maximum benefit of adding index 'a' \n",
    "max_benefit = ibg.compute_max_benefit(a)\n",
    "print(f\"\\nMaximum benefit of adding index {a.index_id}: {max_benefit}\")\n",
    "\n",
    "# compute degree of interaction between indexes 'a' and 'b' in configuration X\n",
    "doi = ibg.compute_doi_configuration(a, b, X)\n",
    "print(f\"\\nDOI between indexes {a.index_id} and {b.index_id} : {doi}\")\n",
    "print(f\"in configuration {[idx.index_id for idx in X]}\")\n",
    "\n",
    "# compute configuration independent degree of interaction between indexes 'a' and 'b'\n",
    "doi = ibg.get_doi_pair(a, b)\n",
    "print(f\"\\nDOI between indexes {a.index_id} and {b.index_id} : {doi}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for key, value in ibg.doi.items():\n",
    "#    print(f\"doi({key[0]},   {key[1]}) = {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load workload from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the workload from a file\n",
    "with open('ssb_static_workload_2.pkl', 'rb') as f:\n",
    "    workload_dict = pickle.load(f) \n",
    "\n",
    "workload_metadata = workload_dict['metadata']\n",
    "workload = workload_dict['workload']    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Greedy Baseline (no memory constraints)\n",
    "\n",
    "In this algorithm, for every new query, we extract all cadidate indexes $C$, then use HypoPG to find the subset $S \\subseteq C$ of indexes used by the query planner and materialize the indexes in $S$ which don't exist currently.  \n",
    "\n",
    "TODO: Put limit on maximum memory for indexes => use some form of bin packing to decide which indexes to keep in the configuration => need to allow index dropping as well as creation, maybe could drop/evict least recently used (LRU) indexes to make room for new indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HypoGreedy:\n",
    "\n",
    "    def __init__(self, config_memory_MB=2048, max_key_columns=3, include_cols=False):\n",
    "        self.currently_materialized_indexes = {}\n",
    "        self.total_whatif_calls = 0\n",
    "        self.total_whatif_time = 0\n",
    "        # memory budget for configuration\n",
    "        self.config_memory_MB = config_memory_MB\n",
    "        # maximum number of key columns in an index\n",
    "        self.max_key_columns = max_key_columns\n",
    "        # allow include columns in indexes\n",
    "        self.include_cols = include_cols\n",
    "        # track time \n",
    "        self.recommendation_time = []\n",
    "        self.materialization_time = []\n",
    "        self.execution_time = []\n",
    "        self.total_recommendation_time = 0\n",
    "        self.total_materialization_time = 0\n",
    "        self.total_execution_time_actual = 0\n",
    "        self.total_time = 0\n",
    "        self.current_round = 0\n",
    "\n",
    "        # index size cache\n",
    "        self.index_size = {}\n",
    "\n",
    "        # index_stats_cache\n",
    "        self.index_stats = {}\n",
    "\n",
    "        print(f\"*** Dropping all materialized indexes...\")\n",
    "        conn = create_connection()\n",
    "        drop_all_indexes(conn)\n",
    "        close_connection(conn)\n",
    "\n",
    "\n",
    "    # create statistics entry for a new index\n",
    "    def create_index_stats(self, index_id):\n",
    "        stats = {'when_selected':[], 'when_materialized':[], 'when_used':[]}\n",
    "        self.index_stats[index_id] = stats\n",
    "\n",
    "\n",
    "    # materialize new indexes and evict old indexes if necessary \n",
    "    def materialize_indexes(self, recommended_indexes):\n",
    "        # materialize new recommendation\n",
    "        indexes_added = [index for index in recommended_indexes if index.index_id not in self.currently_materialized_indexes]\n",
    "\n",
    "        # compute size of currently materialized indexes (use hypothetical sizes)\n",
    "        current_size = sum([self.index_size[index.index_id] for index in self.currently_materialized_indexes.values()])\n",
    "        # remove least recently used indexes to make space for new indexes if necessary\n",
    "        indexes_removed = []\n",
    "        while current_size + sum([self.index_size[index.index_id] for index in indexes_added]) > self.config_memory_MB:\n",
    "            if current_size == 0:\n",
    "                # this means the new indexes are too big to fit in the memory budget, need to pack a subset of them\n",
    "                print(f\"New indexes are too big to fit in the memory budget, packing a subset of them...\")\n",
    "                # sort the new indexes in descending order of size\n",
    "                indexes_added = sorted(indexes_added, key=lambda x: self.index_size[x.index_id], reverse=True)\n",
    "                # pack the indexes until the memory budget is reached\n",
    "                packed_indexes = []\n",
    "                for index in indexes_added:\n",
    "                    if current_size + self.index_size[index.index_id] <= self.config_memory_MB:\n",
    "                        packed_indexes.append(index)\n",
    "                        current_size += self.index_size[index.index_id]    \n",
    "                indexes_added = packed_indexes\n",
    "                break\n",
    "            \n",
    "            # find the least recently selected index\n",
    "            lru_index = min(self.currently_materialized_indexes.values(), key=lambda x: self.index_stats[x.index_id]['when_selected'][-1])\n",
    "            print(f\"Evicting index {lru_index.index_id} to make space for new indexes\")\n",
    "            # remove the index from the materialized indexes\n",
    "            indexes_removed.append(lru_index)\n",
    "            del self.currently_materialized_indexes[lru_index.index_id]\n",
    "            # update the current size\n",
    "            current_size -= self.index_size[lru_index.index_id]\n",
    "\n",
    "        for index in indexes_added:\n",
    "            self.currently_materialized_indexes[index.index_id] = index\n",
    "\n",
    "        print(f\"New indexes added this round: {[index.index_id for index in indexes_added]}\")\n",
    "        print(f\"Old indexes removed this round: {[index.index_id for index in indexes_removed]}\")\n",
    "\n",
    "        # materialize new configuration\n",
    "        conn = create_connection()\n",
    "        bulk_drop_indexes(conn, indexes_removed)\n",
    "        close_connection(conn)\n",
    "        print(f\"Materializing new indexes...\")\n",
    "        start_time = time.time()\n",
    "        conn = create_connection()\n",
    "        bulk_create_indexes(conn, indexes_added)\n",
    "        close_connection(conn)\n",
    "        end_time = time.time()\n",
    "        creation_time = end_time - start_time\n",
    "\n",
    "        # update index usage stats\n",
    "        for index in indexes_added:\n",
    "            self.index_stats[index.index_id]['when_materialized'].append(self.current_round)\n",
    "\n",
    "\n",
    "        print(f\"Currently materialized indexes: {list(self.currently_materialized_indexes.keys())}\")\n",
    "\n",
    "        return creation_time\n",
    "\n",
    "\n",
    "    # extract candidate indexes from given query\n",
    "    def extract_indexes(self, query_object):        \n",
    "        candidate_indexes = extract_query_indexes(query_object,  self.max_key_columns, self.include_cols)\n",
    "        new_indexes = [index for index in candidate_indexes if index.index_id not in self.index_size]\n",
    "        # get hypothetical idnex sizes\n",
    "        conn = create_connection()\n",
    "        new_index_sizes = get_hypothetical_index_sizes(conn, new_indexes)\n",
    "        close_connection(conn)\n",
    "        for index in new_indexes:\n",
    "            self.index_size[index.index_id] = new_index_sizes[index.index_id]\n",
    "\n",
    "        # create index stats for new indexes\n",
    "        for index in new_indexes:\n",
    "            self.create_index_stats(index.index_id)\n",
    "        # update index stats for candidate indexes\n",
    "        for index in candidate_indexes:\n",
    "            self.index_stats[index.index_id]['when_selected'].append(self.current_round)    \n",
    "\n",
    "        return candidate_indexes\n",
    "\n",
    "\n",
    "    # get hypothetical cost and used indexes for the query in the given configuration, recommend the used indexes\n",
    "    def get_recommendation(self, query_string, indexes):\n",
    "        start_time = time.time()\n",
    "        conn = create_connection()\n",
    "        # hide existing indexes\n",
    "        bulk_hide_indexes(conn, list(self.currently_materialized_indexes.values()))\n",
    "        # create hypothetical indexes\n",
    "        hypo_indexes = bulk_create_hypothetical_indexes(conn, indexes)\n",
    "        # map oid to index object\n",
    "        oid2index = {}\n",
    "        for i in range(len(hypo_indexes)):\n",
    "            oid2index[hypo_indexes[i]] = indexes[i]\n",
    "        # get cost and used indexes\n",
    "        cost, indexes_used = get_query_cost_estimate_hypo_indexes(conn, query_string, show_plan=False)\n",
    "        # map used index oids to index objects\n",
    "        used = [oid2index[oid] for oid, scan_type, scan_cost in indexes_used]\n",
    "        # drop hypothetical indexes\n",
    "        bulk_drop_hypothetical_indexes(conn)\n",
    "        # unhide existing indexes\n",
    "        bulk_unhide_indexes(conn, list(self.currently_materialized_indexes.values()))\n",
    "        close_connection(conn)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Store the result in the class-level cache\n",
    "        #self._class_cache[indexes_tuple] = (cost, used)\n",
    "        self.total_whatif_calls += 1\n",
    "        self.total_whatif_time += end_time - start_time\n",
    "\n",
    "        print(f\"Recommended indexes: {[index.index_id for index in used]}\")\n",
    "\n",
    "        return used\n",
    "    \n",
    "    # execute the query\n",
    "    def execute(self, query_object):\n",
    "        # restart the server before each query execution\n",
    "        restart_postgresql()\n",
    "        conn = create_connection()\n",
    "        execution_time, rows, table_access_info, index_access_info, bitmap_heapscan_info = execute_query(conn, query_object.query_string, with_explain=True, return_access_info=True)\n",
    "        close_connection(conn)\n",
    "\n",
    "        # update index usage stats\n",
    "        for index_id in index_access_info:\n",
    "            self.index_stats[index_id]['when_used'].append(self.current_round)    \n",
    "\n",
    "        print(f\"Execution time: {execution_time/1000} s\")\n",
    "        print(f\"Indexes accessed --> {list(index_access_info.keys())}\")\n",
    "        return execution_time\n",
    "\n",
    "\n",
    "    # process the query using greedy algorithm\n",
    "    def process_greedy(self, query_object):\n",
    "        self.current_round += 1\n",
    "        #print(f\"Round# {self.n_rounds}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        # extract candidate indexes\n",
    "        print(\"Extracting candidate indexes...\")\n",
    "        candidate_indexes = self.extract_indexes(query_object)\n",
    "\n",
    "        # get index recommendation\n",
    "        print(\"Getting index recommendation...\")\n",
    "        recommended_indexes = self.get_recommendation(query_object.query_string, candidate_indexes)\n",
    "        end_time = time.time()\n",
    "        recommendation_time = end_time - start_time\n",
    "        self.recommendation_time.append(recommendation_time)\n",
    "\n",
    "        # materialize indexes\n",
    "        print(\"Materializing indexes...\")\n",
    "        materialization_time = self.materialize_indexes(recommended_indexes)\n",
    "        self.materialization_time.append(materialization_time)\n",
    "\n",
    "        # execute query\n",
    "        print(\"Executing query...\")\n",
    "        execution_time = self.execute(query_object)\n",
    "        self.execution_time.append(execution_time)\n",
    "\n",
    "        self.total_recommendation_time += recommendation_time\n",
    "        self.total_materialization_time += materialization_time\n",
    "        self.total_execution_time_actual += execution_time\n",
    "        self.total_time += recommendation_time + materialization_time + (execution_time/100)\n",
    "\n",
    "        print(f\"\\nTotal recommendation time so far: {self.total_recommendation_time} s\")\n",
    "        print(f\"Total materialization time so far: {self.total_materialization_time} s\")\n",
    "        print(f\"Total execution time so far: {self.total_execution_time_actual/1000} s\")\n",
    "        print(f\"Total time spent so far: {self.total_time} s\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Dropping all materialized indexes...\n",
      "Index 'ix_lineorder_lo_quantity_lo_linenumber' on table 'lineorder' dropped successfully\n",
      "Index 'ix_dwdate_d_year_d_datekey' on table 'dwdate' dropped successfully\n",
      "Index 'ix_supplier_s_nation_s_city_s_suppkey' on table 'supplier' dropped successfully\n",
      "Index 'ix_customer_c_nation_c_city_c_custkey' on table 'customer' dropped successfully\n",
      "\n",
      "Processing query # 1\n",
      "Extracting candidate indexes...\n",
      "Getting index recommendation...\n",
      "Recommended indexes: ['ix_dwdate_d_year_d_datekey', 'ix_lineorder_lo_quantity']\n",
      "Materializing indexes...\n",
      "New indexes added this round: ['ix_dwdate_d_year_d_datekey', 'ix_lineorder_lo_quantity']\n",
      "Old indexes removed this round: []\n",
      "Materializing new indexes...\n",
      "Successfully created index: 'ix_dwdate_d_year_d_datekey', size: 0.07031250000000000000 MB, creation time: 8.40 ms\n"
     ]
    }
   ],
   "source": [
    "# instatiate HypoGreedy object\n",
    "hg = HypoGreedy()\n",
    "\n",
    "# run greedy algorithm on 100 queries\n",
    "for i, query_object in enumerate(workload[:9]):\n",
    "    print(f\"\\nProcessing query # {i+1}\")\n",
    "    hg.process_greedy(query_object)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WFIT class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WFIT:\n",
    "\n",
    "    def __init__(self, S_0=[], max_key_columns=None, include_cols=False, max_U=30, idxCnt=50, stateCnt=200, histSize=100, rand_cnt=1):\n",
    "        # initial set of materialzed indexes\n",
    "        self.S_0 = S_0\n",
    "        # maximum number of key columns in an index\n",
    "        self.max_key_columns = max_key_columns\n",
    "        # allow include columns in indexes\n",
    "        self.include_cols = include_cols\n",
    "        # maximum number of candidate indexes for IBG \n",
    "        self.max_U = max_U\n",
    "        # parameter for maximum number of candidate indexes tracked \n",
    "        self.idxCnt = idxCnt\n",
    "        # parameter for maximum number of MTS states/configurations\n",
    "        self.stateCnt = stateCnt\n",
    "        # parameter for maximum number of historical index statistics kept\n",
    "        self.histSize = histSize\n",
    "        # parameter for number of randomized clustering iterations\n",
    "        self.rand_cnt = rand_cnt\n",
    "        # growing list of candidate indexes (initially contains S_0)\n",
    "        self.U = {index.index_id:index for index in S_0}\n",
    "        # index benefit and interaction statistics\n",
    "        self.idxStats = defaultdict(list)\n",
    "        self.intStats = defaultdict(list)\n",
    "        # list of currently monitored indexes\n",
    "        self.C = {index.index_id:index for index in S_0} \n",
    "        # list of currently materialized indexes\n",
    "        self.M = {index.index_id:index for index in S_0}  \n",
    "        # initialize stable partitions (each partition is a singleton set of indexes from S_0)\n",
    "        self.stable_partitions = [[index] for index in S_0] if S_0 else [[]]\n",
    "        # keep track of candidate index sizes\n",
    "        self.index_size = {}\n",
    "        self.n_pos = 0\n",
    "\n",
    "        print(f\"##################################################################\")\n",
    "        # initialize work function instance for each stable partition\n",
    "        self.W = self.initilize_WFA(self.stable_partitions)\n",
    "        # initialize current recommendations for each stable partition\n",
    "        self.current_recommendations = {i:indexes for i, indexes in enumerate(self.stable_partitions)}\n",
    "\n",
    "\n",
    "        print(f\"Initial set of materialized indexes: {[index.index_id for index in S_0]}\")\n",
    "        print(f\"Stable partitions: {[[index.index_id for index in P] for P in self.stable_partitions]}\")\n",
    "        print(f\"Initial work function instances: \")\n",
    "        for i, wf in self.W.items():\n",
    "            print(f\"\\tWFA Instance #{i}: {wf}\")\n",
    "\n",
    "        print(f\"\\nMaximum number of candidate indexes tracked: {idxCnt}\")\n",
    "        print(f\"Maximum number of MTS states/configurations: {stateCnt}\")\n",
    "        print(f\"Maximum number of historical index statistics kept: {histSize}\")\n",
    "        print(f\"Number of randomized clustering iterations: {rand_cnt}\")\n",
    "\n",
    "        # bulk drop all materialized indexes\n",
    "        print(f\"*** Dropping all materialized indexes...\")\n",
    "        conn = create_connection()\n",
    "        drop_all_indexes(conn)\n",
    "        close_connection(conn)\n",
    "        print(f\"##################################################################\\n\")\n",
    "\n",
    "        # set random seed\n",
    "        random.seed(1234)\n",
    "        # track time \n",
    "        self.recommendation_time = []\n",
    "        self.materialization_time = []\n",
    "        self.execution_time = []\n",
    "        self.total_recommendation_time = 0\n",
    "        self.total_materialization_time = 0\n",
    "        self.total_execution_time_actual = 0\n",
    "        self.total_time_actual = 0\n",
    "        self.total_cost_wfit = 0\n",
    "        self.total_cost_simple = 0\n",
    "        self.total_no_index_cost = 0\n",
    "\n",
    "    # initialize a WFA instance for each stable partition\n",
    "    def initilize_WFA(self, stable_partitions):\n",
    "        print(f\"Initializing WFA instances for {len(stable_partitions)} stable partitions...\")\n",
    "        W = {}\n",
    "        for i, P in enumerate(stable_partitions):\n",
    "            # initialize all MTS states, i.e. power set of indexes in the partition\n",
    "            states = [tuple(sorted(state, key=lambda x: x.index_id)) for state in powerset(P)]\n",
    "            # initialize work function instance for the partition\n",
    "            W[i] = {tuple(X):self.compute_transition_cost(self.S_0, X) for X in states}    \n",
    "\n",
    "        for i in W:\n",
    "            print(f\"WFA instance #{i}: {W[i]}\")\n",
    "\n",
    "        return W\n",
    "\n",
    "\n",
    "    # update WFIT step for next query in workload (this is the MAIN INTERFACE for generating an index configuration recommendation)\n",
    "    def process_WFIT(self, query_object, remove_stale_U=True, remove_stale_freq=1, execute=True, materialize=True, verbose=False):\n",
    "        self.n_pos += 1        \n",
    "        previous_config = list(self.M.values())\n",
    " \n",
    "        # get estimated no index cost for the query\n",
    "        conn = create_connection()\n",
    "        self.total_no_index_cost += hypo_query_cost(conn, query_object, [], [])\n",
    "        close_connection(conn)\n",
    "\n",
    "        # generate new partitions \n",
    "        if verbose: print(f\"Generating new partitions for query #{self.n_pos}\")\n",
    "        start_time_1 = time.time()\n",
    "        new_partitions, need_to_repartition, ibg = self.choose_candidates(self.n_pos, query_object, verbose=False)\n",
    "        end_time_1 = time.time()\n",
    "\n",
    "        # repartition if necessary\n",
    "        start_time_2 = time.time()\n",
    "        if need_to_repartition:\n",
    "            if verbose: print(f\"Repartitioning...\")\n",
    "            self.repartition(new_partitions, verbose)\n",
    "        end_time_2 = time.time()\n",
    "        \n",
    "        # analyze the query\n",
    "        if verbose: print(f\"Analyzing query...\")\n",
    "        start_time_3 = time.time()\n",
    "        config_materialization_time = self.analyze_query(query_object, ibg, materialize, verbose=False)\n",
    "        end_time_3 = time.time()    \n",
    "\n",
    "        if verbose: \n",
    "            print(f\"New indexes added this round: {[index.index_id for index in (set(self.M.values()) - set(previous_config))]}\")\n",
    "            print(f\"Old indexes removed this round: {[index.index_id for index in (set(previous_config) - set(self.M.values()))]}\")\n",
    "            print(f\"Currently materialized indexes: {[index.index_id for index in self.M.values()]}\") \n",
    "\n",
    "        # execute the query with the new configuration\n",
    "        if execute:\n",
    "            # restart the server before each query execution\n",
    "            restart_postgresql()\n",
    "            if verbose: print(f\"Executing query...\")\n",
    "            conn = create_connection()\n",
    "            execution_time, rows, table_access_info, index_access_info, bitmap_heapscan_info = execute_query(conn, query_object.query_string, with_explain=True, return_access_info=True)\n",
    "            close_connection(conn)\n",
    "            execution_time /= 1000\n",
    "            print(f\"Indexes accessed --> {list(index_access_info.keys())}\")\n",
    "        else:\n",
    "            execution_time = 0\n",
    "        self.total_materialization_time += config_materialization_time\n",
    "        self.total_execution_time_actual += execution_time\n",
    "        self.materialization_time.append(config_materialization_time)\n",
    "        self.execution_time.append(execution_time)\n",
    "\n",
    "        # remove stale indexes from U\n",
    "        if remove_stale_U and (self.n_pos % remove_stale_freq == 0):\n",
    "            if verbose: print(f\"Removing stale indexes from U...\")\n",
    "            self.remove_stale_indexes_U(verbose)\n",
    "\n",
    "        # simple recommendation, just the used indexes in the IBG root node\n",
    "        self.get_simple_recommendation_ibg(ibg)\n",
    "\n",
    "        # compute hypothetical speedup from switching to new configuration\n",
    "        new_config = list(self.M.values())\n",
    "        if materialize:\n",
    "            materialized_indexes = self.M.values()\n",
    "        else:\n",
    "            materialized_indexes = []    \n",
    "        conn = create_connection()\n",
    "        speedup_wfit, query_execution_cost_wfit = hypo_query_speedup(conn, query_object, previous_config, new_config, materialized_indexes)\n",
    "        self.total_cost_wfit += float(query_execution_cost_wfit) + sum([float(self.get_index_creation_cost(index)) for index in (set(new_config) - set(previous_config))])  \n",
    "        # also compute speed up for the simple recommendation\n",
    "        speedup_simple, query_execution_cost_simple = hypo_query_speedup(conn, query_object, previous_config, list(ibg.root.used), materialized_indexes)\n",
    "        self.total_cost_simple += float(query_execution_cost_simple) + sum([float(self.get_index_creation_cost(index)) for index in (set(ibg.root.used) - set(previous_config))])\n",
    "        close_connection(conn)   \n",
    "\n",
    "        self.total_recommendation_time += (end_time_3 - start_time_1 - config_materialization_time)\n",
    "        self.total_time_actual += (end_time_3 - start_time_1 - config_materialization_time) + execution_time + config_materialization_time \n",
    "        self.recommendation_time.append(end_time_3 - start_time_1 - config_materialization_time)\n",
    "\n",
    "        print(f\"*** Hypothetical Speedup --> WFIT: {speedup_wfit}, Simple: {speedup_simple}\")\n",
    "        print(f\"*** Hypothetical Total cost --> WFIT: {self.total_cost_wfit}, Simple: {self.total_cost_simple}, WFIT/Simple: {self.total_cost_wfit/self.total_cost_simple}\")\n",
    "        print(f\"*** Hypothetical Total Cost No-Index --> {self.total_no_index_cost}\")\n",
    "\n",
    "        print(f\"Total recommendation time taken for query #{self.n_pos}: {end_time_3 - start_time_1 - config_materialization_time} seconds\")\n",
    "        print(f\"Actual execution time for query #{self.n_pos} --> {execution_time} seconds\")\n",
    "        print(f\"\\nTotal recommendation time so far --> {self.total_recommendation_time} seconds\")\n",
    "        print(f\"Total materialization time so far --> {self.total_materialization_time} seconds\")\n",
    "        print(f\"Total execution time so far --> {self.total_execution_time_actual} seconds\")\n",
    "        print(f\"Total time so far --> {self.total_time_actual} seconds\")\n",
    "        print(f\"(Partitioning: {end_time_1 - start_time_1} seconds, Repartitioning: {end_time_2 - start_time_2} seconds, Analyzing: {end_time_3 - start_time_3 - config_materialization_time} seconds), Materializing config: {config_materialization_time} seconds, Executing query: {execution_time} seconds\")\n",
    "\n",
    "\n",
    "    # Simple baseline recommendation: just the used indexes in the IBG root node, i.e. these are the indexes from \n",
    "    # the full set of candidate indexes which are used in the query plan\n",
    "    def get_simple_recommendation_ibg(self, ibg):\n",
    "        simple_recommendation = ibg.root.used  \n",
    "        wfit_recommendation = [index.index_id for i in self.current_recommendations for index in self.current_recommendations[i]]\n",
    "        print(f\"*** WFIT recommendation: {sorted(wfit_recommendation)}\")\n",
    "        print(f\"*** Simple recommendation: {sorted([index.index_id for index in simple_recommendation])}\") \n",
    "\n",
    "\n",
    "    # check for stale indexes in U and remove them\n",
    "    def remove_stale_indexes_U(self, verbose):\n",
    "        # find out which indexes have loweest benefit statistics\n",
    "        avg_benefit = {}\n",
    "        for index_id in self.U:\n",
    "            # compute average benefit of the index from all stats\n",
    "            avg_benefit[index_id] = sum([stat[1] for stat in self.idxStats[index_id]]) / len(self.idxStats[index_id])\n",
    "\n",
    "        # sort indexes by average benefit\n",
    "        sorted_indexes = sorted(avg_benefit, key=avg_benefit.get, reverse=True)\n",
    "\n",
    "        # mark all indexes with zero benefit and not in M and S_0 as stale\n",
    "        stale_indexes = set()\n",
    "        for index_id in sorted_indexes:\n",
    "            if avg_benefit[index_id] == 0 and index_id not in self.M and index_id not in self.S_0:\n",
    "                stale_indexes.add(index_id)\n",
    "\n",
    "        # remove stale indexes from U\n",
    "        print(f\"Number of indexes in U: {len(self.U)}\")\n",
    "        num_removed = 0\n",
    "        for index_id in stale_indexes:\n",
    "            #if verbose: print(f\"Removing stale index: {index_id}\")\n",
    "            del self.U[index_id]\n",
    "            #if verbose: print(f\"Number of indexes in U after removal: {len(self.U)}\")\n",
    "            num_removed += 1\n",
    "\n",
    "        # keep at most self.max_U of highest benefit indexes in U, make sure to keep all indexes in S_0 and M\n",
    "        if self.max_U is not None and len(self.U) > self.max_U:\n",
    "            for index_id in sorted_indexes[self.max_U:]:\n",
    "                if index_id not in self.M and index_id not in self.S_0 and index_id in self.U:\n",
    "                    del self.U[index_id]\n",
    "                    num_removed += 1\n",
    "\n",
    "        # remove stale indexes from stable partitions and C (not sure if this is necessary...)\n",
    "        \n",
    "        if verbose:\n",
    "            #print(f\"Average benefit of indexes:\")\n",
    "            #for index_id in sorted_indexes:\n",
    "            #    print(f\"\\tIndex {index_id}: {avg_benefit[index_id]}, Stale: {index_id in stale_indexes}\")\n",
    "                \n",
    "            print(f\"Number of indexes removed: {num_removed}, Number of indexes remaining: {len(self.U)}\")\n",
    "            #print(f\"Indexes in U: {self.U.keys()}\")\n",
    "                \n",
    "\n",
    "    # repartition the stable partitions based on the new partitions\n",
    "    def repartition(self, new_partitions, verbose):\n",
    "        # all indexes recommmendations across the WFA instances from previous round\n",
    "        S_curr = set(chain(*self.current_recommendations.values()))\n",
    "        C = set(self.C.values()) \n",
    "        S_0 = set(self.S_0)\n",
    "\n",
    "        # compute L2-norm of the work function across all partitions\n",
    "        #l2_norm_wf_old = 0\n",
    "        #for i, wf in self.W.items():\n",
    "        #    l2_norm_wf_old += sum([wf[X]**2 for X in wf])\n",
    "\n",
    "        \n",
    "        # re-initizlize WFA instances and recommendations for each new partition\n",
    "        if verbose: print(f\"Reinitializing WFA instances...\")\n",
    "        W = {}\n",
    "        recommendations = {}\n",
    "        for i, P in enumerate(new_partitions):\n",
    "            partition_all_configs = [tuple(sorted(state, key=lambda x: x.index_id)) for state in powerset(P)]\n",
    "            wf = {}\n",
    "            # initialize work function values for each state\n",
    "            #print(f\"\\tNew partition # {i}\")\n",
    "            for X in partition_all_configs:\n",
    "                wf_x = 0\n",
    "                for j, wf_prev in self.W.items(): \n",
    "                    wf_x += wf_prev[tuple(sorted(set(X) & set(self.stable_partitions[j]), key=lambda x: x.index_id))]\n",
    "                \n",
    "                transition_cost_term = self.compute_transition_cost(S_0 & (set(P) - C), set(X) - C)\n",
    "                wf[X] = wf_x + transition_cost_term #- self.total_no_index_cost\n",
    "                #print(f\"\\t\\t w[{tuple([index.index_id for index in X])}] --> {wf[X]}   ({wf_x} + {transition_cost_term})\")\n",
    "            \n",
    "            W[i] = wf\n",
    "            # initialize current state/recommended configuration of the WFA instance\n",
    "            recommendations[i] = list(set(P) & S_curr)\n",
    "\n",
    "        \"\"\"\n",
    "        # compute l2 norm of the work function across all partitions\n",
    "        l2_norm_wf_new = 0\n",
    "        for i, wf in W.items():\n",
    "            l2_norm_wf_new += sum([wf[X]**2 for X in wf])\n",
    "        # rescale work function values to maintain the same l2 norm (otherwise wf values will keep increasing\n",
    "        # due to the summation terms in repartitioning)\n",
    "        for i, wf in W.items():\n",
    "            for X in wf:\n",
    "                wf[X] = wf[X] * (l2_norm_wf_old / l2_norm_wf_new)    \n",
    "        \"\"\"\n",
    "\n",
    "        # replace current stable partitions, WFA instances and recommendations with the new ones\n",
    "        self.stable_partitions = new_partitions\n",
    "        self.W = W\n",
    "        self.current_recommendations = recommendations\n",
    "        \"\"\"\n",
    "        if verbose: \n",
    "            print(f\"Replaced stable partitions, WFA instances and recommendations with new ones\")\n",
    "            print(f\"New WFA instances:\")\n",
    "            for i, wf in self.W.items():\n",
    "                print(f\"\\tWFA Instance #{i}:\")\n",
    "                for X, value in wf.items():\n",
    "                    print(f\"\\t\\tState: {tuple([index.index_id for index in X])}, Work function value: {value}\")\n",
    "        \"\"\"\n",
    "        \n",
    "        self.C = {}\n",
    "        for P in self.stable_partitions:\n",
    "            for index in P: \n",
    "                self.C[index.index_id] = index      \n",
    "\n",
    "\n",
    "    # update WFA instance on each stable partition and get index configuration recommendation\n",
    "    def analyze_query(self, query_object, ibg, materialize, verbose):\n",
    "        new_recommendations = {}\n",
    "        # update WFA instance for each stable partition\n",
    "        all_indexes_added = []\n",
    "        all_indexes_removed = []\n",
    "        for i in self.W:\n",
    "            if verbose: print(f\"Updating WFA instance: {i}\")\n",
    "            self.W[i], new_recommendations[i]  = self.process_WFA(query_object, self.W[i], self.current_recommendations[i], ibg, verbose)\n",
    "\n",
    "            # materialize new recommendation\n",
    "            indexes_added = set(new_recommendations[i]) - set(self.current_recommendations[i])\n",
    "            indexes_removed = set(self.current_recommendations[i]) - set(new_recommendations[i])\n",
    "            if verbose: print(f\"\\tWFA Instance #{i}, Num States: {len(self.W[i])}, New Recommendation: {[index.index_id for index in new_recommendations[i]]} --> Indexes Added: {[index.index_id for index in indexes_added]}, Indexes Removed: {[index.index_id for index in indexes_removed]}\")\n",
    "            \n",
    "            for index in indexes_added:\n",
    "                self.M[index.index_id] = index\n",
    "            for index in indexes_removed:\n",
    "                del self.M[index.index_id]    \n",
    "                \n",
    "            self.current_recommendations[i] = new_recommendations[i]\n",
    "\n",
    "            all_indexes_added += list(indexes_added)\n",
    "            all_indexes_removed += list(indexes_removed)\n",
    "\n",
    "        if materialize:\n",
    "            # materialize new configuration\n",
    "            if verbose: print(f\"Materializing new configuration...\")\n",
    "            start_time = time.time()\n",
    "            conn = create_connection()\n",
    "            bulk_drop_indexes(conn, all_indexes_removed)\n",
    "            bulk_create_indexes(conn, all_indexes_added)\n",
    "            close_connection(conn)\n",
    "            end_time = time.time()\n",
    "            creation_time = end_time - start_time\n",
    "        else:\n",
    "            creation_time = 0\n",
    "\n",
    "        return creation_time\n",
    "\n",
    "\n",
    "    # update a WFA instance for the given query    \n",
    "    def process_WFA(self, query_object, wf, S_current, ibg, verbose):\n",
    "        # update work function values for each state in the WFA instance\n",
    "        wf_new = {}\n",
    "        p = {}\n",
    "        for Y in wf.keys():\n",
    "            sorted_Y = tuple(sorted(Y, key=lambda x: x.index_id))\n",
    "            #print(f\"\\tComputing work function value for state: {tuple([index.index_id for index in sorted_Y])}, old value --> {wf[sorted_Y]}\")\n",
    "            # compute new work function value for state Y \n",
    "            min_wf_value = float('inf')\n",
    "            wf_X = {}\n",
    "            for X in wf.keys():\n",
    "                sorted_X = tuple(sorted(X, key=lambda x: x.index_id))\n",
    "                wf_term = wf[sorted_X]\n",
    "                query_cost_term = ibg.get_cost_used(list(sorted_X))[0]\n",
    "                transition_cost_term = self.compute_transition_cost(sorted_X, sorted_Y) \n",
    "                wf_value = wf_term + query_cost_term + transition_cost_term\n",
    "                #print(f'\\t\\tValue for X = {tuple([index.index_id for index in sorted_X])} -->  {wf_value}  ({wf_term} + {query_cost_term} + {transition_cost_term})')\n",
    "                \n",
    "                wf_X[sorted_X] = wf_value\n",
    "                # keep track of minimum work function value for the state\n",
    "                if wf_value < min_wf_value:\n",
    "                    min_wf_value = wf_value\n",
    "\n",
    "            wf_new[sorted_Y] = min_wf_value\n",
    "            min_p = []\n",
    "            for X in wf_X:\n",
    "                if wf_X[X] == min_wf_value:\n",
    "                    min_p.append(X)\n",
    "            p[sorted_Y] = min_p\n",
    "            #print(f\"\\tUpdated value: w[{tuple([index.index_id for index in sorted_Y])}] --> {wf_new[sorted_Y]}, p: {[[index.index_id for index in indexes] for indexes in p]}\")\n",
    "\n",
    "        # compute scores and find best state\n",
    "        best_score = float('inf')\n",
    "        best_state = None  \n",
    "        for Y in wf_new:\n",
    "            sorted_Y = tuple(sorted(Y, key=lambda x: x.index_id))\n",
    "            score = wf_new[sorted_Y] + self.compute_transition_cost(sorted_Y, S_current)  \n",
    "            if score < best_score and sorted_Y in p[sorted_Y]:\n",
    "                best_score = score\n",
    "                best_state = sorted_Y  #min_p\n",
    "\n",
    "        if verbose:\n",
    "            #print(f\"\\tAll updated Work function values for WFA instance:\")\n",
    "            #for Y, value in wf_new.items():\n",
    "            #    print(f\"\\t\\tstate :{tuple([index.index_id for index in Y])} , w_value: {value}, p: {[[index.index_id for index in indexes] for indexes in p]}, score: {scores[Y]}\")\n",
    "\n",
    "            print(f\"\\tBest state: {tuple([index.index_id for index in best_state])}, Best score: {best_score}\")\n",
    "        \n",
    "        return wf_new, best_state\n",
    "\n",
    "    # compute index benefit graph for the given query and candidate indexes\n",
    "    def compute_IBG(self, query_object, candidate_indexes):\n",
    "        return IBG(query_object, candidate_indexes)\n",
    "    \n",
    "\n",
    "    # extract candidate indexes from given query\n",
    "    def extract_indexes(self, query_object, max_size_mb=4096):        \n",
    "        candidate_indexes = extract_query_indexes(query_object,  self.max_key_columns, self.include_cols)\n",
    "        new_indexes = [index for index in candidate_indexes if index.index_id not in self.index_size]\n",
    "        # get hypothetical idnex sizes\n",
    "        conn = create_connection()\n",
    "        new_index_sizes = get_hypothetical_index_sizes(conn, new_indexes)\n",
    "        close_connection(conn)\n",
    "        for index in new_indexes:\n",
    "            self.index_size[index.index_id] = new_index_sizes[index.index_id]\n",
    "\n",
    "        # filter out indexes that exceed the maximum size\n",
    "        candidate_indexes = [index for index in candidate_indexes if self.index_size[index.index_id] <= max_size_mb]\n",
    "\n",
    "        return candidate_indexes\n",
    "        \n",
    "\n",
    "    # generate stable partitions/sets of indexes for next query in workload\n",
    "    def choose_candidates(self, n_pos, query_object, verbose):\n",
    "        # extract candidate indexes from the query\n",
    "        candidate_indexes = self.extract_indexes(query_object)\n",
    "        # add new candidate indexes to the list of all candidate indexes\n",
    "        num_new = 0\n",
    "        for index in candidate_indexes:\n",
    "            if index.index_id not in self.U:\n",
    "                self.U[index.index_id] = index\n",
    "                num_new += 1\n",
    "\n",
    "        #if len(self.U) > self.max_U:\n",
    "        #    raise ValueError(\"Number of candidate indexes exceeds the maximum limit. Aborting WFIT...\")\n",
    "\n",
    "        if verbose: \n",
    "            print(f\"Extracted {num_new} new indexes from query.\")\n",
    "            print(f\"Candidate indexes (including those currently materialized), |U| = {len(self.U)}\")\n",
    "            #print(f\"{[index.index_id for index in self.U.values()]}\")\n",
    "\n",
    "        # TODO: need mechanism to evict indexes from U that may have gone \"stale\" to prevent unbounded growth of U\n",
    "\n",
    "        \n",
    "        # compute index benefit graph for the query\n",
    "        if verbose: print(f\"Computing IBG...\")\n",
    "        ibg = self.compute_IBG(query_object, list(self.U.values()))\n",
    "\n",
    "        #if verbose: print(f\"Candidate index sizes in Mb: {[(index.index_id,index.size) for index in self.U.values()]}\")\n",
    "        \n",
    "        # update statistics for the candidate indexes (n_pos is the position of the query in the workload sequence)\n",
    "        if verbose: print(f\"Updating statistics...\")\n",
    "        self.update_stats(n_pos, ibg, verbose=False)\n",
    "\n",
    "        # non-materialized candidate indexes \n",
    "        X = [self.U[index_id] for index_id in self.U if index_id not in self.M]\n",
    "        num_indexes = self.idxCnt - len(self.M)\n",
    "\n",
    "        # determine new set of candidate indexes to monitor for upcoming workload queries\n",
    "        if verbose: print(f\"Choosing top {num_indexes} indexes from {len(X)} non-materialized candidate indexes\")\n",
    "        top_indexes = self.top_indexes(n_pos, X, num_indexes, verbose)\n",
    "        D = self.M | top_indexes\n",
    "        if verbose: print(f\"New set of indexes to monitor for upcoming workload, |D| = {len(D)}\")\n",
    "\n",
    "        # generate new partitions by clustering the new candidate set\n",
    "        if verbose: print(f\"Choosing new partitions...\")\n",
    "        new_partitions, need_to_repartition = self.choose_partition(n_pos, D, verbose)\n",
    "        if verbose:\n",
    "            print(f\"Old partitions:\")\n",
    "            for P in self.stable_partitions:\n",
    "                print(f\"\\t{[index.index_id for index in P]}\")\n",
    "            print(\"New partitions:\")\n",
    "            for P in new_partitions:\n",
    "                print(f\"\\t{[index.index_id for index in P]}\")    \n",
    "\n",
    "        return new_partitions, need_to_repartition, ibg\n",
    "    \n",
    "\n",
    "    # partition the new candidate set into clusters \n",
    "    # (need to optimize this function, currently it is a naive implementation)\n",
    "    def choose_partition(self, N_workload, D, verbose):\n",
    "        \n",
    "        # compute total loss, i.e. sum of doi across indexes from pairs of partitions\n",
    "        def compute_loss(P, current_doi):\n",
    "            loss = 0\n",
    "            for i in range(len(P)):\n",
    "                for j in range(i+1, len(P)):\n",
    "                    for a in P[i]:\n",
    "                        for b in P[j]:\n",
    "                            loss += current_doi[(a.index_id, b.index_id)]\n",
    "            return loss\n",
    "        \n",
    "        # compute current doi values for all pairs of indexes in U\n",
    "        current_doi = defaultdict(int)\n",
    "        for (a_idx, b_idx) in self.intStats.keys():\n",
    "            # take max over incremental averages (optimistic estimate)\n",
    "            current_doi[(a_idx, b_idx)] = 0\n",
    "            doi_total = 0\n",
    "            for (n, doi) in self.intStats[(a_idx, b_idx)]:\n",
    "                doi_total += doi\n",
    "                doi_avg = doi_total / (N_workload-n+1)\n",
    "                current_doi[(a_idx, b_idx)] = max(current_doi[(a_idx, b_idx)], doi_avg)\n",
    "            # save symmetric doi value\n",
    "            current_doi[(b_idx, a_idx)] = current_doi[(a_idx, b_idx)]    \n",
    "\n",
    "        #if verbose:\n",
    "        #    print(\"Current degree of interaction:\")\n",
    "        #    for pair, doi in current_doi.items():\n",
    "        #        print(f\"\\tPair {pair}: {doi}\")     \n",
    "\n",
    "        # from each current stable partition, remove indexes not in D\n",
    "        P = []\n",
    "        for partition in self.stable_partitions:\n",
    "            #P.append([index for index in partition if index.index_id in D])\n",
    "            P.append([index for index in partition])\n",
    "\n",
    "        # add a singleton partition containing each new index in D not in C\n",
    "        for index_id, index in D.items():\n",
    "            if index_id not in self.C:\n",
    "                P.append([index])\n",
    "        \n",
    "        # set the new partition as baseline solution if feasible\n",
    "        total_configurations = sum([2**len(partition) for partition in P])\n",
    "        if total_configurations <= self.stateCnt:\n",
    "            bestSolution = P\n",
    "            bestLoss = compute_loss(P, current_doi)\n",
    "        else:\n",
    "            bestSolution = None\n",
    "            bestLoss = float('inf')    \n",
    "\n",
    "        # perform randomized clustering to find better solution\n",
    "        for i in range(self.rand_cnt):\n",
    "            # create partition of D in singletons\n",
    "            P = [[index] for index in D.values()]\n",
    "            partition2id = {tuple(partition):i for i, partition in enumerate(P)}\n",
    "            loss_cache = {}\n",
    "            \n",
    "            #if verbose:\n",
    "            #    print(f\"Parition to id map: {partition2id}\")\n",
    "\n",
    "            # first merge all singletons, then merge pairs of partitions (randomized merge)\n",
    "            # stopping condition: no feasible merge pairs left (i.e. any merge would exceed stateCnt)\n",
    "            while True:\n",
    "                # find all feasible merge candidates pairs (i.e. pairs with loss > 0 and 2^(|Pi|+|Pj|) <= stateCnt)\n",
    "                E = []\n",
    "                E1 = []\n",
    "\n",
    "                # get loss for all pairs of partitions\n",
    "                total_configurations = sum([2**len(partition) for partition in P])\n",
    "                for i in range(len(P)):\n",
    "                    for j in range(i+1, len(P)):\n",
    "                        Pi_id = partition2id[tuple(P[i])]\n",
    "                        Pj_id = partition2id[tuple(P[j])]\n",
    "                        if (Pi_id, Pj_id) in loss_cache:\n",
    "                            loss = loss_cache[(Pi_id, Pj_id)]\n",
    "                        else:\n",
    "                            loss = compute_loss([P[i], P[j]], current_doi)\n",
    "                            loss_cache[(Pi_id, Pj_id)] = loss\n",
    "\n",
    "                        # only include feasible merge pairs, i.e. a pair which can be merged without the total number of configs exceeding stateCnt\n",
    "                        total_configrations_after_merge = total_configurations - 2**len(P[i]) - 2**len(P[j]) + 2**(len(P[i]) + len(P[j]))\n",
    "                        if loss > 0 and total_configrations_after_merge <= self.stateCnt:\n",
    "                            E.append((P[i], P[j], loss))    \n",
    "                            if len(P[i]) == 1 and len(P[j]) == 1:\n",
    "                                E1.append((P[i],P[j], loss))\n",
    "\n",
    "                #if verbose:    \n",
    "                    #print(f\"E pairs: {[[(index.index_id for index in Pi), (index.index_id for index in Pj), loss] for (Pi, Pj, loss) in E]}\")\n",
    "                    #print(f\"E1 pairs: {[[(index.index_id for index in Pi), (index.index_id for index in Pj), loss] for (Pi, Pj, loss) in E1]}\")\n",
    "\n",
    "                if len(E) == 0:\n",
    "                    break\n",
    "                \n",
    "                elif len(E1) > 0:\n",
    "                    # merge a random pair of singletons, sample randomly from E1 weighted by loss (i.e. high loss pairs more likely to be merged)\n",
    "                    Pi, Pj, loss = random.choices(E1, weights=[loss for (Pi, Pj, loss) in E1], k=1)[0]\n",
    "                    Pij_merged = Pi + Pj\n",
    "                    P.remove(Pi)\n",
    "                    P.remove(Pj)\n",
    "                    P.append(Pij_merged) \n",
    "                    E1.remove((Pi, Pj, loss))  \n",
    "                    partition2id[tuple(Pij_merged)] = len(partition2id) \n",
    "                    #if verbose: \n",
    "                    #    print(f\"Merged singleton partitions {[index.index_id for index in Pi]} and {[index.index_id for index in Pj]} with loss {loss}\")\n",
    "\n",
    "                else:\n",
    "                    # merge a random pair of partitions, sample randomly from E weighted by normalized loss  \n",
    "                    Pi, Pj, loss = random.choices(E, weights=[loss / (2**(len(Pi) + len(Pj)) - 2**len(Pi) - 2**len(Pj)) for (Pi, Pj, loss) in E], k=1)[0]\n",
    "                    Pij_merged = Pi + Pj\n",
    "                    P.remove(Pi)\n",
    "                    P.remove(Pj)\n",
    "                    P.append(Pij_merged)   \n",
    "                    E.remove((Pi, Pj, loss)) \n",
    "                    partition2id[tuple(Pij_merged)] = len(partition2id) \n",
    "                    #if verbose:\n",
    "                    #    print(f\"Merged partitions {[index.index_id for index in Pi]} and {[index.index_id for index in Pj]} with loss {loss}\")    \n",
    "\n",
    "            # check if the new solution is better than the current best solution\n",
    "            loss = compute_loss(P, current_doi)\n",
    "            if loss < bestLoss:\n",
    "                bestSolution = P\n",
    "                bestLoss = loss\n",
    "\n",
    "        # check if old partitions are different from new partitions\n",
    "        need_to_repartition = False\n",
    "        if bestSolution != self.stable_partitions:\n",
    "            need_to_repartition = True\n",
    "\n",
    "        return bestSolution, need_to_repartition\n",
    "\n",
    "\n",
    "    # update candidate index statistics\n",
    "    def update_stats(self, n, ibg, verbose):\n",
    "        # update index benefit statistics\n",
    "        if verbose: print(\"Updating index benefit statistics...\")\n",
    "        for index in self.U.values():\n",
    "            max_benefit = ibg.compute_max_benefit(index)\n",
    "            #if verbose: print(f\"\\tibg max benefit for index {index.index_id}: {max_benefit}\")\n",
    "            self.idxStats[index.index_id].append((n, max_benefit))\n",
    "            #if verbose: print(f\"\\tIndex {index.index_id}: {self.idxStats[index.index_id]}\")\n",
    "            # evict old stats if the size exceeds histSize\n",
    "            self.idxStats[index.index_id] = self.idxStats[index.index_id][-self.histSize:]\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Index benefit statistics:\")\n",
    "            for index_id, stats in self.idxStats.items():\n",
    "                print(f\"\\tIndex {index_id}: {stats}\")\n",
    "\n",
    "\n",
    "        # update index interaction statistics\n",
    "        if verbose: print(\"Updating index interaction statistics...\")\n",
    "        for (a_idx, b_idx) in ibg.doi.keys():\n",
    "            d = ibg.doi[(a_idx, b_idx)]\n",
    "            #if verbose: print(f\"\\tibg doi for pair ({a_idx}, {b_idx}) : {d}\")\n",
    "            if d > 0:\n",
    "                self.intStats[(a_idx, b_idx)].append((n, d))\n",
    "            #if verbose: print(f\"\\tPair ({a_idx}, {b_idx}): {self.intStats[(a_idx, b_idx)]}\")\n",
    "            # evict old stats if the size exceeds histSize\n",
    "            self.intStats[(a_idx, b_idx)] = self.intStats[(a_idx, b_idx)][-self.histSize:]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Index interaction statistics:\")\n",
    "            for pair, stats in self.intStats.items():\n",
    "                print(f\"\\tPair {pair}: {stats}\")\n",
    "\n",
    "\n",
    "    # choose top num_indexes indexes from X with highest potential benefit\n",
    "    def top_indexes(self, N_workload, X, num_indexes, verbose, positive_scores_only=False):\n",
    "        #if verbose:\n",
    "        #    print(f\"Non-materialized candidate indexes, X = {[index.index_id for index in X]}\")\n",
    "\n",
    "        # compute \"current benefit\" of each index in X (these are derived from statistics of observed benefits from recent queries)\n",
    "        score = {}\n",
    "        for index in X:\n",
    "            if len(self.idxStats[index.index_id]) == 0:\n",
    "                # zero current benefit if no statistics are available\n",
    "                current_benefit = 0\n",
    "            else:\n",
    "                # take the maximum over all incremental average benefits (optimistic estimate)\n",
    "                current_benefit = 0\n",
    "                b_total = 0\n",
    "                for (n, b) in self.idxStats[index.index_id]:\n",
    "                    b_total += b \n",
    "                    # incremental average benefit of index up to query n (higher weight/smaller denominator for more recent queries)\n",
    "                    benefit = b_total / (N_workload - n + 1)\n",
    "                    current_benefit = max(current_benefit, benefit)\n",
    "\n",
    "            # use current benefit to compute a score for the index\n",
    "            if index.index_id in self.C:\n",
    "                # if index already being monitored, then score is just current benefit\n",
    "                score[index.index_id] = current_benefit\n",
    "            else:\n",
    "                # if index not being monitored, then score is current benefit minus cost of creating the index\n",
    "                # (unmonitored indexes are penalized so that they are only chosen if they have high potential benefit, which helps keep C stable)\n",
    "                score[index.index_id] = current_benefit - self.get_index_creation_cost(index)\n",
    "\n",
    "        #if verbose:\n",
    "        #    print(\"Index scores:\")\n",
    "        #    for index_id, s in score.items():\n",
    "        #        print(f\"Index {index_id}: {s}\")\n",
    "\n",
    "        # get the top num_indexes indexes with highest scores (keep non-zero scores only)\n",
    "        if positive_scores_only:\n",
    "            top_indexes = [index_id for index_id, s in score.items() if s > 0]\n",
    "        else:\n",
    "            top_indexes = [index_id for index_id, s in score.items()]    \n",
    "        top_indexes = sorted(top_indexes, key=lambda x: score[x], reverse=True)[:num_indexes]\n",
    "        top_indexes = {index_id: self.U[index_id] for index_id in top_indexes}\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{len(top_indexes)} top indexes: {[index.index_id for index in top_indexes.values()]}\")\n",
    "\n",
    "        return top_indexes    \n",
    "\n",
    "\n",
    "    # return index creation cost (using estimated index size as proxy for cost)\n",
    "    def get_index_creation_cost(self, index):\n",
    "        # return estimated size of index\n",
    "        return index.size * 2048  #* 1024 * 1024\n",
    "\n",
    "\n",
    "    # compute transition cost between two MTS states/configurations\n",
    "    def compute_transition_cost(self, S_old, S_new):\n",
    "        # find out which indexes are added\n",
    "        added_indexes = set(S_new) - set(S_old)\n",
    "        \n",
    "        # compute cost of creating the added indexes\n",
    "        transition_cost = sum([self.get_index_creation_cost(index) for index in added_indexes])\n",
    "        #print(f\"\\t\\t\\tComputing transition cost for state: {tuple([index.index_id for index in S_old])} --> {tuple([index.index_id for index in S_new])} = {transition_cost}\")\n",
    "        return transition_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test WFIT implementation on sample SSB workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# generate an SSB workload\\nworkload = [qg.generate_query(i) for i in ([1,2,3]*10)]\\n\\nprint(len(workload))'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# generate an SSB workload\n",
    "workload = [qg.generate_query(i) for i in ([1,2,3]*10)]\n",
    "\n",
    "print(len(workload))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n",
      "Initializing WFA instances for 1 stable partitions...\n",
      "WFA instance #0: {(): 0}\n",
      "Initial set of materialized indexes: []\n",
      "Stable partitions: [[]]\n",
      "Initial work function instances: \n",
      "\tWFA Instance #0: {(): 0}\n",
      "\n",
      "Maximum number of candidate indexes tracked: 25\n",
      "Maximum number of MTS states/configurations: 500\n",
      "Maximum number of historical index statistics kept: 100\n",
      "Number of randomized clustering iterations: 50\n",
      "*** Dropping all materialized indexes...\n",
      "##################################################################\n",
      "\n",
      "Processing query 1\n",
      "-----------------------------------\n",
      "Generating new partitions for query #1\n",
      "Number of candidate indexes: 19\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18\n",
      "Constructing IBG...\n",
      "Number of levels in IBG: 16\n",
      "Number of nodes in IBG: 69, Total number of what-if calls: 69, Time spent on what-if calls: 0.13813281059265137\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 5/5 [00:00<00:00, 54189.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent on computing all pair degree of interaction: 0.11085104942321777\n",
      "Repartitioning...\n",
      "Reinitializing WFA instances...\n",
      "Analyzing query...\n",
      "New indexes added this round: []\n",
      "Old indexes removed this round: []\n",
      "Currently materialized indexes: []\n",
      "PostgreSQL restarted successfully.\n",
      "Executing query...\n",
      "Indexes accessed --> ['pk_dwdate']\n",
      "*** WFIT recommendation: []\n",
      "*** Simple recommendation: ['ix_dwdate_d_year_d_datekey', 'ix_lineorder_lo_quantity']\n",
      "*** Hypothetical Speedup --> WFIT: 1.0, Simple: 116732.34400656815\n",
      "*** Hypothetical Total cost --> WFIT: 1421799.95, Simple: 2721148.18, WFIT/Simple: 0.5225000095364156\n",
      "*** Hypothetical Total Cost No-Index --> 1421799.95\n",
      "Total recommendation time taken for query #1: 0.7386107444763184 seconds\n",
      "Actual execution time for query #1 --> 4.805511 seconds\n",
      "\n",
      "Total recommendation time so far --> 0.7386107444763184 seconds\n",
      "Total materialization time so far --> 0.0015630722045898438 seconds\n",
      "Total execution time so far --> 4.805511 seconds\n",
      "Total time so far --> 5.545684816680908 seconds\n",
      "(Partitioning: 0.305631160736084 seconds, Repartitioning: 0.0006537437438964844 seconds, Analyzing: 0.43232250213623047 seconds), Materializing config: 0.0015630722045898438 seconds, Executing query: 4.805511 seconds\n",
      "\n",
      "\n",
      "\n",
      "Processing query 2\n",
      "-----------------------------------\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Generating new partitions for query #2\n",
      "Number of candidate indexes: 22\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of levels in IBG: 3\n",
      "Number of nodes in IBG: 3, Total number of what-if calls: 3, Time spent on what-if calls: 0.008863687515258789\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 3/3 [00:00<00:00, 56679.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent on computing all pair degree of interaction: 0.008378028869628906\n",
      "Repartitioning...\n",
      "Reinitializing WFA instances...\n",
      "Analyzing query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New indexes added this round: []\n",
      "Old indexes removed this round: []\n",
      "Currently materialized indexes: []\n",
      "PostgreSQL restarted successfully.\n",
      "Executing query...\n",
      "Indexes accessed --> []\n",
      "*** WFIT recommendation: []\n",
      "*** Simple recommendation: ['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "*** Hypothetical Speedup --> WFIT: 1.0, Simple: 68.71620446182237\n",
      "*** Hypothetical Total cost --> WFIT: 2902260.34, Simple: 6143924.74, WFIT/Simple: 0.47237888854738797\n",
      "*** Hypothetical Total Cost No-Index --> 2902260.34\n",
      "Total recommendation time taken for query #2: 0.46253275871276855 seconds\n",
      "Actual execution time for query #2 --> 3.625765 seconds\n",
      "\n",
      "Total recommendation time so far --> 1.201143503189087 seconds\n",
      "Total materialization time so far --> 0.013433694839477539 seconds\n",
      "Total execution time so far --> 8.431276 seconds\n",
      "Total time so far --> 9.645853198028565 seconds\n",
      "(Partitioning: 0.07355332374572754 seconds, Repartitioning: 0.0012383460998535156 seconds, Analyzing: 0.3877370357513428 seconds), Materializing config: 0.011870622634887695 seconds, Executing query: 3.625765 seconds\n",
      "\n",
      "\n",
      "\n",
      "Processing query 3\n",
      "-----------------------------------\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Generating new partitions for query #3\n",
      "Number of candidate indexes: 66\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42_43_44_45_46_47_48_49_50_51_52_53_54_55_56_57_58_59_60_61_62_63_64_65\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of levels in IBG: 14\n",
      "Number of nodes in IBG: 144, Total number of what-if calls: 144, Time spent on what-if calls: 1.7454559803009033\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 4/4 [00:05<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent on computing all pair degree of interaction: 5.479281902313232\n",
      "Repartitioning...\n",
      "Reinitializing WFA instances...\n",
      "Analyzing query...\n",
      "New indexes added this round: []\n",
      "Old indexes removed this round: []\n",
      "Currently materialized indexes: []\n",
      "PostgreSQL restarted successfully.\n",
      "Executing query...\n",
      "Indexes accessed --> []\n",
      "*** WFIT recommendation: []\n",
      "*** Simple recommendation: ['ix_customer_c_nation_c_city_c_custkey', 'ix_dwdate_d_year_d_datekey', 'ix_supplier_s_nation_s_city_s_suppkey']\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "*** Hypothetical Speedup --> WFIT: 1.0, Simple: 1.005024658822846\n",
      "*** Hypothetical Total cost --> WFIT: 4216013.77, Simple: 7490246.01, WFIT/Simple: 0.5628671961336554\n",
      "*** Hypothetical Total Cost No-Index --> 4216013.77\n",
      "Total recommendation time taken for query #3: 9.280156373977661 seconds\n",
      "Actual execution time for query #3 --> 4.3535569999999995 seconds\n",
      "\n",
      "Total recommendation time so far --> 10.481299877166748 seconds\n",
      "Total materialization time so far --> 0.015300273895263672 seconds\n",
      "Total execution time so far --> 12.784832999999999 seconds\n",
      "Total time so far --> 23.28143315106201 seconds\n",
      "(Partitioning: 7.3433427810668945 seconds, Repartitioning: 0.001401662826538086 seconds, Analyzing: 1.9354066848754883 seconds), Materializing config: 0.0018665790557861328 seconds, Executing query: 4.3535569999999995 seconds\n",
      "\n",
      "\n",
      "\n",
      "Processing query 4\n",
      "-----------------------------------\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Generating new partitions for query #4\n",
      "Number of candidate indexes: 66\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42_43_44_45_46_47_48_49_50_51_52_53_54_55_56_57_58_59_60_61_62_63_64_65\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of levels in IBG: 4\n",
      "Number of nodes in IBG: 4, Total number of what-if calls: 4, Time spent on what-if calls: 0.03455471992492676\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 4/4 [00:00<00:00, 66841.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent on computing all pair degree of interaction: 0.13768863677978516\n",
      "Analyzing query...\n",
      "Successfully created index: 'ix_dwdate_d_year', size: 0.03906250000000000000 MB, creation time: 19.57 ms\n",
      "New indexes added this round: ['ix_dwdate_d_year']\n",
      "Old indexes removed this round: []\n",
      "Currently materialized indexes: ['ix_dwdate_d_year']\n",
      "PostgreSQL restarted successfully.\n",
      "Executing query...\n",
      "Indexes accessed --> ['ix_dwdate_d_year']\n",
      "*** WFIT recommendation: ['ix_dwdate_d_year']\n",
      "*** Simple recommendation: ['ix_dwdate_d_year_d_datekey']\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "*** Hypothetical Speedup --> WFIT: 1.0000381544127492, Simple: 1.0000451542541697\n",
      "*** Hypothetical Total cost --> WFIT: 5644762.069999999, Simple: 8919048.31, WFIT/Simple: 0.6328883838056035\n",
      "*** Hypothetical Total Cost No-Index --> 5644736.58\n",
      "Total recommendation time taken for query #4: 0.858919620513916 seconds\n",
      "Actual execution time for query #4 --> 4.895689 seconds\n",
      "\n",
      "Total recommendation time so far --> 11.340219497680664 seconds\n",
      "Total materialization time so far --> 0.03679203987121582 seconds\n",
      "Total execution time so far --> 17.680522 seconds\n",
      "Total time so far --> 29.05753353755188 seconds\n",
      "(Partitioning: 0.24954795837402344 seconds, Repartitioning: 0.0 seconds, Analyzing: 0.6093566417694092 seconds), Materializing config: 0.02149176597595215 seconds, Executing query: 4.895689 seconds\n",
      "\n",
      "\n",
      "\n",
      "Processing query 5\n",
      "-----------------------------------\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Generating new partitions for query #5\n",
      "Number of candidate indexes: 66\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42_43_44_45_46_47_48_49_50_51_52_53_54_55_56_57_58_59_60_61_62_63_64_65\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of levels in IBG: 3\n",
      "Number of nodes in IBG: 3, Total number of what-if calls: 3, Time spent on what-if calls: 0.02874135971069336\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 3/3 [00:00<00:00, 42945.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent on computing all pair degree of interaction: 0.20082449913024902\n",
      "Analyzing query...\n",
      "New indexes added this round: []\n",
      "Old indexes removed this round: []\n",
      "Currently materialized indexes: ['ix_dwdate_d_year']\n",
      "PostgreSQL restarted successfully.\n",
      "Executing query...\n",
      "Indexes accessed --> []\n",
      "*** WFIT recommendation: ['ix_dwdate_d_year']\n",
      "*** Simple recommendation: ['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "*** Hypothetical Speedup --> WFIT: 1.0, Simple: 71.11984927560215\n",
      "*** Hypothetical Total cost --> WFIT: 7123173.76, Simple: 12341067.92, WFIT/Simple: 0.5771926551393617\n",
      "*** Hypothetical Total Cost No-Index --> 7123148.27\n",
      "Total recommendation time taken for query #5: 0.8361806869506836 seconds\n",
      "Actual execution time for query #5 --> 3.73023 seconds\n",
      "\n",
      "Total recommendation time so far --> 12.176400184631348 seconds\n",
      "Total materialization time so far --> 0.03849148750305176 seconds\n",
      "Total execution time so far --> 21.410752 seconds\n",
      "Total time so far --> 33.6256436721344 seconds\n",
      "(Partitioning: 0.30791282653808594 seconds, Repartitioning: 0.0 seconds, Analyzing: 0.5282540321350098 seconds), Materializing config: 0.0016994476318359375 seconds, Executing query: 3.73023 seconds\n",
      "\n",
      "\n",
      "\n",
      "Processing query 6\n",
      "-----------------------------------\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Generating new partitions for query #6\n",
      "Number of candidate indexes: 66\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42_43_44_45_46_47_48_49_50_51_52_53_54_55_56_57_58_59_60_61_62_63_64_65\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of levels in IBG: 13\n",
      "Number of nodes in IBG: 108, Total number of what-if calls: 108, Time spent on what-if calls: 1.1786019802093506\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 4/4 [00:05<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent on computing all pair degree of interaction: 5.492448091506958\n",
      "Analyzing query...\n",
      "New indexes added this round: []\n",
      "Old indexes removed this round: []\n",
      "Currently materialized indexes: ['ix_dwdate_d_year']\n",
      "PostgreSQL restarted successfully.\n",
      "Executing query...\n",
      "Indexes accessed --> []\n",
      "*** WFIT recommendation: ['ix_dwdate_d_year']\n",
      "*** Simple recommendation: ['ix_customer_c_nation_c_city_c_custkey', 'ix_dwdate_d_year_d_datekey', 'ix_supplier_s_nation_s_city_s_suppkey']\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "*** Hypothetical Speedup --> WFIT: 1.0, Simple: 1.0049706128379465\n",
      "*** Hypothetical Total cost --> WFIT: 8449666.44, Simple: 13700135.73, WFIT/Simple: 0.6167578633178986\n",
      "*** Hypothetical Total Cost No-Index --> 8449640.95\n",
      "Total recommendation time taken for query #6: 8.57309341430664 seconds\n",
      "Actual execution time for query #6 --> 4.458508 seconds\n",
      "\n",
      "Total recommendation time so far --> 20.74949359893799 seconds\n",
      "Total materialization time so far --> 0.04007315635681152 seconds\n",
      "Total execution time so far --> 25.869259999999997 seconds\n",
      "Total time so far --> 46.658826755294804 seconds\n",
      "(Partitioning: 6.766572952270508 seconds, Repartitioning: 0.0 seconds, Analyzing: 1.8065078258514404 seconds), Materializing config: 0.0015816688537597656 seconds, Executing query: 4.458508 seconds\n",
      "\n",
      "\n",
      "\n",
      "Processing query 7\n",
      "-----------------------------------\n",
      "Generating new partitions for query #7\n",
      "Number of candidate indexes: 66\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42_43_44_45_46_47_48_49_50_51_52_53_54_55_56_57_58_59_60_61_62_63_64_65\n",
      "Constructing IBG...\n",
      "Number of levels in IBG: 3\n",
      "Number of nodes in IBG: 3, Total number of what-if calls: 3, Time spent on what-if calls: 0.026616334915161133\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 3/3 [00:00<00:00, 48395.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent on computing all pair degree of interaction: 0.10064125061035156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing query...\n",
      "New indexes added this round: []\n",
      "Old indexes removed this round: []\n",
      "Currently materialized indexes: ['ix_dwdate_d_year']\n",
      "PostgreSQL restarted successfully.\n",
      "Executing query...\n",
      "Indexes accessed --> ['ix_dwdate_d_year']\n",
      "*** WFIT recommendation: ['ix_dwdate_d_year']\n",
      "*** Simple recommendation: ['ix_dwdate_d_year_d_datekey']\n",
      "*** Hypothetical Speedup --> WFIT: 1.0, Simple: 1.0000042077327356\n",
      "*** Hypothetical Total cost --> WFIT: 9875618.52, Simple: 15126225.81, WFIT/Simple: 0.6528805429753134\n",
      "*** Hypothetical Total Cost No-Index --> 9875593.28\n",
      "Total recommendation time taken for query #7: 0.728205680847168 seconds\n",
      "Actual execution time for query #7 --> 4.930872 seconds\n",
      "\n",
      "Total recommendation time so far --> 21.477699279785156 seconds\n",
      "Total materialization time so far --> 0.04175686836242676 seconds\n",
      "Total execution time so far --> 30.800131999999998 seconds\n",
      "Total time so far --> 52.31958814814759 seconds\n",
      "(Partitioning: 0.20600414276123047 seconds, Repartitioning: 2.384185791015625e-07 seconds, Analyzing: 0.5221366882324219 seconds), Materializing config: 0.0016837120056152344 seconds, Executing query: 4.930872 seconds\n",
      "\n",
      "\n",
      "\n",
      "Processing query 8\n",
      "-----------------------------------\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Generating new partitions for query #8\n",
      "Number of candidate indexes: 66\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42_43_44_45_46_47_48_49_50_51_52_53_54_55_56_57_58_59_60_61_62_63_64_65\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of levels in IBG: 3\n",
      "Number of nodes in IBG: 3, Total number of what-if calls: 3, Time spent on what-if calls: 0.026139497756958008\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 3/3 [00:00<00:00, 48960.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent on computing all pair degree of interaction: 0.10056805610656738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repartitioning...\n",
      "Reinitializing WFA instances...\n",
      "Analyzing query...\n",
      "Successfully created index: 'ix_lineorder_lo_quantity_lo_linenumber', size: 396.6093750000000000 MB, creation time: 25307.02 ms\n",
      "New indexes added this round: ['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "Old indexes removed this round: []\n",
      "Currently materialized indexes: ['ix_dwdate_d_year', 'ix_lineorder_lo_quantity_lo_linenumber']\n",
      "PostgreSQL restarted successfully.\n",
      "Executing query...\n",
      "Indexes accessed --> ['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "*** WFIT recommendation: ['ix_dwdate_d_year', 'ix_lineorder_lo_quantity_lo_linenumber']\n",
      "*** Simple recommendation: ['ix_lineorder_lo_quantity_lo_linenumber']\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "*** Hypothetical Speedup --> WFIT: 71.56954385067831, Simple: 71.56954385067831\n",
      "*** Hypothetical Total cost --> WFIT: 10708526.309999999, Simple: 15959133.600000001, WFIT/Simple: 0.6709967206490456\n",
      "*** Hypothetical Total Cost No-Index --> 11353632.469999999\n",
      "Total recommendation time taken for query #8: 0.7159004211425781 seconds\n",
      "Actual execution time for query #8 --> 0.033269 seconds\n",
      "\n",
      "Total recommendation time so far --> 22.193599700927734 seconds\n",
      "Total materialization time so far --> 25.365576028823853 seconds\n",
      "Total execution time so far --> 30.833401 seconds\n",
      "Total time so far --> 78.3925767297516 seconds\n",
      "(Partitioning: 0.2059779167175293 seconds, Repartitioning: 0.0018875598907470703 seconds, Analyzing: 0.5080230236053467 seconds), Materializing config: 25.323819160461426 seconds, Executing query: 0.033269 seconds\n",
      "\n",
      "\n",
      "\n",
      "Processing query 9\n",
      "-----------------------------------\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Generating new partitions for query #9\n",
      "Number of candidate indexes: 66\n",
      "Getting hypothetical sizes of candidate indexes...\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42_43_44_45_46_47_48_49_50_51_52_53_54_55_56_57_58_59_60_61_62_63_64_65\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of levels in IBG: 11\n",
      "Number of nodes in IBG: 36, Total number of what-if calls: 36, Time spent on what-if calls: 0.37075090408325195\n",
      "Computing all pair degree of interaction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing nodes in parallel: 100%|██████████| 4/4 [00:01<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent on computing all pair degree of interaction: 1.605438232421875\n",
      "Analyzing query...\n",
      "New indexes added this round: []\n",
      "Old indexes removed this round: []\n",
      "Currently materialized indexes: ['ix_dwdate_d_year', 'ix_lineorder_lo_quantity_lo_linenumber']\n",
      "PostgreSQL restarted successfully.\n",
      "Executing query...\n",
      "Indexes accessed --> []\n",
      "*** WFIT recommendation: ['ix_dwdate_d_year', 'ix_lineorder_lo_quantity_lo_linenumber']\n",
      "*** Simple recommendation: ['ix_customer_c_nation_c_city_c_custkey', 'ix_supplier_s_nation_s_city_s_suppkey']\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "*** Hypothetical Speedup --> WFIT: 1.0, Simple: 1.0049314187050657\n",
      "*** Hypothetical Total cost --> WFIT: 12041723.339999998, Simple: 17324780.34, WFIT/Simple: 0.6950577787239061\n",
      "*** Hypothetical Total Cost No-Index --> 12686829.499999998\n",
      "Total recommendation time taken for query #9: 3.4697797298431396 seconds\n",
      "Actual execution time for query #9 --> 4.436243999999999 seconds\n",
      "\n",
      "Total recommendation time so far --> 25.663379430770874 seconds\n",
      "Total materialization time so far --> 25.367416620254517 seconds\n",
      "Total execution time so far --> 35.269645 seconds\n",
      "Total time so far --> 86.3004410510254 seconds\n",
      "(Partitioning: 2.065647602081299 seconds, Repartitioning: 2.384185791015625e-07 seconds, Analyzing: 1.404115915298462 seconds), Materializing config: 0.0018405914306640625 seconds, Executing query: 4.436243999999999 seconds\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instantiate WFIT\n",
    "#C = extract_query_indexes(qg.generate_query(8), include_cols=False)  \n",
    "S_0 = []#C[0:1]\n",
    "#wfit = WFIT(S_0, idxCnt=20, stateCnt=1000, histSize=100, rand_cnt=10)\n",
    "wfit = WFIT(S_0, max_key_columns=3, include_cols=False, max_U=None, idxCnt=25, stateCnt=500, rand_cnt=50)\n",
    "\n",
    "# process the workload\n",
    "for i, query in enumerate(workload[:9]):\n",
    "    print(f\"Processing query {i+1}\")\n",
    "    print('-----------------------------------')\n",
    "    wfit.process_WFIT(query, remove_stale_U=False, remove_stale_freq=1, execute=True, materialize=True, verbose=True)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_workload_noIndex(workload, drop_indexes=False):\n",
    "    if drop_indexes:\n",
    "        print(f\"*** Dropping all existing indexes...\")\n",
    "        # drop all existing indexes\n",
    "        conn = create_connection()\n",
    "        drop_all_indexes(conn)\n",
    "        close_connection(conn)\n",
    "\n",
    "    print(f\"Executing workload without any indexes...\")\n",
    "    # execute workload without any indexes\n",
    "    total_time = 0\n",
    "    for i, query_object in enumerate(workload):\n",
    "        # restart the server before each query execution\n",
    "        restart_postgresql()\n",
    "        conn = create_connection()\n",
    "        execution_time, rows = execute_query(conn, query_object.query_string, with_explain=True, print_results=False)\n",
    "        close_connection(conn)\n",
    "        execution_time /= 1000\n",
    "        total_time += execution_time\n",
    "        print(f\"\\tExecution time for query# {i+1}: {execution_time} seconds\")\n",
    "\n",
    "    print(f\"Total execution time for workload without any indexes: {total_time} seconds\")\n",
    "    \n",
    "    return total_time    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Dropping all existing indexes...\n",
      "Index 'ix_dwdate_d_year_d_datekey' on table 'dwdate' dropped successfully\n",
      "Index 'ix_lineorder_lo_quantity' on table 'lineorder' dropped successfully\n",
      "Index 'ix_lineorder_lo_quantity_lo_linenumber' on table 'lineorder' dropped successfully\n",
      "Executing workload without any indexes...\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 1: 4.793112 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 2: 3.65022 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 3: 4.453622999999999 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 4: 4.930203000000001 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 5: 3.7944989999999996 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 6: 4.5476030000000005 seconds\n",
      "Total execution time for workload without any indexes: 26.169259999999994 seconds\n"
     ]
    }
   ],
   "source": [
    "# execute workload without any indexes (mini batch test)\n",
    "total_time_noIndex = execute_workload_noIndex(workload[:6], drop_indexes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Dropping all existing indexes...\n",
      "Index 'ix_lineorder_lo_quantity_lo_linenumber' on table 'lineorder' dropped successfully\n",
      "Index 'ix_supplier_s_nation' on table 'supplier' dropped successfully\n",
      "Index 'ix_dwdate_d_year_d_datekey' on table 'dwdate' dropped successfully\n",
      "Index 'ix_supplier_s_nation_s_suppkey_s_city' on table 'supplier' dropped successfully\n",
      "Index 'ix_customer_c_nation_c_custkey_c_city' on table 'customer' dropped successfully\n",
      "Executing workload without any indexes...\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 1: 5.107667999999999 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 2: 3.7785369999999996 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 3: 4.459029 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 4: 4.907470999999999 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 5: 3.844087 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 6: 4.508465 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 7: 5.1566019999999995 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 8: 3.739804 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 9: 4.510882 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 10: 5.061230999999999 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 11: 3.8353200000000003 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 12: 4.449461 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 13: 5.147055999999999 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 14: 3.685326 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 15: 4.381034 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 16: 5.265372 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 17: 3.729861 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 18: 4.485796000000001 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 19: 5.260127000000001 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 20: 3.800541 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 21: 4.456134 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 22: 4.888649 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 23: 3.9167829999999997 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 24: 4.431011000000001 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 25: 5.0085820000000005 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 26: 3.8760790000000003 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 27: 4.586906 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 28: 5.224158 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 29: 3.7954369999999997 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 30: 4.412444000000001 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 31: 5.053777 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 32: 3.732145 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 33: 4.407445 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 34: 4.961923 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 35: 3.677667 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 36: 4.359980999999999 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 37: 5.0337060000000005 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 38: 3.6724409999999996 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 39: 4.394501 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 40: 5.511591 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 41: 3.731891 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 42: 4.483306 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 43: 5.715486 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 44: 3.789736 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 45: 4.435893 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 46: 5.649249 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 47: 3.725534 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 48: 4.399172999999999 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 49: 5.164983 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 50: 3.748465 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 51: 4.504722 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 52: 5.01207 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 53: 3.703 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 54: 4.325061 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 55: 5.172109000000001 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 56: 3.779503 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 57: 4.397385 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 58: 5.179024 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 59: 3.7334169999999998 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 60: 4.369356 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 61: 5.041168 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 62: 3.818813 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 63: 4.4623919999999995 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 64: 5.036048 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 65: 3.869935 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 66: 4.5069740000000005 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 67: 5.441274999999999 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 68: 3.863603 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 69: 4.518133 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 70: 5.433807 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 71: 3.817285 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 72: 4.477308 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 73: 5.129211000000001 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 74: 3.7097849999999997 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 75: 4.493593 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 76: 4.8670230000000005 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 77: 3.712088 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 78: 4.334149999999999 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 79: 5.125377 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 80: 3.713605 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 81: 4.523191 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 82: 5.247558 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 83: 3.7898 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 84: 4.403458 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 85: 5.207205 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 86: 3.8293809999999997 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 87: 4.410875 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 88: 5.3901959999999995 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 89: 3.730377 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 90: 4.494826 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 91: 5.133414 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 92: 3.7573359999999996 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 93: 4.361878 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 94: 5.267437 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 95: 3.69509 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 96: 4.287470999999999 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 97: 5.080113 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 98: 3.774899 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 99: 4.40124 seconds\n",
      "PostgreSQL restarted successfully.\n",
      "\tExecution time for query# 100: 5.090438 seconds\n",
      "Total execution time for workload without any indexes: 446.78214899999995 seconds\n"
     ]
    }
   ],
   "source": [
    "# execute workload without any indexes\n",
    "total_time_noIndex = execute_workload_noIndex(workload[:100], drop_indexes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
