{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WFIT Algorithm Implementation (Schnaitter 2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "import IPython\n",
    "notebook_path = IPython.get_ipython().starting_dir\n",
    "target_subdirectory_path = os.path.abspath(os.path.join(os.path.dirname(notebook_path), 'PostgreSQL'))\n",
    "sys.path.append(target_subdirectory_path)\n",
    "\n",
    "from pg_utils import *\n",
    "from ssb_qgen_class import *\n",
    "\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "import time\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index Benefit Graph (IBG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, id, indexes):\n",
    "        self.id = id\n",
    "        self.indexes = indexes\n",
    "        self.children = []\n",
    "        self.parents = []\n",
    "        self.built = False\n",
    "        self.cost = None\n",
    "        self.used = None\n",
    "\n",
    "\n",
    "# class for creating and storing the IBG\n",
    "class IBG:\n",
    "    # Class-level cache\n",
    "    _class_cache = {}\n",
    "\n",
    "    def __init__(self, query_object, C):\n",
    "        self.q = query_object\n",
    "        self.C = C\n",
    "        print(f\"Number of candidate indexes: {len(self.C)}\")\n",
    "        #print(f\"Candidate indexes: {self.C}\")\n",
    "        \n",
    "        # map index_id to integer\n",
    "        self.idx2id = {index.index_id:i for i, index in enumerate(self.C)}\n",
    "        self.idx2index = {index.index_id:index for index in self.C}\n",
    "        \n",
    "        # create a hash table for keeping track of all created nodes\n",
    "        self.nodes = {}\n",
    "        # create a root node\n",
    "        self.root = Node(self.get_configuration_id(self.C), self.C)\n",
    "        self.nodes[self.root.id] = self.root\n",
    "        print(f\"Created root node with id: {self.root.id}\")\n",
    "        # start the IBG construction\n",
    "        print(\"Constructing IBG...\")\n",
    "        self.construct_ibg(self.root)\n",
    "        print(f\"Number of nodes in IBG: {len(self.nodes)}\")\n",
    "        # compute all pair degree of interaction\n",
    "        print(f\"Computing all pair degree of interaction...\")\n",
    "        self.doi = self.compute_all_pair_doi()\n",
    "\n",
    "\n",
    "    # assign unique string id to a configuration\n",
    "    def get_configuration_id(self, indexes):\n",
    "        # get sorted list of integer ids\n",
    "        ids = sorted([self.idx2id[idx.index_id] for idx in indexes])\n",
    "        return \"_\".join([str(i) for i in ids])\n",
    "    \n",
    "    def _get_cost_used(self, indexes):\n",
    "        # Convert indexes to a tuple to make it hashable\n",
    "        indexes_tuple = tuple(indexes)\n",
    "        # Check if the result is already in the class-level cache\n",
    "        if indexes_tuple in self._class_cache:\n",
    "            return self._class_cache[indexes_tuple]\n",
    "\n",
    "        conn = create_connection()\n",
    "        # create hypothetical indexes\n",
    "        hypo_indexes = bulk_create_hypothetical_indexes(conn, indexes)\n",
    "        # map oid to index object\n",
    "        oid2index = {}\n",
    "        for i in range(len(hypo_indexes)):\n",
    "            oid2index[hypo_indexes[i]] = indexes[i]\n",
    "        # get cost and used indexes\n",
    "        cost, indexes_used = get_query_cost_estimate_hypo_indexes(conn, self.q.query_string, show_plan=False)\n",
    "        # map used index oids to index objects\n",
    "        used = [oid2index[oid] for oid, scan_type, scan_cost in indexes_used]\n",
    "        # drop hypothetical indexes\n",
    "        bulk_drop_hypothetical_indexes(conn)\n",
    "        close_connection(conn)\n",
    "\n",
    "        # Store the result in the class-level cache\n",
    "        self._class_cache[indexes_tuple] = (cost, used)\n",
    "        return cost, used\n",
    "\n",
    "    # Ensure the indexes parameter is hashable\n",
    "    def _cached_get_cost_used(self, indexes):\n",
    "        return self._get_cost_used(tuple(indexes))\n",
    "\n",
    "    # recursive IBG construction algorithm\n",
    "    def construct_ibg(self, Y):\n",
    "        if Y.built:\n",
    "            return \n",
    "        \n",
    "        # obtain query optimizers cost and used indexes\n",
    "        #print(f\"Creating node for configuration: {[idx.index_id for idx in Y.indexes]}\")\n",
    "        \n",
    "        cost, used = self._cached_get_cost_used(Y.indexes)\n",
    "        Y.cost = cost\n",
    "        Y.used = used\n",
    "        Y.built = True\n",
    "        \n",
    "        #print(f\"Cost: {cost}, Used indexes: {[idx.index_id for idx in used]}\")\n",
    "        #for idx in used:\n",
    "        #    print(f\"{idx}\")\n",
    "\n",
    "        # create children\n",
    "        for a in Y.used:\n",
    "            # create a new configuration with index a removed from Y\n",
    "            X_indexes = [index for index in Y.indexes if index != a]\n",
    "            X_id = self.get_configuration_id(X_indexes)\n",
    "            \n",
    "            # if X is not in the hash table, create a new node and recursively build it\n",
    "            if X_id not in self.nodes:\n",
    "                X = Node(X_id, X_indexes)\n",
    "                X.parents.append(Y)\n",
    "                self.nodes[X_id] = X\n",
    "                Y.children.append(X)\n",
    "                self.construct_ibg(X)\n",
    "\n",
    "            else:\n",
    "                X = self.nodes[X_id]\n",
    "                Y.children.append(X)\n",
    "                X.parents.append(Y)\n",
    "\n",
    "\n",
    "    # use IBG to obtain estimated cost and used indexes for arbitrary subset of C\n",
    "    def get_cost_used(self, X):\n",
    "        # get id of the configuration\n",
    "        id = self.get_configuration_id(X)\n",
    "        # check if the configuration is in the IBG\n",
    "        if id in self.nodes:\n",
    "            cost, used = self.nodes[id].cost, self.nodes[id].used\n",
    "        \n",
    "        # if not in the IBG, traverse the IBG to find a covering node\n",
    "        else:\n",
    "            Y = self.find_covering_node(X)              \n",
    "            cost, used = Y.cost, Y.used\n",
    "\n",
    "        return cost, used    \n",
    "\n",
    "\n",
    "    # traverses the IBG to find a node that removes indexes not in X (i.e. a covering node for X)\n",
    "    def find_covering_node(self, X):\n",
    "        X_indexes = set([index.index_id for index in X])\n",
    "        Y = self.root\n",
    "        Y_indexes = set([index.index_id for index in Y.indexes])\n",
    "        # traverse IBG to find covering node\n",
    "        while (len(Y_indexes - X_indexes) != 0) or (len(Y.children) > 0):               \n",
    "            # traverse down to the child node that removes an index not in X\n",
    "            child_found = False\n",
    "            for child in Y.children:\n",
    "                child_indexes = set([index.index_id for index in child.indexes])\n",
    "                child_indexes_removed = Y_indexes - child_indexes\n",
    "                child_indexes_removed_not_in_X = child_indexes_removed - X_indexes\n",
    "        \n",
    "                # check if child removes an index not in X\n",
    "                if len(child_indexes_removed_not_in_X) > 0:\n",
    "                    Y = child\n",
    "                    Y_indexes = child_indexes\n",
    "                    child_found = True\n",
    "                    break\n",
    "\n",
    "            # if no children remove indexes not in X    \n",
    "            if not child_found:\n",
    "                break    \n",
    "    \n",
    "        return Y        \n",
    "\n",
    "    # compute benefit of an index for a given configuration \n",
    "    # input X is a list of index objects and 'a' is a single index object\n",
    "    # X must not contain 'a'\n",
    "    def compute_benefit(self, a, X):\n",
    "        if a in X:\n",
    "            # zero benefit if 'a' is already in X\n",
    "            #raise ValueError(\"Index 'a' is already in X\")\n",
    "            return 0\n",
    "        \n",
    "        # get cost  for X\n",
    "        cost_X = self.get_cost_used(X)[0]\n",
    "        # create a new configuration with index a added to X\n",
    "        X_a = X + [a]\n",
    "        # get cost for X + {a}\n",
    "        cost_X_a = self.get_cost_used(X_a)[0]\n",
    "        # compute benefit\n",
    "        benefit = cost_X - cost_X_a\n",
    "        return benefit \n",
    "\n",
    "\n",
    "    # compute maximum benefit of adding an index to any possibe configuration\n",
    "    def compute_max_benefit(self, a):\n",
    "        max_benefit = float('-inf')\n",
    "        for id, node in self.nodes.items():\n",
    "            #print(f\"Computing benefit for node: {[index.index_id for index in node.indexes]}\")\n",
    "            benefit = self.compute_benefit(a, node.indexes)\n",
    "            if benefit > max_benefit:\n",
    "                max_benefit = benefit\n",
    "\n",
    "        return max_benefit\n",
    "    \n",
    "    # compute the degree of interaction between two indexes a,b in configuration X \n",
    "    def compute_doi_configuration(self, a, b, X):\n",
    "        # X must not contain a or b\n",
    "        if a in X or b in X:\n",
    "            raise ValueError(\"a or b is already in X\")\n",
    "\n",
    "        doi = abs(self.compute_benefit(a, X) - self.compute_benefit(a, X + [b]))\n",
    "        doi /= self.get_cost_used(X + [a,b])[0]   \n",
    "        return doi\n",
    "   \n",
    "    \n",
    "    # Cache the results of find_covering_node and get_cost_used to avoid redundant calculations\n",
    "    @lru_cache(maxsize=None)\n",
    "    def cached_find_covering_node(self, indexes):\n",
    "        return self.find_covering_node(tuple(indexes))\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def cached_get_cost_used(self, indexes):\n",
    "        return self.get_cost_used(tuple(indexes))\n",
    "\n",
    "\n",
    "    # computes the degree of interaction between all pairs of indexes (a,b) in candidate set C\n",
    "    # Note: doi is symmetric, i.e. doi(a,b) = doi(b,a)\n",
    "    def compute_all_pair_doi(self):\n",
    "        # hash table for storing doi values\n",
    "        doi = {}\n",
    "        # intialize doi values to zero\n",
    "        for i in range(len(self.C)):\n",
    "            for j in range(i+1, len(self.C)):\n",
    "                doi[(self.C[i].index_id, self.C[j].index_id)] = 0\n",
    "                doi[(self.C[j].index_id, self.C[i].index_id)] = 0\n",
    "\n",
    "        S_idxs = set([index.index_id for index in self.C])\n",
    "\n",
    "        # iterate over each IBG node\n",
    "        for Y in self.nodes.values():\n",
    "            # remove Y.used from S\n",
    "            Y_idxs = set([index.index_id for index in Y.indexes])\n",
    "            S_Y = list(S_idxs - Y_idxs)\n",
    "            # iterate over all pairs of indexes in S_Y\n",
    "            for i in range(len(S_Y)):\n",
    "                for j in range(i+1, len(S_Y)):\n",
    "                    a_idx = S_Y[i]\n",
    "                    b_idx = S_Y[j]\n",
    "                     \n",
    "                    # find Ya covering node in IBG\n",
    "                    Ya = (Y_idxs - {a_idx, b_idx}) | {a_idx}\n",
    "                    Ya = [self.idx2index[idx] for idx in Ya]\n",
    "                    Ya = self.cached_find_covering_node(tuple(Ya)).indexes\n",
    "                    # find Yab covering node in IBG\n",
    "                    Yab = (Y_idxs - {a_idx, b_idx}) | {a_idx, b_idx}\n",
    "                    Yab = [self.idx2index[idx] for idx in Yab]\n",
    "                    Yab = self.cached_find_covering_node(tuple(Yab)).indexes\n",
    "\n",
    "                    used_Y = self.cached_get_cost_used(tuple(Y.indexes))[1]\n",
    "                    used_Ya = self.cached_get_cost_used(tuple(Ya))[1]\n",
    "                    used_Yab = self.cached_get_cost_used(tuple(Yab))[1]\n",
    "                    \n",
    "                    Uab = set([index.index_id for index in used_Y]) | set([index.index_id for index in used_Ya]) | set([index.index_id for index in used_Yab]) \n",
    "                    # find Yb_minus covering node in IBG \n",
    "                    Yb_minus = list((Uab - {a_idx, b_idx}) | {b_idx})\n",
    "                    Yb_minus = [self.idx2index[idx] for idx in Yb_minus]\n",
    "                    Yb_minus = self.cached_find_covering_node(tuple(Yb_minus)).indexes\n",
    "                    # find Yb_plus covering node in IBG\n",
    "                    Yb_plus = list((Y_idxs - {a_idx, b_idx}) | {b_idx})\n",
    "                    Yb_plus = [self.idx2index[idx] for idx in Yb_plus]\n",
    "                    Yb_plus = self.cached_find_covering_node(tuple(Yb_plus)).indexes\n",
    "\n",
    "                    # generate quadruples\n",
    "                    quadruples = [(Y.indexes, Ya, Yb_minus, Yab), (Y.indexes, Ya, Yb_plus, Yab)]\n",
    "\n",
    "                    # compute doi using the quadruples\n",
    "                    for Y_indexes, Ya_indexes, Yb_indexes, Yab_indexes in quadruples:\n",
    "                        cost_Y = self.cached_get_cost_used(tuple(Y_indexes))[0]\n",
    "                        cost_Ya = self.cached_get_cost_used(tuple(Ya_indexes))[0]\n",
    "                        cost_Yb = self.cached_get_cost_used(tuple(Yb_indexes))[0]\n",
    "                        cost_Yab = self.cached_get_cost_used(tuple(Yab_indexes))[0]\n",
    "                        d = abs(cost_Y - cost_Ya - cost_Yb + cost_Yab) / cost_Yab\n",
    "                        \"\"\"\n",
    "                        if (a_idx, b_idx) in doi:\n",
    "                            doi[(a_idx,b_idx)] = max(doi[(a_idx,b_idx)], d)\n",
    "                        elif (b_idx, a_idx) in doi:\n",
    "                            doi[(b_idx,a_idx)] = max(doi[(b_idx,a_idx)], d)\n",
    "                        else:\n",
    "                            raise ValueError(\"Invalid pair of indexes\")    \n",
    "                        \"\"\"\n",
    "                        doi[(a_idx,b_idx)] = max(doi[(a_idx,b_idx)], d)\n",
    "                        # save doi value for the symmetric pair\n",
    "                        doi[(b_idx,a_idx)] = max(doi[(b_idx,a_idx)], d)     \n",
    "                            \n",
    "        return doi\n",
    "\n",
    "\n",
    "    # get precomputed degree of interaction between a pair of indexes\n",
    "    def get_doi_pair(self, a, b):\n",
    "            return self.doi[(a.index_id, b.index_id)]\n",
    "\n",
    "\n",
    "    # function for printing the IBG, using BFS level order traversal\n",
    "    def print_ibg(self):\n",
    "        q = [self.root]\n",
    "        # traverse level by level, print all node ids in a level in a single line before moving to the next level\n",
    "        while len(q) > 0:\n",
    "            next_q = []\n",
    "            for node in q:\n",
    "                print(f\"{node.id} -> \", end=\"\")\n",
    "                for child in node.children:\n",
    "                    next_q.append(child)\n",
    "            print()\n",
    "            q = next_q  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an SSB query generator object\n",
    "qg = QGEN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "template id: 8, query: \n",
      "                SELECT c_city, s_city, d_year, SUM(lo_revenue) AS revenue\n",
      "                FROM customer, lineorder, supplier, dwdate\n",
      "                WHERE lo_custkey = c_custkey\n",
      "                AND lo_suppkey = s_suppkey\n",
      "                AND lo_orderdate = d_datekey\n",
      "                AND c_nation = 'UNITED STATES  '\n",
      "                AND s_nation = 'UNITED STATES  '\n",
      "                AND d_year >= 1996 AND d_year <= 1996\n",
      "                GROUP BY c_city, s_city, d_year\n",
      "                ORDER BY d_year ASC, revenue DESC;\n",
      "            , payload: {'lineorder': ['lo_revenue'], 'dwdate': ['d_year'], 'customer': ['c_city'], 'supplier': ['s_city']}, predicates: {'lineorder': ['lo_custkey', 'lo_suppkey', 'lo_orderdate'], 'dwdate': ['d_year', 'd_datekey'], 'customer': ['c_custkey', 'c_nation', 'c_city'], 'supplier': ['s_suppkey', 's_nation', 's_city']}, order by: {'lineorder': ['lo_revenue'], 'dwdate': ['d_year']}, group by: {'customer': ['c_city'], 'supplier': ['s_city'], 'dwdate': ['d_year']}\n",
      "Number of candidate indexes: 49\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42_43_44_45_46_47_48\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of nodes in IBG: 1536\n",
      "Computing all pair degree of interaction...\n"
     ]
    }
   ],
   "source": [
    "# test IBG \n",
    "\n",
    "query = qg.generate_query(8)\n",
    "print(query)\n",
    "\n",
    "C = extract_query_indexes(qg.generate_query(8), include_cols=False)  \n",
    "\n",
    "ibg = IBG(query, C)\n",
    "\n",
    "ibg.print_ibg()\n",
    "\n",
    "# pick random subset of candidate indexes\n",
    "X = random.sample(ibg.C, 8)\n",
    "cost, used = ibg.get_cost_used(X)\n",
    "print(f\"IBG     --> Cost: {cost}, Used indexes: {[idx.index_id for idx in used]}\")\n",
    "\n",
    "cost, used = ibg._cached_get_cost_used(X)\n",
    "print(f\"What-if --> Cost: {cost}, Used indexes: {[idx.index_id for idx in used]}\")\n",
    "\n",
    "# pick two indexes and a configuration\n",
    "a = ibg.C[0]\n",
    "b = ibg.C[4] \n",
    "X = [ibg.C[1], ibg.C[2], ibg.C[5], ibg.C[6], ibg.C[8]]\n",
    "\n",
    "# compute maximum benefit of adding index 'a' \n",
    "max_benefit = ibg.compute_max_benefit(a)\n",
    "print(f\"\\nMaximum benefit of adding index {a.index_id}: {max_benefit}\")\n",
    "\n",
    "# compute degree of interaction between indexes 'a' and 'b' in configuration X\n",
    "doi = ibg.compute_doi_configuration(a, b, X)\n",
    "print(f\"\\nDOI between indexes {a.index_id} and {b.index_id} : {doi}\")\n",
    "print(f\"in configuration {[idx.index_id for idx in X]}\")\n",
    "\n",
    "# compute configuration independent degree of interaction between indexes 'a' and 'b'\n",
    "doi = ibg.get_doi_pair(a, b)\n",
    "print(f\"\\nDOI between indexes {a.index_id} and {b.index_id} : {doi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for key, value in ibg.doi.items():\n",
    "#    print(f\"doi({key[0]},   {key[1]}) = {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WFIT class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WFIT:\n",
    "\n",
    "    def __init__(self, S_0=[], idxCnt=1000, stateCnt=1000, histSize=100, rand_cnt=5):\n",
    "        # initial set of materialzed indexes\n",
    "        self.S_0 = S_0\n",
    "        # parameter for maximum number of candidate indexes tracked \n",
    "        self.idxCnt = idxCnt\n",
    "        # parameter for maximum number of MTS states/configurations\n",
    "        self.stateCnt = stateCnt\n",
    "        # parameter for maximum number of historical index statistics kept\n",
    "        self.histSize = histSize\n",
    "        # parameter for number of randomized clustering iterations\n",
    "        self.rand_cnt = rand_cnt\n",
    "        # growing list of candidate indexes\n",
    "        self.U = {}\n",
    "        # index benefit and interaction statistics\n",
    "        self.idxStats = defaultdict(list)\n",
    "        self.intStats = defaultdict(list)\n",
    "        # list of currently monitored indexes\n",
    "        self.C = {index.index_id:index for index in S_0} \n",
    "        # list of currently materialized indexes\n",
    "        self.M = {index.index_id:index for index in S_0}  \n",
    "        # initialize stable partitions (each partition is a singleton set of indexes from S_0)\n",
    "        self.stable_partitions = [[index] for index in S_0]\n",
    "        self.n_pos = 0\n",
    "\n",
    "\n",
    "    # update WFIT step for next query in workload (this is the main interface for generating an index configuration recommendation)\n",
    "    def process_WFIT(self, query_object, verbose=False):\n",
    "        self.n_pos += 1\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # generate new partitions \n",
    "        if verbose: print(f\"Generating new partitions for query #{self.n_pos}\")\n",
    "        new_partitions = self.choose_candidates(self.n_pos, query_object, verbose)\n",
    "        # repartition if necessary\n",
    "        if verbose: print(f\"Repartitioning...\")\n",
    "        self.repartition(new_partitions)\n",
    "        # analyze the query\n",
    "        if verbose: print(f\"Analyzing query...\")\n",
    "        self.analyze_query(query_object)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"Time taken for processing query #{self.n_pos}: {end_time - start_time} seconds\")\n",
    "\n",
    "\n",
    "    # TODO: repartition the stable partitions based on the new partitions\n",
    "    def repartition(self, new_partitions):\n",
    "        pass\n",
    "\n",
    "\n",
    "    # TODO: update WFA instance on each stable partition\n",
    "    def analyze_query(self, query_object):\n",
    "        pass\n",
    "\n",
    "\n",
    "    # TODO: update a WFA instance for the given query    \n",
    "    def process_WFA(self, query_object, w, S, S_curr):\n",
    "        pass    \n",
    "\n",
    "\n",
    "    # compute index benefit graph for the given query and candidate indexes\n",
    "    def compute_IBG(self, query_object, candidate_indexes):\n",
    "        return IBG(query_object, candidate_indexes)\n",
    "    \n",
    "\n",
    "    # extract candidate indexes from given query\n",
    "    def extract_indexes(self, query_object, include_cols=False):\n",
    "        return extract_query_indexes(query_object, include_cols)\n",
    "\n",
    "\n",
    "    # generate stable partitions/sets of indexes for next query in workload\n",
    "    def choose_candidates(self, n_pos, query_object, verbose):\n",
    "        # extract new candidate indexes from the query\n",
    "        new_indexes = self.extract_indexes(query_object)\n",
    "        if verbose: print(f\"Extracted {len(new_indexes)} indexes from query.\")\n",
    "        # add new indexes to the list of all candidate indexes\n",
    "        for index in new_indexes:\n",
    "            if index.index_id not in self.U:\n",
    "                self.U[index.index_id] = index\n",
    "        \n",
    "        if verbose: print(f\"Num candidate indexes (including those currently materialized), |U| = {len(self.U)}\")\n",
    "        \n",
    "\n",
    "        # TODO: check if the number of candidate indexes exceeds the limit, then need to evict some indexes\n",
    "        \n",
    "        # compute index benefit graph for the query\n",
    "        if verbose: print(f\"Computing IBG...\")\n",
    "        ibg = self.compute_IBG(query_object, list(self.U.values()))\n",
    "        \n",
    "        # update statistics for the candidate indexes (n_pos is the position of the query in the workload sequence)\n",
    "        if verbose: print(f\"Updating statistics...\")\n",
    "        self.update_stats(n_pos, ibg, verbose=False)\n",
    "\n",
    "        # non-materialized candidate indexes \n",
    "        X = [self.U[index_id] for index_id in self.U if index_id not in self.M]\n",
    "        num_indexes = self.idxCnt - len(self.M)\n",
    "\n",
    "        # determine new set of candidate indexes to monitor for upcoming workload queries\n",
    "        if verbose: print(f\"Choosing top {num_indexes} indexes from {len(X)} non-materialized candidate indexes\")\n",
    "        top_indexes = self.top_indexes(n_pos, X, num_indexes, verbose)\n",
    "        D = self.M | top_indexes\n",
    "        if verbose: print(f\"New set of indexes to monitor for upcoming workload, |D| = {len(D)}\")\n",
    "\n",
    "        # generate new partitions by clustering the new candidate set\n",
    "        if verbose: print(f\"Choosing new partitions...\")\n",
    "        new_partitions = self.choose_partition(D, verbose)\n",
    "\n",
    "\n",
    "\n",
    "    # partition the new candidate set into clusters \n",
    "    # (need to optimize this function, currently it is a naive implementation)\n",
    "    def choose_partition(self, N_workload, D, verbose):\n",
    "        \n",
    "        # compute total loss, i.e. sum of doi across indexes from pairs of partitions\n",
    "        def compute_loss(P, current_doi):\n",
    "            loss = 0\n",
    "            for i in range(len(P)):\n",
    "                for j in range(i+1, len(P)):\n",
    "                    for a in P[i]:\n",
    "                        for b in P[j]:\n",
    "                            loss += current_doi(a, b)\n",
    "            return loss\n",
    "        \n",
    "        # compute current doi values for all pairs of indexes in U\n",
    "        current_doi = defaultdict(int)\n",
    "        for (a_idx, b_idx) in self.intStats.keys():\n",
    "            # take max over incremental averages\n",
    "            current_doi[(a_idx, b_idx)] = 0\n",
    "            doi_total = 0\n",
    "            for (n, doi) in self.intStats[(a_idx, b_idx)]:\n",
    "                doi_total += doi\n",
    "                doi_avg = doi_total / (N_workload-n+1)\n",
    "                current_doi[(a_idx, b_idx)] = max(current_doi[(a_idx, b_idx)], doi_avg)\n",
    "            # save symmetric doi value\n",
    "            current_doi[(b_idx, a_idx)] = current_doi[(a_idx, b_idx)]    \n",
    "\n",
    "        # from each current stable partition, remove indexes not in D\n",
    "        P = []\n",
    "        for partition in self.stable_partitions:\n",
    "            P.append([index for index in partition if index.index_id in D])\n",
    "\n",
    "        # add a singleton partition containing each new index in D not in C\n",
    "        for index_id, index in D.items():\n",
    "            if index_id not in self.C:\n",
    "                P.append([index])\n",
    "        \n",
    "        # set the new partition as baseline solution\n",
    "        bestSolution = P\n",
    "        bestLoss = compute_loss(P, current_doi)\n",
    "\n",
    "        # perform randomized clustering to find better solution\n",
    "        for i in range(self.rand_cnt):\n",
    "            # create partition of D in singletons\n",
    "            P = [[index] for index in D.values()]\n",
    "            partition2id = {tuple(partition):i for i, partition in enumerate(P)}\n",
    "            #id2partition = {i:P for i, partition in enumerate(P)}\n",
    "            loss_cache = {}\n",
    "            \n",
    "            # merge singletons until only one partition remains\n",
    "            while True:\n",
    "                # find all feasible merge candidates pairs (i.e. pairs with loss > 0)\n",
    "                E = []\n",
    "                E1 = []\n",
    "\n",
    "                # get loss for all pairs of partitions\n",
    "                for i in range(len(P)):\n",
    "                    for j in range(i+1, len(P)):\n",
    "                        Pi_id = partition2id[tuple(P[i])]\n",
    "                        Pj_id = partition2id[tuple(P[j])]\n",
    "                        if (Pi_id, Pj_id) in loss_cache:\n",
    "                            loss = loss_cache[(Pi_id, Pj_id)]\n",
    "                        else:\n",
    "                            loss = compute_loss(P[i] + P[j], current_doi)\n",
    "                            loss_cache[(Pi_id, Pj_id)] = loss\n",
    "                        \n",
    "                        if loss > 0:\n",
    "                            E.append((P[i], P[j], loss))    \n",
    "                            if len(P[i]) == 1 and len(P[j]) == 1:\n",
    "                                E1.append((P[i],P[j], loss))\n",
    "\n",
    "                if len(E) == 0:\n",
    "                    break\n",
    "                \n",
    "                elif len(E1) > 0:\n",
    "                    # merge a random pair of singletons, sample randomly from E1 weighted by loss (i.e. high loss pairs more likely to be merged)\n",
    "                    Pi, Pj, loss = random.choices(E1, weights=[loss for (Pi, Pj, loss) in E1], k=1)[0]\n",
    "                    Pij_merged = Pi + Pj\n",
    "                    P.remove(Pi)\n",
    "                    P.remove(Pj)\n",
    "                    P.append(Pij_merged) \n",
    "                    E1.remove((Pi, Pj, loss))  \n",
    "                    partition2id[len(partition2id)] = tuple(Pij_merged)\n",
    "\n",
    "                else:\n",
    "                    # merge a random pair of partitions, sample randomly from E weighted by normalized loss  \n",
    "                    Pi, Pj, loss = random.choices(E, weights=[loss for (Pi, Pj, loss) in E], k=1)[0]\n",
    "                    Pi, Pj = random.choices(E, weights=[loss / (2**(len(Pi) + len(Pj)) - 2**len(Pi) - 2**len(Pj)) for (Pi, Pj, loss) in E], k=1)[0]\n",
    "                    Pij_merged = Pi + Pj\n",
    "                    P.remove(Pi)\n",
    "                    P.remove(Pj)\n",
    "                    P.append(Pij_merged)   \n",
    "                    E.remove((Pi, Pj, loss)) \n",
    "                    partition2id[len(partition2id)] = tuple(Pij_merged)\n",
    "\n",
    "            # check if the new solution is better than the current best solution\n",
    "            loss = compute_loss(P, current_doi)\n",
    "            if loss < bestLoss:\n",
    "                bestSolution = P\n",
    "                bestLoss = loss\n",
    "\n",
    "        return bestSolution\n",
    "\n",
    "\n",
    "    # update candidate index statistics\n",
    "    def update_stats(self, n, ibg, verbose):\n",
    "        # update index benefit statistics\n",
    "        for index in self.U.values():\n",
    "            max_benefit = ibg.compute_max_benefit(index)\n",
    "            self.idxStats[index.index_id].append((n, max_benefit))\n",
    "            # evict old stats if the size exceeds histSize\n",
    "            self.idxStats[index.index_id] = self.idxStats[index.index_id][-self.histSize:]\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Index benefit statistics:\")\n",
    "            for index_id, stats in self.idxStats.items():\n",
    "                print(f\"Index {index_id}: {stats}\")\n",
    "\n",
    "\n",
    "        # update index interaction statistics\n",
    "        for (a_idx, b_idx) in ibg.doi.keys():\n",
    "            d = ibg.doi[(a_idx, b_idx)]\n",
    "            if d > 0:\n",
    "                self.intStats[(a_idx, b_idx)].append((n, doi))\n",
    "            # evict old stats if the size exceeds histSize\n",
    "            self.intStats[(a_idx, b_idx)] = self.intStats[(a_idx, b_idx)][-self.histSize:]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Index interaction statistics:\")\n",
    "            for pair, stats in self.intStats.items():\n",
    "                print(f\"Pair {pair}: {stats}\")\n",
    "\n",
    "\n",
    "    # choose top num_indexes indexes from X with highest potential benefit\n",
    "    def top_indexes(self, N_workload, X, num_indexes, verbose):\n",
    "        if verbose:\n",
    "            print(f\"Non-materialized candidate indexes, X = {[index.index_id for index in X]}\")\n",
    "\n",
    "        # compute \"current benefit\" of each index in X (these are derived from statistics of observed benefits from recent queries)\n",
    "        score = {}\n",
    "        for index in X:\n",
    "            if len(self.idxStats[index.index_id]) == 0:\n",
    "                # zero current benefit if no statistics are available\n",
    "                current_benefit = 0\n",
    "            else:\n",
    "                # take the maximum over all incremental average benefits \n",
    "                current_benefit = 0\n",
    "                b_total = 0\n",
    "                for (n, b) in self.idxStats[index.index_id]:\n",
    "                    b_total += b \n",
    "                    # incremental average benefit of index up to query n (higher weight/smaller denominator for more recent queries)\n",
    "                    benefit = b_total / (N_workload - n + 1)\n",
    "                    current_benefit = max(current_benefit, benefit)\n",
    "\n",
    "            # use current benefit to compute a score for the index\n",
    "            if index.index_id in self.C:\n",
    "                # if index already being monitored, then score is just current benefit\n",
    "                score[index.index_id] = current_benefit\n",
    "            else:\n",
    "                # if index not being monitored, then score is current benefit minus cost of creating the index\n",
    "                # (unmonitored indexes are penalized so that they are only chosen if they have high potential benefit, which helps keep C stable)\n",
    "                score[index.index_id] = current_benefit - self.get_index_creation_cost(index)\n",
    "\n",
    "        #if verbose:\n",
    "        #    print(\"Index scores:\")\n",
    "        #    for index_id, s in score.items():\n",
    "        #        print(f\"Index {index_id}: {s}\")\n",
    "\n",
    "        # get the top num_indexes indexes with highest scores (keep non-zero scores only)\n",
    "        top_indexes = [index_id for index_id, s in score.items() if s > 0]\n",
    "        top_indexes = sorted(top_indexes, key=lambda x: score[x], reverse=True)[:num_indexes]\n",
    "        top_indexes = {index_id: self.U[index_id] for index_id in top_indexes}\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{len(top_indexes)} top indexes: {[index.index_id for index in top_indexes.values()]}\")\n",
    "\n",
    "        return top_indexes    \n",
    "\n",
    "\n",
    "    # TODO: return index creation cost\n",
    "    def get_index_creation_cost(self, index):\n",
    "        return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test WFIT implementation on sample SSB workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                SELECT SUM(lo_extendedprice * lo_discount) AS revenue\n",
      "                FROM lineorder, dwdate\n",
      "                WHERE lo_orderdate = d_datekey\n",
      "                AND d_year = 1996\n",
      "                AND lo_discount BETWEEN 4 AND 6 \n",
      "                AND lo_quantity < 29;\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "# generate an SSB workload\n",
    "workload = [qg.generate_query(i) for i in range(1, 9)]\n",
    "\n",
    "#print(workload[0].query_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query 1\n",
      "Generating new partitions for query #1\n",
      "Extracted 19 indexes from query.\n",
      "Num candidate indexes (including those currently materialized), |U| = 19\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 19\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of nodes in IBG: 4\n",
      "Computing all pair degree of interaction...\n",
      "Updating statistics...\n",
      "Choosing top 19 indexes from 19 non-materialized candidate indexes\n",
      "Non-materialized candidate indexes, X = ['IX_lineorder_lo_orderdate', 'IX_lineorder_lo_discount', 'IX_lineorder_lo_quantity', 'IX_lineorder_lo_orderdate_lo_discount', 'IX_lineorder_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_orderdate', 'IX_lineorder_lo_discount_lo_quantity', 'IX_lineorder_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_discount', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_lineorder_lo_discount_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_orderdate_lo_discount', 'IX_lineorder_lo_quantity_lo_discount_lo_orderdate', 'IX_dwdate_d_datekey', 'IX_dwdate_d_year', 'IX_dwdate_d_datekey_d_year', 'IX_dwdate_d_year_d_datekey']\n",
      "3 top indexes: ['IX_dwdate_d_year_d_datekey', 'IX_dwdate_d_year', 'IX_dwdate_d_datekey_d_year']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 4\n",
      "Generating new partitions...\n",
      "Repartitioning...\n",
      "Analyzing query...\n",
      "Time taken for processing query #1: 0.07083010673522949 seconds\n",
      "\n",
      "\n",
      "\n",
      "Processing query 2\n",
      "Generating new partitions for query #2\n",
      "Extracted 19 indexes from query.\n",
      "Num candidate indexes (including those currently materialized), |U| = 22\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 22\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of nodes in IBG: 19\n",
      "Computing all pair degree of interaction...\n",
      "Updating statistics...\n",
      "Choosing top 19 indexes from 22 non-materialized candidate indexes\n",
      "Non-materialized candidate indexes, X = ['IX_lineorder_lo_orderdate', 'IX_lineorder_lo_discount', 'IX_lineorder_lo_quantity', 'IX_lineorder_lo_orderdate_lo_discount', 'IX_lineorder_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_orderdate', 'IX_lineorder_lo_discount_lo_quantity', 'IX_lineorder_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_discount', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_lineorder_lo_discount_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_orderdate_lo_discount', 'IX_lineorder_lo_quantity_lo_discount_lo_orderdate', 'IX_dwdate_d_datekey', 'IX_dwdate_d_year', 'IX_dwdate_d_datekey_d_year', 'IX_dwdate_d_year_d_datekey', 'IX_dwdate_d_yearmonthnum', 'IX_dwdate_d_datekey_d_yearmonthnum', 'IX_dwdate_d_yearmonthnum_d_datekey']\n",
      "9 top indexes: ['IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_dwdate_d_yearmonthnum_d_datekey', 'IX_dwdate_d_yearmonthnum', 'IX_dwdate_d_datekey_d_yearmonthnum', 'IX_lineorder_lo_orderdate_lo_discount', 'IX_dwdate_d_year_d_datekey', 'IX_dwdate_d_year', 'IX_dwdate_d_datekey_d_year']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 10\n",
      "Generating new partitions...\n",
      "Repartitioning...\n",
      "Analyzing query...\n",
      "Time taken for processing query #2: 0.2815120220184326 seconds\n",
      "\n",
      "\n",
      "\n",
      "Processing query 3\n",
      "Generating new partitions for query #3\n",
      "Extracted 19 indexes from query.\n",
      "Num candidate indexes (including those currently materialized), |U| = 25\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 25\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of nodes in IBG: 42\n",
      "Computing all pair degree of interaction...\n",
      "Updating statistics...\n",
      "Choosing top 19 indexes from 25 non-materialized candidate indexes\n",
      "Non-materialized candidate indexes, X = ['IX_lineorder_lo_orderdate', 'IX_lineorder_lo_discount', 'IX_lineorder_lo_quantity', 'IX_lineorder_lo_orderdate_lo_discount', 'IX_lineorder_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_orderdate', 'IX_lineorder_lo_discount_lo_quantity', 'IX_lineorder_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_discount', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_lineorder_lo_discount_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_orderdate_lo_discount', 'IX_lineorder_lo_quantity_lo_discount_lo_orderdate', 'IX_dwdate_d_datekey', 'IX_dwdate_d_year', 'IX_dwdate_d_datekey_d_year', 'IX_dwdate_d_year_d_datekey', 'IX_dwdate_d_yearmonthnum', 'IX_dwdate_d_datekey_d_yearmonthnum', 'IX_dwdate_d_yearmonthnum_d_datekey', 'IX_dwdate_d_weeknuminyear', 'IX_dwdate_d_datekey_d_weeknuminyear', 'IX_dwdate_d_weeknuminyear_d_datekey']\n",
      "15 top indexes: ['IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_lineorder_lo_orderdate_lo_discount', 'IX_lineorder_lo_orderdate_lo_quantity', 'IX_lineorder_lo_orderdate', 'IX_dwdate_d_yearmonthnum_d_datekey', 'IX_dwdate_d_yearmonthnum', 'IX_dwdate_d_datekey_d_yearmonthnum', 'IX_dwdate_d_year', 'IX_dwdate_d_year_d_datekey', 'IX_dwdate_d_weeknuminyear', 'IX_dwdate_d_weeknuminyear_d_datekey', 'IX_dwdate_d_datekey_d_year', 'IX_dwdate_d_datekey_d_weeknuminyear', 'IX_dwdate_d_datekey']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 16\n",
      "Generating new partitions...\n",
      "Repartitioning...\n",
      "Analyzing query...\n",
      "Time taken for processing query #3: 0.6820690631866455 seconds\n",
      "\n",
      "\n",
      "\n",
      "Processing query 4\n",
      "Generating new partitions for query #4\n",
      "Extracted 38 indexes from query.\n",
      "Num candidate indexes (including those currently materialized), |U| = 58\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 58\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42_43_44_45_46_47_48_49_50_51_52_53_54_55_56_57\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of nodes in IBG: 18\n",
      "Computing all pair degree of interaction...\n",
      "Updating statistics...\n",
      "Choosing top 19 indexes from 58 non-materialized candidate indexes\n",
      "Non-materialized candidate indexes, X = ['IX_lineorder_lo_orderdate', 'IX_lineorder_lo_discount', 'IX_lineorder_lo_quantity', 'IX_lineorder_lo_orderdate_lo_discount', 'IX_lineorder_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_orderdate', 'IX_lineorder_lo_discount_lo_quantity', 'IX_lineorder_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_discount', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_lineorder_lo_discount_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_orderdate_lo_discount', 'IX_lineorder_lo_quantity_lo_discount_lo_orderdate', 'IX_dwdate_d_datekey', 'IX_dwdate_d_year', 'IX_dwdate_d_datekey_d_year', 'IX_dwdate_d_year_d_datekey', 'IX_dwdate_d_yearmonthnum', 'IX_dwdate_d_datekey_d_yearmonthnum', 'IX_dwdate_d_yearmonthnum_d_datekey', 'IX_dwdate_d_weeknuminyear', 'IX_dwdate_d_datekey_d_weeknuminyear', 'IX_dwdate_d_weeknuminyear_d_datekey', 'IX_lineorder_lo_partkey', 'IX_lineorder_lo_suppkey', 'IX_lineorder_lo_orderdate_lo_partkey', 'IX_lineorder_lo_orderdate_lo_suppkey', 'IX_lineorder_lo_partkey_lo_orderdate', 'IX_lineorder_lo_partkey_lo_suppkey', 'IX_lineorder_lo_suppkey_lo_orderdate', 'IX_lineorder_lo_suppkey_lo_partkey', 'IX_lineorder_lo_orderdate_lo_partkey_lo_suppkey', 'IX_lineorder_lo_orderdate_lo_suppkey_lo_partkey', 'IX_lineorder_lo_partkey_lo_orderdate_lo_suppkey', 'IX_lineorder_lo_partkey_lo_suppkey_lo_orderdate', 'IX_lineorder_lo_suppkey_lo_orderdate_lo_partkey', 'IX_lineorder_lo_suppkey_lo_partkey_lo_orderdate', 'IX_part_p_partkey', 'IX_part_p_category', 'IX_part_p_brand', 'IX_part_p_partkey_p_category', 'IX_part_p_partkey_p_brand', 'IX_part_p_category_p_partkey', 'IX_part_p_category_p_brand', 'IX_part_p_brand_p_partkey', 'IX_part_p_brand_p_category', 'IX_part_p_partkey_p_category_p_brand', 'IX_part_p_partkey_p_brand_p_category', 'IX_part_p_category_p_partkey_p_brand', 'IX_part_p_category_p_brand_p_partkey', 'IX_part_p_brand_p_partkey_p_category', 'IX_part_p_brand_p_category_p_partkey', 'IX_supplier_s_suppkey', 'IX_supplier_s_region', 'IX_supplier_s_suppkey_s_region', 'IX_supplier_s_region_s_suppkey']\n",
      "19 top indexes: ['IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_lineorder_lo_orderdate_lo_discount', 'IX_lineorder_lo_orderdate_lo_quantity', 'IX_lineorder_lo_orderdate', 'IX_dwdate_d_yearmonthnum_d_datekey', 'IX_dwdate_d_yearmonthnum', 'IX_dwdate_d_datekey_d_yearmonthnum', 'IX_part_p_category_p_partkey_p_brand', 'IX_part_p_category_p_brand_p_partkey', 'IX_part_p_category', 'IX_part_p_category_p_partkey', 'IX_part_p_category_p_brand', 'IX_supplier_s_region_s_suppkey', 'IX_dwdate_d_year', 'IX_dwdate_d_year_d_datekey', 'IX_supplier_s_region', 'IX_dwdate_d_weeknuminyear', 'IX_dwdate_d_weeknuminyear_d_datekey']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 20\n",
      "Generating new partitions...\n",
      "Repartitioning...\n",
      "Analyzing query...\n",
      "Time taken for processing query #4: 0.4061915874481201 seconds\n",
      "\n",
      "\n",
      "\n",
      "Processing query 5\n",
      "Generating new partitions for query #5\n",
      "Extracted 38 indexes from query.\n",
      "Num candidate indexes (including those currently materialized), |U| = 58\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 58\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42_43_44_45_46_47_48_49_50_51_52_53_54_55_56_57\n",
      "Constructing IBG...\n",
      "Number of nodes in IBG: 18\n",
      "Computing all pair degree of interaction...\n",
      "Updating statistics...\n",
      "Choosing top 19 indexes from 58 non-materialized candidate indexes\n",
      "Non-materialized candidate indexes, X = ['IX_lineorder_lo_orderdate', 'IX_lineorder_lo_discount', 'IX_lineorder_lo_quantity', 'IX_lineorder_lo_orderdate_lo_discount', 'IX_lineorder_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_orderdate', 'IX_lineorder_lo_discount_lo_quantity', 'IX_lineorder_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_discount', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_lineorder_lo_discount_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_orderdate_lo_discount', 'IX_lineorder_lo_quantity_lo_discount_lo_orderdate', 'IX_dwdate_d_datekey', 'IX_dwdate_d_year', 'IX_dwdate_d_datekey_d_year', 'IX_dwdate_d_year_d_datekey', 'IX_dwdate_d_yearmonthnum', 'IX_dwdate_d_datekey_d_yearmonthnum', 'IX_dwdate_d_yearmonthnum_d_datekey', 'IX_dwdate_d_weeknuminyear', 'IX_dwdate_d_datekey_d_weeknuminyear', 'IX_dwdate_d_weeknuminyear_d_datekey', 'IX_lineorder_lo_partkey', 'IX_lineorder_lo_suppkey', 'IX_lineorder_lo_orderdate_lo_partkey', 'IX_lineorder_lo_orderdate_lo_suppkey', 'IX_lineorder_lo_partkey_lo_orderdate', 'IX_lineorder_lo_partkey_lo_suppkey', 'IX_lineorder_lo_suppkey_lo_orderdate', 'IX_lineorder_lo_suppkey_lo_partkey', 'IX_lineorder_lo_orderdate_lo_partkey_lo_suppkey', 'IX_lineorder_lo_orderdate_lo_suppkey_lo_partkey', 'IX_lineorder_lo_partkey_lo_orderdate_lo_suppkey', 'IX_lineorder_lo_partkey_lo_suppkey_lo_orderdate', 'IX_lineorder_lo_suppkey_lo_orderdate_lo_partkey', 'IX_lineorder_lo_suppkey_lo_partkey_lo_orderdate', 'IX_part_p_partkey', 'IX_part_p_category', 'IX_part_p_brand', 'IX_part_p_partkey_p_category', 'IX_part_p_partkey_p_brand', 'IX_part_p_category_p_partkey', 'IX_part_p_category_p_brand', 'IX_part_p_brand_p_partkey', 'IX_part_p_brand_p_category', 'IX_part_p_partkey_p_category_p_brand', 'IX_part_p_partkey_p_brand_p_category', 'IX_part_p_category_p_partkey_p_brand', 'IX_part_p_category_p_brand_p_partkey', 'IX_part_p_brand_p_partkey_p_category', 'IX_part_p_brand_p_category_p_partkey', 'IX_supplier_s_suppkey', 'IX_supplier_s_region', 'IX_supplier_s_suppkey_s_region', 'IX_supplier_s_region_s_suppkey']\n",
      "19 top indexes: ['IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_lineorder_lo_orderdate_lo_discount', 'IX_lineorder_lo_orderdate_lo_quantity', 'IX_lineorder_lo_orderdate', 'IX_dwdate_d_yearmonthnum_d_datekey', 'IX_dwdate_d_yearmonthnum', 'IX_dwdate_d_datekey_d_yearmonthnum', 'IX_part_p_category_p_partkey_p_brand', 'IX_part_p_category_p_brand_p_partkey', 'IX_part_p_category', 'IX_part_p_category_p_partkey', 'IX_part_p_category_p_brand', 'IX_supplier_s_region_s_suppkey', 'IX_dwdate_d_year', 'IX_supplier_s_region', 'IX_dwdate_d_year_d_datekey', 'IX_dwdate_d_weeknuminyear', 'IX_dwdate_d_weeknuminyear_d_datekey']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 20\n",
      "Generating new partitions...\n",
      "Repartitioning...\n",
      "Analyzing query...\n",
      "Time taken for processing query #5: 0.005135774612426758 seconds\n",
      "\n",
      "\n",
      "\n",
      "Processing query 6\n",
      "Generating new partitions for query #6\n",
      "Extracted 38 indexes from query.\n",
      "Num candidate indexes (including those currently materialized), |U| = 58\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 58\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42_43_44_45_46_47_48_49_50_51_52_53_54_55_56_57\n",
      "Constructing IBG...\n",
      "Number of nodes in IBG: 18\n",
      "Computing all pair degree of interaction...\n",
      "Updating statistics...\n",
      "Choosing top 19 indexes from 58 non-materialized candidate indexes\n",
      "Non-materialized candidate indexes, X = ['IX_lineorder_lo_orderdate', 'IX_lineorder_lo_discount', 'IX_lineorder_lo_quantity', 'IX_lineorder_lo_orderdate_lo_discount', 'IX_lineorder_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_orderdate', 'IX_lineorder_lo_discount_lo_quantity', 'IX_lineorder_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_discount', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_lineorder_lo_discount_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_orderdate_lo_discount', 'IX_lineorder_lo_quantity_lo_discount_lo_orderdate', 'IX_dwdate_d_datekey', 'IX_dwdate_d_year', 'IX_dwdate_d_datekey_d_year', 'IX_dwdate_d_year_d_datekey', 'IX_dwdate_d_yearmonthnum', 'IX_dwdate_d_datekey_d_yearmonthnum', 'IX_dwdate_d_yearmonthnum_d_datekey', 'IX_dwdate_d_weeknuminyear', 'IX_dwdate_d_datekey_d_weeknuminyear', 'IX_dwdate_d_weeknuminyear_d_datekey', 'IX_lineorder_lo_partkey', 'IX_lineorder_lo_suppkey', 'IX_lineorder_lo_orderdate_lo_partkey', 'IX_lineorder_lo_orderdate_lo_suppkey', 'IX_lineorder_lo_partkey_lo_orderdate', 'IX_lineorder_lo_partkey_lo_suppkey', 'IX_lineorder_lo_suppkey_lo_orderdate', 'IX_lineorder_lo_suppkey_lo_partkey', 'IX_lineorder_lo_orderdate_lo_partkey_lo_suppkey', 'IX_lineorder_lo_orderdate_lo_suppkey_lo_partkey', 'IX_lineorder_lo_partkey_lo_orderdate_lo_suppkey', 'IX_lineorder_lo_partkey_lo_suppkey_lo_orderdate', 'IX_lineorder_lo_suppkey_lo_orderdate_lo_partkey', 'IX_lineorder_lo_suppkey_lo_partkey_lo_orderdate', 'IX_part_p_partkey', 'IX_part_p_category', 'IX_part_p_brand', 'IX_part_p_partkey_p_category', 'IX_part_p_partkey_p_brand', 'IX_part_p_category_p_partkey', 'IX_part_p_category_p_brand', 'IX_part_p_brand_p_partkey', 'IX_part_p_brand_p_category', 'IX_part_p_partkey_p_category_p_brand', 'IX_part_p_partkey_p_brand_p_category', 'IX_part_p_category_p_partkey_p_brand', 'IX_part_p_category_p_brand_p_partkey', 'IX_part_p_brand_p_partkey_p_category', 'IX_part_p_brand_p_category_p_partkey', 'IX_supplier_s_suppkey', 'IX_supplier_s_region', 'IX_supplier_s_suppkey_s_region', 'IX_supplier_s_region_s_suppkey']\n",
      "19 top indexes: ['IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_lineorder_lo_orderdate_lo_discount', 'IX_lineorder_lo_orderdate_lo_quantity', 'IX_lineorder_lo_orderdate', 'IX_dwdate_d_yearmonthnum_d_datekey', 'IX_dwdate_d_yearmonthnum', 'IX_dwdate_d_datekey_d_yearmonthnum', 'IX_part_p_category_p_partkey_p_brand', 'IX_part_p_category_p_brand_p_partkey', 'IX_part_p_category', 'IX_part_p_category_p_partkey', 'IX_part_p_category_p_brand', 'IX_supplier_s_region_s_suppkey', 'IX_supplier_s_region', 'IX_dwdate_d_year', 'IX_dwdate_d_year_d_datekey', 'IX_dwdate_d_weeknuminyear', 'IX_dwdate_d_weeknuminyear_d_datekey']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 20\n",
      "Generating new partitions...\n",
      "Repartitioning...\n",
      "Analyzing query...\n",
      "Time taken for processing query #6: 0.005040884017944336 seconds\n",
      "\n",
      "\n",
      "\n",
      "Processing query 7\n",
      "Generating new partitions for query #7\n",
      "Extracted 49 indexes from query.\n",
      "Num candidate indexes (including those currently materialized), |U| = 95\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 95\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42_43_44_45_46_47_48_49_50_51_52_53_54_55_56_57_58_59_60_61_62_63_64_65_66_67_68_69_70_71_72_73_74_75_76_77_78_79_80_81_82_83_84_85_86_87_88_89_90_91_92_93_94\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of nodes in IBG: 54\n",
      "Computing all pair degree of interaction...\n",
      "Updating statistics...\n",
      "Choosing top 19 indexes from 95 non-materialized candidate indexes\n",
      "Non-materialized candidate indexes, X = ['IX_lineorder_lo_orderdate', 'IX_lineorder_lo_discount', 'IX_lineorder_lo_quantity', 'IX_lineorder_lo_orderdate_lo_discount', 'IX_lineorder_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_orderdate', 'IX_lineorder_lo_discount_lo_quantity', 'IX_lineorder_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_discount', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_lineorder_lo_discount_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_orderdate_lo_discount', 'IX_lineorder_lo_quantity_lo_discount_lo_orderdate', 'IX_dwdate_d_datekey', 'IX_dwdate_d_year', 'IX_dwdate_d_datekey_d_year', 'IX_dwdate_d_year_d_datekey', 'IX_dwdate_d_yearmonthnum', 'IX_dwdate_d_datekey_d_yearmonthnum', 'IX_dwdate_d_yearmonthnum_d_datekey', 'IX_dwdate_d_weeknuminyear', 'IX_dwdate_d_datekey_d_weeknuminyear', 'IX_dwdate_d_weeknuminyear_d_datekey', 'IX_lineorder_lo_partkey', 'IX_lineorder_lo_suppkey', 'IX_lineorder_lo_orderdate_lo_partkey', 'IX_lineorder_lo_orderdate_lo_suppkey', 'IX_lineorder_lo_partkey_lo_orderdate', 'IX_lineorder_lo_partkey_lo_suppkey', 'IX_lineorder_lo_suppkey_lo_orderdate', 'IX_lineorder_lo_suppkey_lo_partkey', 'IX_lineorder_lo_orderdate_lo_partkey_lo_suppkey', 'IX_lineorder_lo_orderdate_lo_suppkey_lo_partkey', 'IX_lineorder_lo_partkey_lo_orderdate_lo_suppkey', 'IX_lineorder_lo_partkey_lo_suppkey_lo_orderdate', 'IX_lineorder_lo_suppkey_lo_orderdate_lo_partkey', 'IX_lineorder_lo_suppkey_lo_partkey_lo_orderdate', 'IX_part_p_partkey', 'IX_part_p_category', 'IX_part_p_brand', 'IX_part_p_partkey_p_category', 'IX_part_p_partkey_p_brand', 'IX_part_p_category_p_partkey', 'IX_part_p_category_p_brand', 'IX_part_p_brand_p_partkey', 'IX_part_p_brand_p_category', 'IX_part_p_partkey_p_category_p_brand', 'IX_part_p_partkey_p_brand_p_category', 'IX_part_p_category_p_partkey_p_brand', 'IX_part_p_category_p_brand_p_partkey', 'IX_part_p_brand_p_partkey_p_category', 'IX_part_p_brand_p_category_p_partkey', 'IX_supplier_s_suppkey', 'IX_supplier_s_region', 'IX_supplier_s_suppkey_s_region', 'IX_supplier_s_region_s_suppkey', 'IX_lineorder_lo_custkey', 'IX_lineorder_lo_custkey_lo_suppkey', 'IX_lineorder_lo_custkey_lo_orderdate', 'IX_lineorder_lo_suppkey_lo_custkey', 'IX_lineorder_lo_orderdate_lo_custkey', 'IX_lineorder_lo_custkey_lo_suppkey_lo_orderdate', 'IX_lineorder_lo_custkey_lo_orderdate_lo_suppkey', 'IX_lineorder_lo_suppkey_lo_custkey_lo_orderdate', 'IX_lineorder_lo_suppkey_lo_orderdate_lo_custkey', 'IX_lineorder_lo_orderdate_lo_custkey_lo_suppkey', 'IX_lineorder_lo_orderdate_lo_suppkey_lo_custkey', 'IX_customer_c_custkey', 'IX_customer_c_region', 'IX_customer_c_nation', 'IX_customer_c_custkey_c_region', 'IX_customer_c_custkey_c_nation', 'IX_customer_c_region_c_custkey', 'IX_customer_c_region_c_nation', 'IX_customer_c_nation_c_custkey', 'IX_customer_c_nation_c_region', 'IX_customer_c_custkey_c_region_c_nation', 'IX_customer_c_custkey_c_nation_c_region', 'IX_customer_c_region_c_custkey_c_nation', 'IX_customer_c_region_c_nation_c_custkey', 'IX_customer_c_nation_c_custkey_c_region', 'IX_customer_c_nation_c_region_c_custkey', 'IX_supplier_s_nation', 'IX_supplier_s_suppkey_s_nation', 'IX_supplier_s_region_s_nation', 'IX_supplier_s_nation_s_suppkey', 'IX_supplier_s_nation_s_region', 'IX_supplier_s_suppkey_s_region_s_nation', 'IX_supplier_s_suppkey_s_nation_s_region', 'IX_supplier_s_region_s_suppkey_s_nation', 'IX_supplier_s_region_s_nation_s_suppkey', 'IX_supplier_s_nation_s_suppkey_s_region', 'IX_supplier_s_nation_s_region_s_suppkey']\n",
      "19 top indexes: ['IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_lineorder_lo_orderdate_lo_discount', 'IX_lineorder_lo_orderdate_lo_quantity', 'IX_lineorder_lo_orderdate', 'IX_dwdate_d_yearmonthnum_d_datekey', 'IX_dwdate_d_yearmonthnum', 'IX_dwdate_d_datekey_d_yearmonthnum', 'IX_part_p_category_p_partkey_p_brand', 'IX_part_p_category_p_brand_p_partkey', 'IX_part_p_category', 'IX_part_p_category_p_partkey', 'IX_part_p_category_p_brand', 'IX_customer_c_region_c_custkey_c_nation', 'IX_customer_c_region_c_nation_c_custkey', 'IX_supplier_s_region_s_suppkey', 'IX_supplier_s_region', 'IX_supplier_s_region_s_suppkey_s_nation', 'IX_supplier_s_region_s_nation_s_suppkey']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 20\n",
      "Generating new partitions...\n",
      "Repartitioning...\n",
      "Analyzing query...\n",
      "Time taken for processing query #7: 1.4168672561645508 seconds\n",
      "\n",
      "\n",
      "\n",
      "Processing query 8\n",
      "Generating new partitions for query #8\n",
      "Extracted 49 indexes from query.\n",
      "Num candidate indexes (including those currently materialized), |U| = 117\n",
      "Computing IBG...\n",
      "Number of candidate indexes: 117\n",
      "Created root node with id: 0_1_2_3_4_5_6_7_8_9_10_11_12_13_14_15_16_17_18_19_20_21_22_23_24_25_26_27_28_29_30_31_32_33_34_35_36_37_38_39_40_41_42_43_44_45_46_47_48_49_50_51_52_53_54_55_56_57_58_59_60_61_62_63_64_65_66_67_68_69_70_71_72_73_74_75_76_77_78_79_80_81_82_83_84_85_86_87_88_89_90_91_92_93_94_95_96_97_98_99_100_101_102_103_104_105_106_107_108_109_110_111_112_113_114_115_116\n",
      "Constructing IBG...\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "No index scans were explicitly noted in the query plan.\n",
      "Number of nodes in IBG: 4104\n",
      "Computing all pair degree of interaction...\n",
      "Updating statistics...\n",
      "Choosing top 19 indexes from 117 non-materialized candidate indexes\n",
      "Non-materialized candidate indexes, X = ['IX_lineorder_lo_orderdate', 'IX_lineorder_lo_discount', 'IX_lineorder_lo_quantity', 'IX_lineorder_lo_orderdate_lo_discount', 'IX_lineorder_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_orderdate', 'IX_lineorder_lo_discount_lo_quantity', 'IX_lineorder_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_discount', 'IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_lineorder_lo_discount_lo_orderdate_lo_quantity', 'IX_lineorder_lo_discount_lo_quantity_lo_orderdate', 'IX_lineorder_lo_quantity_lo_orderdate_lo_discount', 'IX_lineorder_lo_quantity_lo_discount_lo_orderdate', 'IX_dwdate_d_datekey', 'IX_dwdate_d_year', 'IX_dwdate_d_datekey_d_year', 'IX_dwdate_d_year_d_datekey', 'IX_dwdate_d_yearmonthnum', 'IX_dwdate_d_datekey_d_yearmonthnum', 'IX_dwdate_d_yearmonthnum_d_datekey', 'IX_dwdate_d_weeknuminyear', 'IX_dwdate_d_datekey_d_weeknuminyear', 'IX_dwdate_d_weeknuminyear_d_datekey', 'IX_lineorder_lo_partkey', 'IX_lineorder_lo_suppkey', 'IX_lineorder_lo_orderdate_lo_partkey', 'IX_lineorder_lo_orderdate_lo_suppkey', 'IX_lineorder_lo_partkey_lo_orderdate', 'IX_lineorder_lo_partkey_lo_suppkey', 'IX_lineorder_lo_suppkey_lo_orderdate', 'IX_lineorder_lo_suppkey_lo_partkey', 'IX_lineorder_lo_orderdate_lo_partkey_lo_suppkey', 'IX_lineorder_lo_orderdate_lo_suppkey_lo_partkey', 'IX_lineorder_lo_partkey_lo_orderdate_lo_suppkey', 'IX_lineorder_lo_partkey_lo_suppkey_lo_orderdate', 'IX_lineorder_lo_suppkey_lo_orderdate_lo_partkey', 'IX_lineorder_lo_suppkey_lo_partkey_lo_orderdate', 'IX_part_p_partkey', 'IX_part_p_category', 'IX_part_p_brand', 'IX_part_p_partkey_p_category', 'IX_part_p_partkey_p_brand', 'IX_part_p_category_p_partkey', 'IX_part_p_category_p_brand', 'IX_part_p_brand_p_partkey', 'IX_part_p_brand_p_category', 'IX_part_p_partkey_p_category_p_brand', 'IX_part_p_partkey_p_brand_p_category', 'IX_part_p_category_p_partkey_p_brand', 'IX_part_p_category_p_brand_p_partkey', 'IX_part_p_brand_p_partkey_p_category', 'IX_part_p_brand_p_category_p_partkey', 'IX_supplier_s_suppkey', 'IX_supplier_s_region', 'IX_supplier_s_suppkey_s_region', 'IX_supplier_s_region_s_suppkey', 'IX_lineorder_lo_custkey', 'IX_lineorder_lo_custkey_lo_suppkey', 'IX_lineorder_lo_custkey_lo_orderdate', 'IX_lineorder_lo_suppkey_lo_custkey', 'IX_lineorder_lo_orderdate_lo_custkey', 'IX_lineorder_lo_custkey_lo_suppkey_lo_orderdate', 'IX_lineorder_lo_custkey_lo_orderdate_lo_suppkey', 'IX_lineorder_lo_suppkey_lo_custkey_lo_orderdate', 'IX_lineorder_lo_suppkey_lo_orderdate_lo_custkey', 'IX_lineorder_lo_orderdate_lo_custkey_lo_suppkey', 'IX_lineorder_lo_orderdate_lo_suppkey_lo_custkey', 'IX_customer_c_custkey', 'IX_customer_c_region', 'IX_customer_c_nation', 'IX_customer_c_custkey_c_region', 'IX_customer_c_custkey_c_nation', 'IX_customer_c_region_c_custkey', 'IX_customer_c_region_c_nation', 'IX_customer_c_nation_c_custkey', 'IX_customer_c_nation_c_region', 'IX_customer_c_custkey_c_region_c_nation', 'IX_customer_c_custkey_c_nation_c_region', 'IX_customer_c_region_c_custkey_c_nation', 'IX_customer_c_region_c_nation_c_custkey', 'IX_customer_c_nation_c_custkey_c_region', 'IX_customer_c_nation_c_region_c_custkey', 'IX_supplier_s_nation', 'IX_supplier_s_suppkey_s_nation', 'IX_supplier_s_region_s_nation', 'IX_supplier_s_nation_s_suppkey', 'IX_supplier_s_nation_s_region', 'IX_supplier_s_suppkey_s_region_s_nation', 'IX_supplier_s_suppkey_s_nation_s_region', 'IX_supplier_s_region_s_suppkey_s_nation', 'IX_supplier_s_region_s_nation_s_suppkey', 'IX_supplier_s_nation_s_suppkey_s_region', 'IX_supplier_s_nation_s_region_s_suppkey', 'IX_customer_c_city', 'IX_customer_c_custkey_c_city', 'IX_customer_c_nation_c_city', 'IX_customer_c_city_c_custkey', 'IX_customer_c_city_c_nation', 'IX_customer_c_custkey_c_nation_c_city', 'IX_customer_c_custkey_c_city_c_nation', 'IX_customer_c_nation_c_custkey_c_city', 'IX_customer_c_nation_c_city_c_custkey', 'IX_customer_c_city_c_custkey_c_nation', 'IX_customer_c_city_c_nation_c_custkey', 'IX_supplier_s_city', 'IX_supplier_s_suppkey_s_city', 'IX_supplier_s_nation_s_city', 'IX_supplier_s_city_s_suppkey', 'IX_supplier_s_city_s_nation', 'IX_supplier_s_suppkey_s_nation_s_city', 'IX_supplier_s_suppkey_s_city_s_nation', 'IX_supplier_s_nation_s_suppkey_s_city', 'IX_supplier_s_nation_s_city_s_suppkey', 'IX_supplier_s_city_s_suppkey_s_nation', 'IX_supplier_s_city_s_nation_s_suppkey']\n",
      "19 top indexes: ['IX_lineorder_lo_orderdate_lo_discount_lo_quantity', 'IX_lineorder_lo_orderdate_lo_quantity_lo_discount', 'IX_lineorder_lo_orderdate_lo_discount', 'IX_lineorder_lo_orderdate_lo_quantity', 'IX_lineorder_lo_orderdate', 'IX_dwdate_d_yearmonthnum_d_datekey', 'IX_dwdate_d_yearmonthnum', 'IX_dwdate_d_datekey_d_yearmonthnum', 'IX_lineorder_lo_orderdate_lo_suppkey', 'IX_lineorder_lo_suppkey_lo_orderdate', 'IX_lineorder_lo_orderdate_lo_suppkey_lo_partkey', 'IX_lineorder_lo_suppkey_lo_orderdate_lo_partkey', 'IX_lineorder_lo_suppkey_lo_orderdate_lo_custkey', 'IX_lineorder_lo_orderdate_lo_suppkey_lo_custkey', 'IX_supplier_s_suppkey_s_nation_s_city', 'IX_supplier_s_suppkey_s_city_s_nation', 'IX_supplier_s_city_s_suppkey_s_nation', 'IX_supplier_s_city_s_nation_s_suppkey', 'IX_part_p_category_p_partkey_p_brand']\n",
      "New set of indexes to monitor for upcoming workload, |D| = 20\n",
      "Generating new partitions...\n",
      "Repartitioning...\n",
      "Analyzing query...\n",
      "Time taken for processing query #8: 161.76788115501404 seconds\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instantiate WFIT\n",
    "C = extract_query_indexes(qg.generate_query(14), include_cols=True)  \n",
    "S_0 = C[0:1]\n",
    "wfit = WFIT(S_0, idxCnt=20, stateCnt=1000, histSize=100)\n",
    "\n",
    "# process the workload\n",
    "for i, query in enumerate(workload):\n",
    "    print(f\"Processing query {i+1}\")\n",
    "    wfit.process_WFIT(query, verbose=True)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
