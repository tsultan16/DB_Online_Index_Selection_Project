{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAB Index Selection (v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "import IPython\n",
    "notebook_path = IPython.get_ipython().starting_dir\n",
    "target_subdirectory_path = os.path.abspath(os.path.join(os.path.dirname(notebook_path), 'PostgreSQL'))\n",
    "sys.path.append(target_subdirectory_path)\n",
    "\n",
    "from pg_utils import *\n",
    "from ssb_qgen_class import *\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lineorder': [('lo_orderkey', 'INT'), ('lo_linenumber', 'INT'), ('lo_custkey', 'INT'), ('lo_partkey', 'INT'), ('lo_suppkey', 'INT'), ('lo_orderdate', 'DATE'), ('lo_orderpriority', 'CHAR(15)'), ('lo_shippriority', 'CHAR(1)'), ('lo_quantity', 'INT'), ('lo_extendedprice', 'DECIMAL(18,2)'), ('lo_ordtotalprice', 'DECIMAL(18,2)'), ('lo_discount', 'DECIMAL(18,2)'), ('lo_revenue', 'DECIMAL(18,2)'), ('lo_supplycost', 'DECIMAL(18,2)'), ('lo_tax', 'INT'), ('lo_commitdate', 'DATE'), ('lo_shipmode', 'CHAR(10)')], 'part': [('p_partkey', 'INT'), ('p_name', 'VARCHAR(22)'), ('p_mfgr', 'CHAR(6)'), ('p_category', 'CHAR(7)'), ('p_brand', 'CHAR(9)'), ('p_color', 'VARCHAR(11)'), ('p_type', 'VARCHAR(25)'), ('p_size', 'INT'), ('p_container', 'CHAR(15)')], 'supplier': [('s_suppkey', 'INT'), ('s_name', 'CHAR(25)'), ('s_address', 'VARCHAR(25)'), ('s_city', 'CHAR(10)'), ('s_nation', 'CHAR(15)'), ('s_region', 'CHAR(12)'), ('s_phone', 'CHAR(20)')], 'customer': [('c_custkey', 'INT'), ('c_name', 'VARCHAR(25)'), ('c_address', 'VARCHAR(25)'), ('c_city', 'CHAR(10)'), ('c_nation', 'CHAR(15)'), ('c_region', 'CHAR(12)'), ('c_phone', 'CHAR(15)'), ('c_mktsegment', 'CHAR(12)')], 'dwdate': [('d_datekey', 'DATE'), ('d_date', 'CHAR(18)'), ('d_dayofweek', 'CHAR(9)'), ('d_month', 'CHAR(9)'), ('d_year', 'INT'), ('d_yearmonthnum', 'INT'), ('d_yearmonth', 'CHAR(7)'), ('d_daynuminweek', 'INT'), ('d_daynuminmonth', 'INT'), ('d_daynuminyear', 'INT'), ('d_monthnuminyear', 'INT'), ('d_weeknuminyear', 'INT'), ('d_sellingseason', 'CHAR(12)'), ('d_lastdayinweekfl', 'BIT'), ('d_lastdayinmonthfl', 'BIT'), ('d_holidayfl', 'BIT'), ('d_weekdayfl', 'BIT')]}\n",
      "{'lineorder': ['lo_orderkey', 'lo_linenumber'], 'part': ['p_partkey'], 'supplier': ['s_suppkey'], 'customer': ['c_custkey'], 'dwdate': ['d_datekey']}\n"
     ]
    }
   ],
   "source": [
    "tables, pk_columns = get_ssb_schema()\n",
    "print(tables)\n",
    "print(pk_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get table size, row count and column info\n",
    "def get_table_and_column_details():\n",
    "    ssb_schema, pk_columns = get_ssb_schema()\n",
    "    columns = {}\n",
    "    tables = {}\n",
    "    for table_name in ssb_schema:\n",
    "        conn = create_connection()\n",
    "        table_info = get_table_size_and_row_count(conn, table_name)\n",
    "        print(f\"Table name : {table_name}, table info : {table_info}\")\n",
    "        tables[table_name] =  {**table_info, **{\"pk_columns\": pk_columns[table_name]}}\n",
    "        close_connection(conn)\n",
    "        columns[table_name] = [c[0] for c in ssb_schema[table_name]]  \n",
    "         \n",
    "    return tables, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table name : lineorder, table info : {'size': 8969, 'row_count': 59986214}\n",
      "Table name : part, table info : {'size': 112, 'row_count': 800000}\n",
      "Table name : supplier, table info : {'size': 3, 'row_count': 20000}\n",
      "Table name : customer, table info : {'size': 47, 'row_count': 300000}\n",
      "Table name : dwdate, table info : {'size': 0, 'row_count': 2556}\n",
      "{'lineorder': {'size': 8969, 'row_count': 59986214, 'pk_columns': ['lo_orderkey', 'lo_linenumber']}, 'part': {'size': 112, 'row_count': 800000, 'pk_columns': ['p_partkey']}, 'supplier': {'size': 3, 'row_count': 20000, 'pk_columns': ['s_suppkey']}, 'customer': {'size': 47, 'row_count': 300000, 'pk_columns': ['c_custkey']}, 'dwdate': {'size': 0, 'row_count': 2556, 'pk_columns': ['d_datekey']}}\n",
      "{'lineorder': ['lo_orderkey', 'lo_linenumber', 'lo_custkey', 'lo_partkey', 'lo_suppkey', 'lo_orderdate', 'lo_orderpriority', 'lo_shippriority', 'lo_quantity', 'lo_extendedprice', 'lo_ordtotalprice', 'lo_discount', 'lo_revenue', 'lo_supplycost', 'lo_tax', 'lo_commitdate', 'lo_shipmode'], 'part': ['p_partkey', 'p_name', 'p_mfgr', 'p_category', 'p_brand', 'p_color', 'p_type', 'p_size', 'p_container'], 'supplier': ['s_suppkey', 's_name', 's_address', 's_city', 's_nation', 's_region', 's_phone'], 'customer': ['c_custkey', 'c_name', 'c_address', 'c_city', 'c_nation', 'c_region', 'c_phone', 'c_mktsegment'], 'dwdate': ['d_datekey', 'd_date', 'd_dayofweek', 'd_month', 'd_year', 'd_yearmonthnum', 'd_yearmonth', 'd_daynuminweek', 'd_daynuminmonth', 'd_daynuminyear', 'd_monthnuminyear', 'd_weeknuminyear', 'd_sellingseason', 'd_lastdayinweekfl', 'd_lastdayinmonthfl', 'd_holidayfl', 'd_weekdayfl']}\n"
     ]
    }
   ],
   "source": [
    "tables, columns = get_table_and_column_details()\n",
    "print(tables)\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MAB:\n",
    "\n",
    "    def __init__(self, alpha=1.0, vlambda=0.5, alpha_decay_rate=1.0, config_memory_MB=128, qoi_memory=5):\n",
    "        # define Lin UCB parameters\n",
    "        self.alpha = alpha     # UCB exploration parameter\n",
    "        self.vlambda = vlambda # regularization parameter\n",
    "        self.alpha_decay_rate = alpha_decay_rate  # decay rate for alpha\n",
    "        self.config_memory_MB = config_memory_MB  # memory budget for storing indexes\n",
    "        self.qoi_memory = qoi_memory  # how far back to look for queries of interest (QoIs)\n",
    "\n",
    "        # get all columns\n",
    "        self.tables, self.all_columns = get_table_and_column_details()\n",
    "        self.num_columns = sum([len(columns) for columns in self.all_columns.values()])\n",
    "        # drop all non clustered indices\n",
    "        conn = create_connection()\n",
    "        drop_all_indexes(conn)\n",
    "        close_connection(conn)\n",
    "\n",
    "        # get database size\n",
    "        conn = create_connection()\n",
    "        self.database_size = get_database_size(conn)\n",
    "        close_connection(conn)\n",
    "\n",
    "        # context vector dims  \n",
    "        self.context_size = self.num_columns + self.num_columns + 2  # index_columns + include_columns + derived_context\n",
    "\n",
    "        # create a mapping from column name to integer \n",
    "        self.columns_to_idx = {}\n",
    "        i = 0\n",
    "        for table_name, columns in self.all_columns.items():\n",
    "            for column in columns:\n",
    "                self.columns_to_idx[column] = i\n",
    "                i += 1\n",
    "\n",
    "        self.idx_to_columns = {v: k for k, v in self.columns_to_idx.items()}   \n",
    "\n",
    "        # initialize matrix V and vector b\n",
    "        self.V = np.identity(self.context_size) * self.vlambda\n",
    "        self.b = np.zeros(shape=(self.context_size, 1))\n",
    "        self.context_vectors = None\n",
    "        self.upper_bounds  = None\n",
    "        self.index_selection_count = defaultdict(int)\n",
    "        self.query_store = {}\n",
    "        self.selected_indices_last_round = {}\n",
    "        self.table_scan_times = defaultdict(list)\n",
    "        self.index_average_reward = defaultdict(float)\n",
    "    \n",
    "        # initialize query store\n",
    "        self.query_store = {}\n",
    "        # track current round    \n",
    "        self.current_round = 0\n",
    "\n",
    "        # create a cache for column context vectors\n",
    "        self.column_context_cache = {}\n",
    "\n",
    "        # cache for storing index stats\n",
    "        self.index_average_reward = defaultdict(float)\n",
    "        self.index_size = {}\n",
    "\n",
    "        # keep copy of indexes selected in previous round\n",
    "        self.selected_indexes_last_round = {}\n",
    "\n",
    "        # constants\n",
    "        self.MAX_INDEX_COLUMNS = 3\n",
    "        self.MAX_INCLUDE_COLUMNS = 4 \n",
    "        self.SMALL_TABLE_IGNORE = 1000\n",
    "        self.TABLE_MIN_SELECTIVITY = 0.5\n",
    "        self.MAX_INDEXES_PER_TABLE = 3\n",
    "        self.TABLE_SCAN_TIME_LENGTH = 1000\n",
    "        self.INCLUDE_COLS = False\n",
    "\n",
    "        \n",
    "    # step through a round of the MAB\n",
    "    def step_round(self, mini_workload, verbose=False):\n",
    "        self.current_round += 1\n",
    "\n",
    "        # identify new query templates from the mini workload and update stats    \n",
    "        if verbose: print(f\"Identifying new query templates from the mini workload...\")\n",
    "        self.identify_new_query_templates(mini_workload)\n",
    "\n",
    "        # select queries of interest (QoIs) from past workload and use them to extract candidate indexes, i.e. bandit arms\n",
    "        if verbose: print(f\"Selecting QoIs and extracting candidate indexes...\")\n",
    "        QoIs = self.select_queries_of_interest(mini_workload)\n",
    "        candidate_indexes = self.extract_candidate_indexes(QoIs)\n",
    "\n",
    "        # generate context vectors for each candidate index\n",
    "        if verbose: print(f\"Generating context vectors...\")\n",
    "        self.context_vectors = self.generate_context_vectors(candidate_indexes)\n",
    "\n",
    "        # select best configuration/super-arm based on C^2 LinUCB\n",
    "        if verbose: print(f\"Selecting best configuration...\")\n",
    "        selected_indexes = self.select_best_configuration(self.context_vectors, candidate_indexes)\n",
    "\n",
    "        # materialize the selected indexes\n",
    "        if verbose: print(f\"Materializing selected indexes...\")\n",
    "        self.materialize_indexes(selected_indexes)\n",
    "\n",
    "        # execute the mini workload and observe bandit arm rewards\n",
    "        if verbose: print(f\"Executing mini workload...\")\n",
    "        ...\n",
    "\n",
    "        # update the LinUCB model parameters\n",
    "        ...\n",
    "\n",
    "\n",
    "\n",
    "    # identify new query templates from the mini workload and update stats\n",
    "    def identify_new_query_templates(self, mini_workload):\n",
    "        for query in mini_workload:\n",
    "            if query.template_id not in self.query_store:\n",
    "                # add to query store\n",
    "                self.query_store[query.template_id] = query\n",
    "                self.query_store[query.template_id].frequency = 1\n",
    "                self.query_store[query.template_id].first_seen = self.current_round\n",
    "            else:\n",
    "                # update stats    \n",
    "                self.query_store[query.template_id].frequency += 1\n",
    "                self.query_store[query.template_id].query_string = query.query_string   # keep most recent query string\n",
    "            \n",
    "            self.query_store[query.template_id].last_seen = self.current_round\n",
    "\n",
    "\n",
    "    # select queries of interest (QoIs) from past workload and use them to extract candidate indexes, i.e. bandit arms\n",
    "    def select_queries_of_interest(self, mini_workload):\n",
    "        # select queries of interest (QoIs) from past workload and use them to extract candidate indexes, i.e. bandit arms\n",
    "        QoIs = []\n",
    "        for query in self.query_store.values():\n",
    "            # select queries that have been seen in the last qoi_memory rounds, excluding the current round\n",
    "            if self.current_round - query.last_seen <= self.qoi_memory and query.last_seen != self.current_round:\n",
    "                QoIs.append(query)\n",
    "\n",
    "        return QoIs        \n",
    "\n",
    "\n",
    "    # extract candidate indexes from QoIs\n",
    "    def extract_candidate_indexes(self, QoIs):\n",
    "        # extract candidate indexes from QoIs\n",
    "        candidate_indexes = {}\n",
    "        for query_object in QoIs:\n",
    "            # extract indexes from the query\n",
    "            indexes = extract_query_indexes(query_object,  self.MAX_INDEX_COLUMNS, self.INCLUDE_COLS)\n",
    "            for index in indexes:\n",
    "                if index not in candidate_indexes:\n",
    "                    candidate_indexes[index.index_id] = index\n",
    "\n",
    "        return candidate_indexes   \n",
    "\n",
    "\n",
    "    # generate context vectors for each candidate index\n",
    "    def generate_context_vectors(self, candidate_indexes):\n",
    "        # generate column context\n",
    "        column_context_vectors = self.generate_column_context(candidate_indexes)\n",
    "        # generate derived context\n",
    "        derived_context_vectors = self.generate_derived_context(candidate_indexes)\n",
    "        # concatenate column and derived context vectors\n",
    "        context_vectors = np.hstack((derived_context_vectors, column_context_vectors))\n",
    "\n",
    "        return context_vectors     \n",
    "\n",
    "\n",
    "    # generate column context vectors\n",
    "    def generate_column_context(self, candidate_indexes):\n",
    "        column_context_vectors = []\n",
    "        for index in candidate_indexes.values():\n",
    "            if index.index_id in self.column_context_cache:\n",
    "                # check if column context is already cached\n",
    "                column_context = self.column_context_cache[index.index_id]\n",
    "            else:\n",
    "                # create separate encoding segments for index columns and include columns    \n",
    "                index_column_context_vector = np.zeros(len(columns_to_idx), dtype=float)\n",
    "                include_column_context_vector = np.zeros(len(columns_to_idx), dtype=float)\n",
    "\n",
    "                for position, column in enumerate(index.index_columns):\n",
    "                    # encode index columns with exponentially decreasing weight based on position (since order matters)\n",
    "                    index_column_context_vector[self.columns_to_idx[column]] = 1 / (10**position)\n",
    "\n",
    "                for position, column in enumerate(index.include_columns):\n",
    "                    # encode include columns with uniform weights (since order doesn't matter)\n",
    "                    include_column_context_vector[self.columns_to_idx[column]] = 1\n",
    "\n",
    "                # concatenate index columns and include columns\n",
    "                column_context = np.hstack((index_column_context_vector, include_column_context_vector))    \n",
    "\n",
    "                # cache the column context\n",
    "                self.column_context_cache[index.index_id] = column_context\n",
    "\n",
    "            column_context_vectors.append(column_context)\n",
    "    \n",
    "        column_context_vectors = np.vstack(column_context_vectors)\n",
    "\n",
    "        return column_context_vectors\n",
    "\n",
    "\n",
    "    # generate derived context vectors\n",
    "    def generate_derived_context(self, candidate_indexes):\n",
    "        # get hypothetical sizes of all new candidate indexes not in the cache\n",
    "        new_indexes = [index for index in candidate_indexes.values() if index.index_id not in self.index_size]\n",
    "        if new_indexes:\n",
    "            conn = create_connection()\n",
    "            self.index_size[index.index_id] = get_hypothetical_index_sizes(conn, new_indexes)\n",
    "            close_connection(conn)    \n",
    "\n",
    "        derived_context_vectors = []\n",
    "        for index in candidate_indexes.values():\n",
    "            derived_context = np.zeros(2, dtype=float)\n",
    "            derived_context[0] = self.index_average_reward[index.index_id]\n",
    "\n",
    "            if index.index_id not in self.selected_indexes_last_round:\n",
    "                derived_context[1] = self.index_size[index.index_id] / self.database_size\n",
    "            else:\n",
    "                # if candidate index was selected in the last round, then set to 0   \n",
    "                derived_context[1] = 0\n",
    "            derived_context_vectors.append(derived_context)\n",
    "\n",
    "        derived_context_vectors = np.vstack(derived_context_vectors)\n",
    "\n",
    "        return derived_context_vectors\n",
    "\n",
    "\n",
    "    # select best configuration/super-arm using C^2 LinUCB\n",
    "    def select_best_configuration(self, context_vectors, candidate_indexes, creation_cost_reduction_factor=1):\n",
    "        # compute linUCB parameters\n",
    "        V_inv = np.linalg.inv(self.V)\n",
    "        theta = V_inv @ self.b\n",
    "        # rescale the parameter corresponding to second component of the derived context vector\n",
    "        theta[1] = theta[1]/creation_cost_reduction_factor\n",
    "        # compute expected rewards\n",
    "        expected_rewards = (context_vectors @ theta).reshape(-1)\n",
    "        # estimate upper confidence bound/variance\n",
    "        variances = self.alpha * np.sqrt(np.diag(context_vectors @ V_inv @ context_vectors.T))\n",
    "        # compute upper bounds\n",
    "        upper_bounds = expected_rewards + variances\n",
    "        # convert to dict\n",
    "        upper_bounds = {index_id: upper_bound for index_id, upper_bound in zip(candidate_indexes.keys(), upper_bounds)}\n",
    "        # solve 0-1 knapsack problem to select best configuration\n",
    "        selected_indexes = self.solve_knapsack(upper_bounds, candidate_indexes)\n",
    "\n",
    "        return selected_index\n",
    "\n",
    "\n",
    "    # greedy 1/2 approximation algorithm for knapsack problem\n",
    "    def solve_knapsack(self, upper_bounds, candidate_indexes):\n",
    "        # compute estimated reward upper bound to index size ratio\n",
    "        ratios = {index_id: upper_bound / self.index_size[index_id] for index_id, upper_bound in upper_bounds.items()}\n",
    "        # sort indexes by decreasing order of ratio\n",
    "        sorted_indexes = sorted(ratios, key=ratios.get, reverse=True)\n",
    "        # select indexes greedily to fit within memory budget\n",
    "        selected_indexes = []\n",
    "        memory_used = 0\n",
    "        for index_id in sorted_indexes:\n",
    "            if memory_used + self.index_size[index_id] <= self.config_memory_MB:\n",
    "                selected_indexes.append(candidate_indexes[index_id])\n",
    "                memory_used += self.index_size[index_id]\n",
    "            if memory_used >= self.config_memory_MB:\n",
    "                break \n",
    "\n",
    "        return selected_indexes\n",
    "\n",
    "\n",
    "    # materialize the selected indexes\n",
    "    def materialize_indexes(self, selected_indexes):\n",
    "        indexes_added = set(selected_indexes) - set(self.selected_indexes_last_round)\n",
    "        indexes_dropped = set(self.selected_indexes_last_round) - set(selected_indexes)\n",
    "\n",
    "        # drop indexes that are no longer selected\n",
    "        conn = create_connection()\n",
    "        drop_index(conn, indexes_dropped)\n",
    "        close_connection(conn)\n",
    "\n",
    "        # materialize the selected indexes\n",
    "        conn = create_connection()\n",
    "        bulk_create_indexes(conn, indexes_added)\n",
    "        close_connection(conn)\n",
    "\n",
    "        # update the cached index sizes with actual sizes\n",
    "        for index in indexes_added:\n",
    "            self.index_size[index.index_id] = index.size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
