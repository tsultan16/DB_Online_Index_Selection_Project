{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAB Index Selection (v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "import IPython\n",
    "notebook_path = IPython.get_ipython().starting_dir\n",
    "target_subdirectory_path = os.path.abspath(os.path.join(os.path.dirname(notebook_path), 'PostgreSQL'))\n",
    "sys.path.append(target_subdirectory_path)\n",
    "\n",
    "from pg_utils import *\n",
    "from ssb_qgen_class import *\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lineorder': [('lo_orderkey', 'INT'), ('lo_linenumber', 'INT'), ('lo_custkey', 'INT'), ('lo_partkey', 'INT'), ('lo_suppkey', 'INT'), ('lo_orderdate', 'DATE'), ('lo_orderpriority', 'CHAR(15)'), ('lo_shippriority', 'CHAR(1)'), ('lo_quantity', 'INT'), ('lo_extendedprice', 'DECIMAL(18,2)'), ('lo_ordtotalprice', 'DECIMAL(18,2)'), ('lo_discount', 'DECIMAL(18,2)'), ('lo_revenue', 'DECIMAL(18,2)'), ('lo_supplycost', 'DECIMAL(18,2)'), ('lo_tax', 'INT'), ('lo_commitdate', 'DATE'), ('lo_shipmode', 'CHAR(10)')], 'part': [('p_partkey', 'INT'), ('p_name', 'VARCHAR(22)'), ('p_mfgr', 'CHAR(6)'), ('p_category', 'CHAR(7)'), ('p_brand', 'CHAR(9)'), ('p_color', 'VARCHAR(11)'), ('p_type', 'VARCHAR(25)'), ('p_size', 'INT'), ('p_container', 'CHAR(15)')], 'supplier': [('s_suppkey', 'INT'), ('s_name', 'CHAR(25)'), ('s_address', 'VARCHAR(25)'), ('s_city', 'CHAR(10)'), ('s_nation', 'CHAR(15)'), ('s_region', 'CHAR(12)'), ('s_phone', 'CHAR(20)')], 'customer': [('c_custkey', 'INT'), ('c_name', 'VARCHAR(25)'), ('c_address', 'VARCHAR(25)'), ('c_city', 'CHAR(10)'), ('c_nation', 'CHAR(15)'), ('c_region', 'CHAR(12)'), ('c_phone', 'CHAR(15)'), ('c_mktsegment', 'CHAR(12)')], 'dwdate': [('d_datekey', 'DATE'), ('d_date', 'CHAR(18)'), ('d_dayofweek', 'CHAR(9)'), ('d_month', 'CHAR(9)'), ('d_year', 'INT'), ('d_yearmonthnum', 'INT'), ('d_yearmonth', 'CHAR(7)'), ('d_daynuminweek', 'INT'), ('d_daynuminmonth', 'INT'), ('d_daynuminyear', 'INT'), ('d_monthnuminyear', 'INT'), ('d_weeknuminyear', 'INT'), ('d_sellingseason', 'CHAR(12)'), ('d_lastdayinweekfl', 'BIT'), ('d_lastdayinmonthfl', 'BIT'), ('d_holidayfl', 'BIT'), ('d_weekdayfl', 'BIT')]}\n",
      "{'lineorder': ['lo_orderkey', 'lo_linenumber'], 'part': ['p_partkey'], 'supplier': ['s_suppkey'], 'customer': ['c_custkey'], 'dwdate': ['d_datekey']}\n"
     ]
    }
   ],
   "source": [
    "tables, pk_columns = get_ssb_schema()\n",
    "print(tables)\n",
    "print(pk_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get table size, row count, sequential scan time and column info\n",
    "@lru_cache(maxsize=None)\n",
    "def get_table_and_column_details():\n",
    "    ssb_schema, pk_columns = get_ssb_schema()\n",
    "    columns = {}\n",
    "    tables = {}\n",
    "    for table_name in ssb_schema:\n",
    "        conn = create_connection()\n",
    "        table_info = get_table_size_and_row_count(conn, table_name)\n",
    "        table_scan_time = get_sequential_scan_time(conn, table_name)\n",
    "        #print(f\"Table name : {table_name}, table info : {table_info}\")\n",
    "        tables[table_name] =  {**table_info, **{\"pk_columns\": pk_columns[table_name], \"sequential_scan_time\": [table_scan_time]}}\n",
    "        close_connection(conn)\n",
    "        columns[table_name] = [c[0] for c in ssb_schema[table_name]]  \n",
    "         \n",
    "    return tables, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential scan time for table 'lineorder': 12484.58 ms\n",
      "Sequential scan time for table 'part': 38.27 ms\n",
      "Sequential scan time for table 'supplier': 1.25 ms\n",
      "Sequential scan time for table 'customer': 17.69 ms\n",
      "Sequential scan time for table 'dwdate': 0.20 ms\n",
      "{'lineorder': {'size': 9367, 'row_count': 59986214, 'pk_columns': ['lo_orderkey', 'lo_linenumber'], 'sequential_scan_time': [12484.576]}, 'part': {'size': 112, 'row_count': 800000, 'pk_columns': ['p_partkey'], 'sequential_scan_time': [38.274]}, 'supplier': {'size': 3, 'row_count': 20000, 'pk_columns': ['s_suppkey'], 'sequential_scan_time': [1.255]}, 'customer': {'size': 47, 'row_count': 300000, 'pk_columns': ['c_custkey'], 'sequential_scan_time': [17.693]}, 'dwdate': {'size': 1, 'row_count': 2556, 'pk_columns': ['d_datekey'], 'sequential_scan_time': [0.197]}}\n",
      "{'lineorder': ['lo_orderkey', 'lo_linenumber', 'lo_custkey', 'lo_partkey', 'lo_suppkey', 'lo_orderdate', 'lo_orderpriority', 'lo_shippriority', 'lo_quantity', 'lo_extendedprice', 'lo_ordtotalprice', 'lo_discount', 'lo_revenue', 'lo_supplycost', 'lo_tax', 'lo_commitdate', 'lo_shipmode'], 'part': ['p_partkey', 'p_name', 'p_mfgr', 'p_category', 'p_brand', 'p_color', 'p_type', 'p_size', 'p_container'], 'supplier': ['s_suppkey', 's_name', 's_address', 's_city', 's_nation', 's_region', 's_phone'], 'customer': ['c_custkey', 'c_name', 'c_address', 'c_city', 'c_nation', 'c_region', 'c_phone', 'c_mktsegment'], 'dwdate': ['d_datekey', 'd_date', 'd_dayofweek', 'd_month', 'd_year', 'd_yearmonthnum', 'd_yearmonth', 'd_daynuminweek', 'd_daynuminmonth', 'd_daynuminyear', 'd_monthnuminyear', 'd_weeknuminyear', 'd_sellingseason', 'd_lastdayinweekfl', 'd_lastdayinmonthfl', 'd_holidayfl', 'd_weekdayfl']}\n"
     ]
    }
   ],
   "source": [
    "tables, columns = get_table_and_column_details()\n",
    "print(tables)\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAB:\n",
    "    database_size_cache = None\n",
    "    table_info_cache = None\n",
    "    column_info_cache = None\n",
    "\n",
    "    def __init__(self, alpha=1.0, vlambda=0.5, alpha_decay_rate=1.0, config_memory_MB=2048, qoi_memory=4):\n",
    "        # define Lin UCB parameters\n",
    "        self.alpha = alpha     # UCB exploration parameter\n",
    "        self.vlambda = vlambda # regularization parameter\n",
    "        self.alpha_decay_rate = alpha_decay_rate  # decay rate for alpha\n",
    "        self.config_memory_MB = config_memory_MB  # memory budget for storing indexes\n",
    "        self.qoi_memory = qoi_memory  # how far back to look for queries of interest (QoIs)\n",
    "\n",
    "        # get database size\n",
    "        if MAB.database_size_cache is None:\n",
    "            conn = create_connection()\n",
    "            MAB.database_size_cache = get_database_size(conn)\n",
    "            close_connection(conn)\n",
    "        else:\n",
    "            print(\"Found database size in cache\")    \n",
    "        self.database_size = MAB.database_size_cache    \n",
    "        print(f\"Database size: {self.database_size}\")\n",
    "        # get all tables and columns info\n",
    "        if MAB.table_info_cache is None:\n",
    "            MAB.table_info_cache, MAB.column_info_cache = get_table_and_column_details()\n",
    "        else:\n",
    "            print(\"Found table and column info in cache\")    \n",
    "        self.tables = MAB.table_info_cache\n",
    "        self.all_columns = MAB.column_info_cache    \n",
    "        self.num_columns = sum([len(columns) for columns in self.all_columns.values()])\n",
    "        print(f\"Table info: {self.tables}\")\n",
    "        print(f\"Total number of columns: {self.num_columns}\")\n",
    "\n",
    "        # drop all non clustered indices\n",
    "        print(\"Dropping all existing secondary indexes...\")\n",
    "        conn = create_connection()\n",
    "        drop_all_indexes(conn)\n",
    "        close_connection(conn)\n",
    "\n",
    "        # context vector dims  \n",
    "        self.context_size = self.num_columns + self.num_columns + 2  # index_columns + include_columns + derived_context\n",
    "        print(f\"Context vector size: {self.context_size}\")\n",
    "\n",
    "        # create a mapping from column name to integer \n",
    "        self.columns_to_idx = {}\n",
    "        i = 0\n",
    "        for table_name, columns in self.all_columns.items():\n",
    "            for column in columns:\n",
    "                self.columns_to_idx[column] = i\n",
    "                i += 1\n",
    "\n",
    "        self.idx_to_columns = {v: k for k, v in self.columns_to_idx.items()}   \n",
    "\n",
    "        # initialize matrix V and vector b\n",
    "        self.V = np.identity(self.context_size) * self.vlambda\n",
    "        self.b = np.zeros(shape=(self.context_size, 1))\n",
    "        self.context_vectors = None\n",
    "        self.upper_bounds  = None\n",
    "        self.index_selection_count = defaultdict(int)\n",
    "        self.query_store = {}\n",
    "        self.selected_indices_last_round = {}\n",
    "        self.table_scan_times = defaultdict(list)\n",
    "        self.index_average_reward = defaultdict(float)\n",
    "    \n",
    "        # initialize query store\n",
    "        self.query_store = {}\n",
    "        # track current round    \n",
    "        self.current_round = 0\n",
    "        # create a cache for column context vectors\n",
    "        self.column_context_cache = {}\n",
    "        # cache for storing index stats\n",
    "        self.index_average_reward = defaultdict(float)\n",
    "        self.index_creation_time = defaultdict(list)\n",
    "        self.index_size = {}\n",
    "        self.index_usage_rounds = defaultdict(list) # tracks which rounds an index was used in and the scan time\n",
    "        self.table_scan_times = {}\n",
    "        for table_name in self.tables.keys():\n",
    "            self.table_scan_times[table_name] = self.tables[table_name][\"sequential_scan_time\"]\n",
    "        # keep copy of indexes selected in previous round and current round candidates\n",
    "        self.selected_indexes_last_round = {}\n",
    "        self.candidate_indexes = {}\n",
    "        # track materialization time of indexes for current round\n",
    "        self.current_round_index_creation_time = {}\n",
    "\n",
    "        # constants\n",
    "        self.MAX_INDEX_COLUMNS = 3\n",
    "        self.MAX_INCLUDE_COLUMNS = 4 \n",
    "        self.SMALL_TABLE_IGNORE = 1000\n",
    "        self.TABLE_MIN_SELECTIVITY = 0.5\n",
    "        self.MAX_INDEXES_PER_TABLE = 3\n",
    "        self.TABLE_SCAN_TIME_HISTORY = 1000\n",
    "        self.INCLUDE_COLS = False\n",
    "\n",
    "        \n",
    "    # step through a round of the MAB\n",
    "    def step_round(self, mini_workload, verbose=False):\n",
    "        self.current_round += 1\n",
    "        print(f\"Current round: {self.current_round}\")\n",
    "\n",
    "        # identify new query templates from the mini workload and update stats    \n",
    "        if verbose: print(f\"Identifying new query templates from the mini workload...\")\n",
    "        self.identify_new_query_templates(mini_workload, verbose)\n",
    "\n",
    "        # select queries of interest (QoIs) from past workload and use them to extract candidate indexes, i.e. bandit arms\n",
    "        if verbose: print(f\"Selecting QoIs and extracting candidate indexes...\")\n",
    "        QoIs = self.select_queries_of_interest(mini_workload, verbose)\n",
    "        self.candidate_indexes = self.extract_candidate_indexes(QoIs, verbose)\n",
    "\n",
    "        if len(self.candidate_indexes) == 0:\n",
    "            print(f\"No candidate indexes extracted. Skipping round...\")\n",
    "            return\n",
    "            \n",
    "        # generate context vectors for each candidate index\n",
    "        if verbose: print(f\"Generating context vectors...\")\n",
    "        self.context_vectors = self.generate_context_vectors(self.candidate_indexes, False)\n",
    "\n",
    "        # select best configuration/super-arm based on C^2 LinUCB\n",
    "        if verbose: print(f\"Selecting best configuration...\")\n",
    "        selected_indexes = self.select_best_configuration(self.context_vectors, self.candidate_indexes, verbose)\n",
    "\n",
    "        # materialize the selected indexes\n",
    "        if verbose: print(f\"Materializing selected indexes...\")\n",
    "        self.materialize_indexes(selected_indexes, verbose)\n",
    "\n",
    "        # execute the mini workload and observe bandit arm rewards\n",
    "        if verbose: print(f\"Executing mini workload...\")\n",
    "        index_reward = self.execute_mini_workload(mini_workload, self.candidate_indexes, selected_indexes, verbose)\n",
    "\n",
    "        # update the LinUCB model parameters\n",
    "        if verbose: print(f\"Updating parameters...\")\n",
    "        self.update_parameters(self.candidate_indexes, index_reward, self.context_vectors, verbose)\n",
    "\n",
    "        # update selected indexes for next round\n",
    "        self.selected_indexes_last_round = selected_indexes\n",
    "\n",
    "        print(f\"\\nRound {self.current_round} completed!\")\n",
    "\n",
    "    # identify new query templates from the mini workload and update stats\n",
    "    def identify_new_query_templates(self, mini_workload, verbose):\n",
    "        for query in mini_workload:\n",
    "            if query.template_id not in self.query_store:\n",
    "                # add to query store\n",
    "                self.query_store[query.template_id] = query\n",
    "                self.query_store[query.template_id].frequency = 1\n",
    "                self.query_store[query.template_id].first_seen = self.current_round\n",
    "            else:\n",
    "                # update stats    \n",
    "                self.query_store[query.template_id].frequency += 1\n",
    "                self.query_store[query.template_id].query_string = query.query_string   # keep most recent query string\n",
    "            \n",
    "            self.query_store[query.template_id].last_seen = self.current_round\n",
    "\n",
    "        if verbose: \n",
    "            print(f\"\\tQuery Store:\")\n",
    "            for query in self.query_store.values():\n",
    "                print(f\"\\t\\tTemplate ID: {query.template_id}, Frequency: {query.frequency}, First Seen: {query.first_seen}, Last Seen: {query.last_seen}\")\n",
    "\n",
    "    # select queries of interest (QoIs) from past workload and use them to extract candidate indexes, i.e. bandit arms\n",
    "    def select_queries_of_interest(self, mini_workload, verbose):\n",
    "        # select queries of interest (QoIs) from past workload and use them to extract candidate indexes, i.e. bandit arms\n",
    "        QoIs = []\n",
    "        for query in self.query_store.values():\n",
    "            print(f\"Query template: {query.template_id}, Query last seen: {query.last_seen}\")\n",
    "            # select queries that have been seen in the last qoi_memory rounds, excluding new templates from current round\n",
    "            if self.current_round - query.last_seen <= self.qoi_memory and query.first_seen != self.current_round:\n",
    "                QoIs.append(query)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\tQueries of Interest (QoIs):\")\n",
    "            for query in QoIs:\n",
    "                print(f\"\\t\\tTemplate ID: {query.template_id}, Frequency: {query.frequency}, First Seen: {query.first_seen}, Last Seen: {query.last_seen}\")        \n",
    "\n",
    "        return QoIs        \n",
    "\n",
    "\n",
    "    # extract candidate indexes from QoIs\n",
    "    def extract_candidate_indexes(self, QoIs, verbose):\n",
    "        # extract candidate indexes from QoIs\n",
    "        candidate_indexes = {}\n",
    "        for query_object in QoIs:\n",
    "            # extract indexes from the query\n",
    "            indexes = extract_query_indexes(query_object,  self.MAX_INDEX_COLUMNS, self.INCLUDE_COLS)\n",
    "            for index in indexes:\n",
    "                if index not in candidate_indexes:\n",
    "                    candidate_indexes[index.index_id] = index\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\tCandidate Indexes:\")\n",
    "            for index in candidate_indexes.values():\n",
    "                print(f\"\\t\\tIndex ID: {index.index_id}, Index Columns: {index.index_columns}, Include Columns: {index.include_columns}\")            \n",
    "\n",
    "        return candidate_indexes   \n",
    "\n",
    "\n",
    "    # generate context vectors for each candidate index\n",
    "    def generate_context_vectors(self, candidate_indexes, verbose):\n",
    "        # generate column context\n",
    "        column_context_vectors = self.generate_column_context(candidate_indexes)\n",
    "        # generate derived context\n",
    "        derived_context_vectors = self.generate_derived_context(candidate_indexes)\n",
    "        # concatenate column and derived context vectors\n",
    "        context_vectors = np.hstack((derived_context_vectors, column_context_vectors))\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\tContext Vectors:\")\n",
    "            for index, context_vector in zip(candidate_indexes.values(), context_vectors):\n",
    "                print(f\"\\t\\tIndex ID: {index.index_id}, Context Vector: {context_vector}\")\n",
    "\n",
    "        return context_vectors     \n",
    "\n",
    "\n",
    "    # generate column context vectors\n",
    "    def generate_column_context(self, candidate_indexes):\n",
    "        column_context_vectors = []\n",
    "        for index in candidate_indexes.values():\n",
    "            if index.index_id in self.column_context_cache:\n",
    "                # check if column context is already cached\n",
    "                column_context = self.column_context_cache[index.index_id]\n",
    "            else:\n",
    "                # create separate encoding segments for index columns and include columns    \n",
    "                index_column_context_vector = np.zeros(len(self.columns_to_idx), dtype=float)\n",
    "                include_column_context_vector = np.zeros(len(self.columns_to_idx), dtype=float)\n",
    "\n",
    "                for position, column in enumerate(index.index_columns):\n",
    "                    # encode index columns with exponentially decreasing weight based on position (since order matters)\n",
    "                    index_column_context_vector[self.columns_to_idx[column]] = 1 / (10**position)\n",
    "\n",
    "                for position, column in enumerate(index.include_columns):\n",
    "                    # encode include columns with uniform weights (since order doesn't matter)\n",
    "                    include_column_context_vector[self.columns_to_idx[column]] = 1\n",
    "\n",
    "                # concatenate index columns and include columns\n",
    "                column_context = np.hstack((index_column_context_vector, include_column_context_vector))    \n",
    "\n",
    "                # cache the column context\n",
    "                self.column_context_cache[index.index_id] = column_context\n",
    "\n",
    "            column_context_vectors.append(column_context)\n",
    "    \n",
    "        column_context_vectors = np.vstack(column_context_vectors)\n",
    "\n",
    "        return column_context_vectors\n",
    "\n",
    "\n",
    "    # generate derived context vectors\n",
    "    def generate_derived_context(self, candidate_indexes):\n",
    "        # get hypothetical sizes of all new candidate indexes not in the cache\n",
    "        new_indexes = [index for index in candidate_indexes.values() if index.index_id not in self.index_size]\n",
    "        if new_indexes:\n",
    "            conn = create_connection()\n",
    "            new_index_sizes = get_hypothetical_index_sizes(conn, new_indexes)\n",
    "            close_connection(conn)    \n",
    "            for index, size in new_index_sizes.items():\n",
    "                self.index_size[index] = size\n",
    "\n",
    "        derived_context_vectors = []\n",
    "        for index in candidate_indexes.values():\n",
    "            derived_context = np.zeros(2, dtype=float)\n",
    "            derived_context[0] = self.index_average_reward[index.index_id]\n",
    "\n",
    "            if index.index_id not in self.selected_indexes_last_round:\n",
    "                derived_context[1] = self.index_size[index.index_id] / self.database_size\n",
    "            else:\n",
    "                # if candidate index was selected in the last round, then set to 0   \n",
    "                derived_context[1] = 0\n",
    "            derived_context_vectors.append(derived_context)\n",
    "\n",
    "        derived_context_vectors = np.vstack(derived_context_vectors)\n",
    "\n",
    "        return derived_context_vectors\n",
    "\n",
    "\n",
    "    # select best configuration/super-arm using C^2 LinUCB\n",
    "    def select_best_configuration(self, context_vectors, candidate_indexes, verbose, creation_time_reduction_factor=1):\n",
    "        # compute linUCB parameters\n",
    "        V_inv = np.linalg.inv(self.V)\n",
    "        theta = V_inv @ self.b\n",
    "        # rescale the weight corresponding to second component of the derived context vector, i.e. creation cost\n",
    "        # (this is useful for reducing the impact of creation cost on the selection process)\n",
    "        theta[1] = theta[1]/creation_time_reduction_factor\n",
    "        # compute expected rewards\n",
    "        expected_rewards = (context_vectors @ theta).reshape(-1)\n",
    "        # estimate upper confidence bound/variance\n",
    "        variances = self.alpha * np.sqrt(np.diag(context_vectors @ V_inv @ context_vectors.T))\n",
    "        # compute upper bounds\n",
    "        upper_bounds = expected_rewards + variances\n",
    "        # convert to dict\n",
    "        upper_bounds = {index_id: upper_bound for index_id, upper_bound in zip(candidate_indexes.keys(), upper_bounds)}\n",
    "        # solve 0-1 knapsack problem to select best configuration\n",
    "        selected_indexes = self.solve_knapsack(upper_bounds, candidate_indexes)\n",
    "        # convert to dict\n",
    "        selected_indexes = {index.index_id: index for index in selected_indexes}\n",
    "        # decay alpha\n",
    "        self.alpha = self.alpha * self.alpha_decay_rate\n",
    "\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\tUpper Bounds:\")\n",
    "            for index, upper_bound in upper_bounds.items():\n",
    "                print(f\"\\t\\tIndex ID: {index}, Upper Bound: {upper_bound}, Index size: {self.index_size[index]}\")\n",
    "\n",
    "            print(f\"\\tSelected Indexes:\")\n",
    "            print(f\"\\t\\t{list(selected_indexes.keys())}\")    \n",
    "\n",
    "        return selected_indexes\n",
    "\n",
    "\n",
    "    # greedy 1/2 approximation algorithm for knapsack problem\n",
    "    def solve_knapsack(self, upper_bounds, candidate_indexes):\n",
    "        # compute estimated reward upper bound to index size ratio\n",
    "        ratios = {index_id: upper_bound / self.index_size[index_id] for index_id, upper_bound in upper_bounds.items()}\n",
    "        # sort indexes by decreasing order of ratio\n",
    "        sorted_indexes = sorted(ratios, key=ratios.get, reverse=True)\n",
    "        # select indexes greedily to fit within memory budget\n",
    "        selected_indexes = []\n",
    "        memory_used = 0\n",
    "        for index_id in sorted_indexes:\n",
    "            if memory_used + self.index_size[index_id] <= self.config_memory_MB:\n",
    "                selected_indexes.append(candidate_indexes[index_id])\n",
    "                memory_used += self.index_size[index_id]\n",
    "            if memory_used >= self.config_memory_MB:\n",
    "                break \n",
    "\n",
    "        return selected_indexes\n",
    "\n",
    "\n",
    "    # materialize the selected indexes\n",
    "    def materialize_indexes(self, selected_indexes, verbose):\n",
    "        indexes_added = [index for index in selected_indexes.values() if index.index_id not in self.selected_indexes_last_round]\n",
    "        indexes_dropped = [index for index in self.selected_indexes_last_round.values() if index.index_id not in selected_indexes]\n",
    "        if verbose:\n",
    "            print(f\"\\tIndexes Added: {[index.index_id for index in indexes_added]}\")\n",
    "            print(f\"\\tIndexes Dropped: {[index.index_id for index in indexes_dropped]}\")\n",
    "\n",
    "        # drop indexes that are no longer selected\n",
    "        conn = create_connection()\n",
    "        bulk_drop_indexes(conn, indexes_dropped)\n",
    "        close_connection(conn)\n",
    "\n",
    "        # materialize the selected indexes\n",
    "        conn = create_connection()\n",
    "        bulk_create_indexes(conn, indexes_added)\n",
    "        close_connection(conn)\n",
    "\n",
    "        # update the cached index sizes with actual sizes\n",
    "        self.current_round_index_creation_time = {} # reset this\n",
    "        for index in indexes_added:\n",
    "            self.index_size[index.index_id] = index.size\n",
    "            self.index_creation_time[index.index_id].append((self.current_round, index.creation_time))\n",
    "            self.current_round_index_creation_time[index.index_id] = index.creation_time\n",
    "\n",
    "   \n",
    "    # execute the mini workload and observe bandit arm rewards\n",
    "    def execute_mini_workload(self, mini_workload, candidate_indexes, selected_indexes, verbose):\n",
    "        execution_info = []\n",
    "        for i,query in enumerate(mini_workload):\n",
    "            if verbose: print(f\"Executing query# {i+1}\", end='\\r', flush=True)\n",
    "            # execute the query and observe the reward\n",
    "            conn = create_connection()\n",
    "            total_execution_time, results_rows, table_access_info, index_access_info = execute_query(conn, query.query_string, with_explain=True,  return_access_info=True)\n",
    "            close_connection(conn)\n",
    "            execution_info.append((total_execution_time, table_access_info, index_access_info))\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\tExecution Info:\")\n",
    "            for i, (query, info) in enumerate(zip(mini_workload, execution_info)):\n",
    "                print(f\"\\t\\tQuery# {i+1} , Execution Time: {info[0]}, Table Access Info: {info[1]}, Index Access Info: {info[2]}\")\n",
    "\n",
    "        # extract index scan times and table sequential scan times from the execution info and shape index reward\n",
    "        index_reward = {}\n",
    "        for (total_execution_time, table_access_info, index_access_info) in execution_info:\n",
    "            for table_name, table_info in table_access_info.items():\n",
    "                self.table_scan_times[table_name].append(table_info[\"actual_total_time\"])\n",
    "\n",
    "            for index_id, index_info in index_access_info.items():\n",
    "                scan_type = index_info[\"scan_type\"]\n",
    "                # skip bitmap index scans\n",
    "                if scan_type == 'Bitmap Index Scan':\n",
    "                    # for bitmap index scans, the query plan does not provide table name so we use the table name from the actual index object \n",
    "                    table_name = candidate_indexes[index_id].table_name\n",
    "                else:\n",
    "                    table_name = index_info[\"table\"]\n",
    "    \n",
    "                index_scan_time = index_info[\"actual_total_time\"]\n",
    "                    \n",
    "                # save to index usage stats\n",
    "                self.index_usage_rounds[index_id].append((self.current_round, index_scan_time))                \n",
    "                # craft the reward signal for this index:\n",
    "                # reward = gain - creation_cost\n",
    "                # where gain = sequential scan time without index - access time with index\n",
    "                # and creation_cost = index creation time if index was created in this round otherwise 0\n",
    "                max_table_scan_time = max(self.table_scan_times[table_name][-self.TABLE_SCAN_TIME_HISTORY:])\n",
    "                gain = max_table_scan_time - index_scan_time\n",
    "                creation_time = self.current_round_index_creation_time.get(index_id, 0)\n",
    "                if verbose: print(f\"\\t\\t\\tAccumulating reward for index: {index_id}, table: {table_name}, scan type:{scan_type}, scan time: {index_scan_time}, gain: {gain}, creation time: {creation_time}\")\n",
    "                \n",
    "                # save gain and creation cost as separate components, accumulated gain over multiple uses\n",
    "                if index_id in index_reward:\n",
    "                    index_reward[index_id] = (index_reward[index_id][0] + gain, index_reward[index_id][1])\n",
    "                else:\n",
    "                    index_reward[index_id] = (gain, creation_time)    \n",
    "\n",
    "\n",
    "        # for indexes that were selected on this round but not used, set gain to 0\n",
    "        unused_indexes = set(selected_indexes.keys()) - set(index_reward.keys())\n",
    "        for index_id in unused_indexes:\n",
    "            index_reward[index_id] = (0, self.current_round_index_creation_time.get(index_id,0))\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\tIndex Rewards:\")\n",
    "            for index_id, reward in index_reward.items():\n",
    "                print(f\"\\t\\tIndex ID: {index_id}, Gain: {reward[0]}, Creation Time: {reward[1]}\")\n",
    "\n",
    "        return index_reward\n",
    "\n",
    "\n",
    "    # update the LinUCB model parameters\n",
    "    def update_parameters(self, candidate_indexes, index_reward, context_vectors, verbose):\n",
    "        for i, (index_id, index) in enumerate(candidate_indexes.items()):\n",
    "            if index.index_id in index_reward:\n",
    "                reward = index_reward[index.index_id]\n",
    "                # update moving average of index rewards\n",
    "                self.index_average_reward[index.index_id] = (self.index_average_reward[index.index_id] + reward[0])/2    \n",
    "                context_vector = context_vectors[i]\n",
    "\n",
    "                # update V and b for creation cost reward component\n",
    "                if reward[1] != 0:\n",
    "                    creation_reward_context = np.zeros_like(context_vector)\n",
    "                    creation_reward_context[1] = context_vector[1]  \n",
    "                    self.V += np.outer(creation_reward_context, creation_reward_context)   \n",
    "                    self.b += reward[1] * creation_reward_context.reshape(-1, 1)\n",
    "\n",
    "                # update V and b for gain reward component\n",
    "                if reward[0] != 0:\n",
    "                    context_vector[1] = 0\n",
    "                    gain_reward_context = context_vector\n",
    "                    self.V += np.outer(gain_reward_context, gain_reward_context)  \n",
    "                    self.b += reward[0] * gain_reward_context.reshape(-1, 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an SSB query generator object\n",
    "qg = QGEN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "miniworkload = [qg.generate_query(i) for i in [1,2,3]] * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database size: 10825\n",
      "Table info: {'lineorder': {'size': 9367, 'row_count': 59986214, 'pk_columns': ['lo_orderkey', 'lo_linenumber'], 'sequential_scan_time': [12484.576, 6897.517, 6873.624, 8057.097, 7929.826]}, 'part': {'size': 112, 'row_count': 800000, 'pk_columns': ['p_partkey'], 'sequential_scan_time': [38.274]}, 'supplier': {'size': 3, 'row_count': 20000, 'pk_columns': ['s_suppkey'], 'sequential_scan_time': [1.255]}, 'customer': {'size': 47, 'row_count': 300000, 'pk_columns': ['c_custkey'], 'sequential_scan_time': [17.693]}, 'dwdate': {'size': 1, 'row_count': 2556, 'pk_columns': ['d_datekey'], 'sequential_scan_time': [0.197]}}\n",
      "Total number of columns: 58\n",
      "Dropping all existing secondary indexes...\n",
      "Index 'ix_dwdate_d_datekey_d_yearmonthnum' on table 'dwdate' dropped successfully\n",
      "Index 'ix_dwdate_d_datekey_d_year' on table 'dwdate' dropped successfully\n",
      "Index 'ix_dwdate_d_yearmonthnum' on table 'dwdate' dropped successfully\n",
      "Index 'ix_dwdate_d_datekey_d_weeknuminyear' on table 'dwdate' dropped successfully\n",
      "Index 'ix_dwdate_d_yearmonthnum_d_datekey' on table 'dwdate' dropped successfully\n",
      "Index 'ix_lineorder_lo_discount' on table 'lineorder' dropped successfully\n",
      "Index 'ix_dwdate_d_year_d_datekey' on table 'dwdate' dropped successfully\n",
      "Index 'ix_dwdate_d_datekey' on table 'dwdate' dropped successfully\n",
      "Index 'ix_lineorder_lo_orderdate' on table 'lineorder' dropped successfully\n",
      "Index 'ix_dwdate_d_weeknuminyear' on table 'dwdate' dropped successfully\n",
      "Index 'ix_dwdate_d_weeknuminyear_d_datekey' on table 'dwdate' dropped successfully\n",
      "Index 'ix_dwdate_d_year' on table 'dwdate' dropped successfully\n",
      "Context vector size: 118\n"
     ]
    }
   ],
   "source": [
    "# test MAB\n",
    "mab = MAB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current round: 3\n",
      "Identifying new query templates from the mini workload...\n",
      "\tQuery Store:\n",
      "\t\tTemplate ID: 1, Frequency: 3, First Seen: 1, Last Seen: 3\n",
      "\t\tTemplate ID: 2, Frequency: 3, First Seen: 1, Last Seen: 3\n",
      "\t\tTemplate ID: 3, Frequency: 3, First Seen: 1, Last Seen: 3\n",
      "Selecting QoIs and extracting candidate indexes...\n",
      "Query template: 1, Query last seen: 3\n",
      "Query template: 2, Query last seen: 3\n",
      "Query template: 3, Query last seen: 3\n",
      "\tQueries of Interest (QoIs):\n",
      "\t\tTemplate ID: 1, Frequency: 3, First Seen: 1, Last Seen: 3\n",
      "\t\tTemplate ID: 2, Frequency: 3, First Seen: 1, Last Seen: 3\n",
      "\t\tTemplate ID: 3, Frequency: 3, First Seen: 1, Last Seen: 3\n",
      "\tCandidate Indexes:\n",
      "\t\tIndex ID: ix_lineorder_lo_orderdate, Index Columns: ('lo_orderdate',), Include Columns: ()\n",
      "\t\tIndex ID: ix_lineorder_lo_discount, Index Columns: ('lo_discount',), Include Columns: ()\n",
      "\t\tIndex ID: ix_lineorder_lo_quantity, Index Columns: ('lo_quantity',), Include Columns: ()\n",
      "\t\tIndex ID: ix_lineorder_lo_orderdate_lo_discount, Index Columns: ('lo_orderdate', 'lo_discount'), Include Columns: ()\n",
      "\t\tIndex ID: ix_lineorder_lo_orderdate_lo_quantity, Index Columns: ('lo_orderdate', 'lo_quantity'), Include Columns: ()\n",
      "\t\tIndex ID: ix_lineorder_lo_discount_lo_orderdate, Index Columns: ('lo_discount', 'lo_orderdate'), Include Columns: ()\n",
      "\t\tIndex ID: ix_lineorder_lo_discount_lo_quantity, Index Columns: ('lo_discount', 'lo_quantity'), Include Columns: ()\n",
      "\t\tIndex ID: ix_lineorder_lo_quantity_lo_orderdate, Index Columns: ('lo_quantity', 'lo_orderdate'), Include Columns: ()\n",
      "\t\tIndex ID: ix_lineorder_lo_quantity_lo_discount, Index Columns: ('lo_quantity', 'lo_discount'), Include Columns: ()\n",
      "\t\tIndex ID: ix_lineorder_lo_orderdate_lo_discount_lo_quantity, Index Columns: ('lo_orderdate', 'lo_discount', 'lo_quantity'), Include Columns: ()\n",
      "\t\tIndex ID: ix_lineorder_lo_orderdate_lo_quantity_lo_discount, Index Columns: ('lo_orderdate', 'lo_quantity', 'lo_discount'), Include Columns: ()\n",
      "\t\tIndex ID: ix_lineorder_lo_discount_lo_orderdate_lo_quantity, Index Columns: ('lo_discount', 'lo_orderdate', 'lo_quantity'), Include Columns: ()\n",
      "\t\tIndex ID: ix_lineorder_lo_discount_lo_quantity_lo_orderdate, Index Columns: ('lo_discount', 'lo_quantity', 'lo_orderdate'), Include Columns: ()\n",
      "\t\tIndex ID: ix_lineorder_lo_quantity_lo_orderdate_lo_discount, Index Columns: ('lo_quantity', 'lo_orderdate', 'lo_discount'), Include Columns: ()\n",
      "\t\tIndex ID: ix_lineorder_lo_quantity_lo_discount_lo_orderdate, Index Columns: ('lo_quantity', 'lo_discount', 'lo_orderdate'), Include Columns: ()\n",
      "\t\tIndex ID: ix_dwdate_d_datekey, Index Columns: ('d_datekey',), Include Columns: ()\n",
      "\t\tIndex ID: ix_dwdate_d_year, Index Columns: ('d_year',), Include Columns: ()\n",
      "\t\tIndex ID: ix_dwdate_d_datekey_d_year, Index Columns: ('d_datekey', 'd_year'), Include Columns: ()\n",
      "\t\tIndex ID: ix_dwdate_d_year_d_datekey, Index Columns: ('d_year', 'd_datekey'), Include Columns: ()\n",
      "\t\tIndex ID: ix_dwdate_d_yearmonthnum, Index Columns: ('d_yearmonthnum',), Include Columns: ()\n",
      "\t\tIndex ID: ix_dwdate_d_datekey_d_yearmonthnum, Index Columns: ('d_datekey', 'd_yearmonthnum'), Include Columns: ()\n",
      "\t\tIndex ID: ix_dwdate_d_yearmonthnum_d_datekey, Index Columns: ('d_yearmonthnum', 'd_datekey'), Include Columns: ()\n",
      "\t\tIndex ID: ix_dwdate_d_weeknuminyear, Index Columns: ('d_weeknuminyear',), Include Columns: ()\n",
      "\t\tIndex ID: ix_dwdate_d_datekey_d_weeknuminyear, Index Columns: ('d_datekey', 'd_weeknuminyear'), Include Columns: ()\n",
      "\t\tIndex ID: ix_dwdate_d_weeknuminyear_d_datekey, Index Columns: ('d_weeknuminyear', 'd_datekey'), Include Columns: ()\n",
      "Generating context vectors...\n",
      "Selecting best configuration...\n",
      "\tUpper Bounds:\n",
      "\t\tIndex ID: ix_lineorder_lo_orderdate, Upper Bound: 17143.65409425662, Index size: 397.5078125\n",
      "\t\tIndex ID: ix_lineorder_lo_discount, Upper Bound: 703.0565111976634, Index size: 1328.609375\n",
      "\t\tIndex ID: ix_lineorder_lo_quantity, Upper Bound: 703.0565111976634, Index size: 1328.609375\n",
      "\t\tIndex ID: ix_lineorder_lo_orderdate_lo_discount, Upper Bound: 9197.389713699695, Index size: 1660.7578125\n",
      "\t\tIndex ID: ix_lineorder_lo_orderdate_lo_quantity, Upper Bound: 9197.389713699695, Index size: 1660.7578125\n",
      "\t\tIndex ID: ix_lineorder_lo_discount_lo_orderdate, Upper Bound: 1710.4201383115667, Index size: 1660.7578125\n",
      "\t\tIndex ID: ix_lineorder_lo_discount_lo_quantity, Upper Bound: 878.4751842970602, Index size: 1660.7578125\n",
      "\t\tIndex ID: ix_lineorder_lo_quantity_lo_orderdate, Upper Bound: 1710.4201383115667, Index size: 1660.7578125\n",
      "\t\tIndex ID: ix_lineorder_lo_quantity_lo_discount, Upper Bound: 878.4751842970602, Index size: 1660.7578125\n",
      "\t\tIndex ID: ix_lineorder_lo_orderdate_lo_discount_lo_quantity, Upper Bound: 9372.81155894501, Index size: 1992.9140625\n",
      "\t\tIndex ID: ix_lineorder_lo_orderdate_lo_quantity_lo_discount, Upper Bound: 9372.81155894501, Index size: 1992.9140625\n",
      "\t\tIndex ID: ix_lineorder_lo_discount_lo_orderdate_lo_quantity, Upper Bound: 1885.8372700062803, Index size: 1992.9140625\n",
      "\t\tIndex ID: ix_lineorder_lo_discount_lo_quantity_lo_orderdate, Upper Bound: 1137.087207089875, Index size: 1992.9140625\n",
      "\t\tIndex ID: ix_lineorder_lo_quantity_lo_orderdate_lo_discount, Upper Bound: 1885.8372700062803, Index size: 1992.9140625\n",
      "\t\tIndex ID: ix_lineorder_lo_quantity_lo_discount_lo_orderdate, Upper Bound: 1137.087207089875, Index size: 1992.9140625\n",
      "\t\tIndex ID: ix_dwdate_d_datekey, Upper Bound: 1.4606202817877105, Index size: 0.0703125\n",
      "\t\tIndex ID: ix_dwdate_d_year, Upper Bound: 0.5190926683266628, Index size: 0.0390625\n",
      "\t\tIndex ID: ix_dwdate_d_datekey_d_year, Upper Bound: 1.4188101498202048, Index size: 0.0703125\n",
      "\t\tIndex ID: ix_dwdate_d_year_d_datekey, Upper Bound: 0.2682751308985821, Index size: 0.0703125\n",
      "\t\tIndex ID: ix_dwdate_d_yearmonthnum, Upper Bound: 0.7591945007095455, Index size: 0.0390625\n",
      "\t\tIndex ID: ix_dwdate_d_datekey_d_yearmonthnum, Upper Bound: 1.4474089754132409, Index size: 0.0703125\n",
      "\t\tIndex ID: ix_dwdate_d_yearmonthnum_d_datekey, Upper Bound: 0.7635093370011943, Index size: 0.0703125\n",
      "\t\tIndex ID: ix_dwdate_d_weeknuminyear, Upper Bound: 1.4142135623730951, Index size: 0.0390625\n",
      "\t\tIndex ID: ix_dwdate_d_datekey_d_weeknuminyear, Upper Bound: 1.467738958128459, Index size: 0.0703125\n",
      "\t\tIndex ID: ix_dwdate_d_weeknuminyear_d_datekey, Upper Bound: 1.4270805113963014, Index size: 0.0703125\n",
      "\tSelected Indexes:\n",
      "\t\t['ix_lineorder_lo_orderdate', 'ix_dwdate_d_weeknuminyear', 'ix_dwdate_d_datekey_d_weeknuminyear', 'ix_dwdate_d_datekey', 'ix_dwdate_d_datekey_d_yearmonthnum', 'ix_dwdate_d_weeknuminyear_d_datekey', 'ix_dwdate_d_datekey_d_year', 'ix_dwdate_d_yearmonthnum', 'ix_dwdate_d_year', 'ix_dwdate_d_yearmonthnum_d_datekey', 'ix_dwdate_d_year_d_datekey', 'ix_lineorder_lo_discount']\n",
      "Materializing selected indexes...\n",
      "\tIndexes Added: ['ix_lineorder_lo_discount']\n",
      "\tIndexes Dropped: []\n",
      "Successfully created index: 'ix_lineorder_lo_discount', size: 1286.8046875000000000 MB, creation time: 62999.84 ms\n",
      "Executing mini workload...\n",
      "Executing query# 2\r"
     ]
    }
   ],
   "source": [
    "# run some MAB steps\n",
    "mab.step_round(miniworkload, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
