{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Online Index Selection Via Combinatorial Contextual Multi Armed Bandits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "import uuid\n",
    "\n",
    "import pyodbc\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import re\n",
    "import json\n",
    "import itertools\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import IPython\n",
    "notebook_path = IPython.get_ipython().starting_dir\n",
    "target_subdirectory_path = os.path.abspath(os.path.join(os.path.dirname(notebook_path), 'database'))\n",
    "sys.path.append(target_subdirectory_path)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100\n"
     ]
    }
   ],
   "source": [
    "# read workload queries from JSON file\n",
    "def read_workload(workload_filepath):\n",
    "    workload = []\n",
    "    with open(workload_filepath) as f:\n",
    "        line = f.readline()\n",
    "        # read the queries from each line\n",
    "        while line:\n",
    "            workload.append(json.loads(line))\n",
    "            line = f.readline()\n",
    "\n",
    "    return workload\n",
    "\n",
    "# Base directory containing the generated queries\n",
    "workload_filepath = '../datagen/TPCH_workloads/TPCH_static_100_workload.json'\n",
    "\n",
    "# Read the workload queries from file\n",
    "workload = read_workload(workload_filepath)\n",
    "print(len(workload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAB index selection algorithm. On each round, do the following:\n",
    "\n",
    "1) Generate candidate arms/indices using mini-workload from previous round\n",
    "2) Generate context vector for each candidate arm\n",
    "3) Select the best super-arm, i.e. configuration/subset of candidate indices\n",
    "4) Materialize the super-arm configuration, then execute new mini-workload for current round\n",
    "\n",
    "\n",
    "We will implement these 4 steps separately in the given order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Generation of Candidate indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "   Index class definition\n",
    "\"\"\"\n",
    "class Index:\n",
    "    def __init__(self, table_name, index_id, index_columns, size, include_columns=(), value=None):\n",
    "        self.table_name = table_name\n",
    "        self.index_id = index_id\n",
    "        self.index_columns = index_columns\n",
    "        self.size = size\n",
    "        self.include_columns = include_columns\n",
    "        self.value = value\n",
    "        self.query_template_ids = None\n",
    "        self.clustered_index_time = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Index({self.table_name}, {self.index_id}, {self.index_columns}, {self.include_columns}, {self.size}, {self.value})\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Given a query, generate candidate indices based on the predicates and payload columns in the query.\n",
    "\"\"\"\n",
    "def generate_candidate_indices_from_predicates(connection, query, MAX_COLUMNS=6, SMALL_TABLE_IGNORE=10000, TABLE_MIN_SELECTIVITY=0.2, verbose=False):\n",
    "    # get all tables in the db\n",
    "    tables = get_all_tables(connection)\n",
    "    if verbose:\n",
    "        print(f\"Tables:\")\n",
    "        for key in tables:\n",
    "            print(tables[key])\n",
    "\n",
    "    query_template_id = query.template_id\n",
    "    query_predicates = query.predicates\n",
    "    query_payload = query.payload\n",
    "    \n",
    "    indices = {}\n",
    "\n",
    "    # indexes on predicate columns only\n",
    "    for table_name, table_predicates in query_predicates.items():\n",
    "        table = tables[table_name]\n",
    "        if verbose: print(f\"\\nTable --> {table_name}, Predicate Columns --> {set(table_predicates)}, table row count --> {table.row_count}\")\n",
    "        \n",
    "        # identify include columns\n",
    "        include_columns = []\n",
    "        if table_name in query_payload:\n",
    "            include_columns = list(set(query_payload[table_name]) - set(table_predicates))\n",
    "        \n",
    "        if verbose: \n",
    "            print(f\"Include columns: {include_columns}\")\n",
    "            print(f\"Query selectivity: {query.selectivity[table_name]}\")\n",
    "\n",
    "\n",
    "        # check if conditions for cheap full table scan are met\n",
    "        if table.row_count < SMALL_TABLE_IGNORE or ((query.selectivity[table_name] > TABLE_MIN_SELECTIVITY) and (len(include_columns)>0)):\n",
    "            if verbose: print(f\"Full table scan for table: {table_name} is cheap, skipping\")\n",
    "            continue\n",
    "\n",
    "        # generate all possible permutations of predicate columns, from single column up to MAX_COLUMNS-column indices\n",
    "        table_predicates = list(table_predicates.keys())  #[0:6]\n",
    "        col_permutations = []\n",
    "        for num_columns in range(1, min(MAX_COLUMNS, len(table_predicates)+1)):\n",
    "            col_permutations = col_permutations + list(itertools.permutations(table_predicates, num_columns)) \n",
    "        \n",
    "        if verbose: print(f\"Column permutations: \\n{col_permutations}\")\n",
    "\n",
    "        # assign an id and value to each index/column permutation\n",
    "        for cp in col_permutations:\n",
    "            index_id = get_index_id(cp, table_name)\n",
    "            \n",
    "            if index_id not in indices:\n",
    "                index_size = get_estimated_index_size(connection, table_name, cp)\n",
    "                if verbose:  print(f\"index_id: {index_id}, index columns: {cp}, index size: {index_size:.2f} Mb\")\n",
    "                # assign value...\n",
    "\n",
    "                # create index object\n",
    "                indices[index_id] = Index(table_name, index_id, cp, index_size)\n",
    "\n",
    "    # indexes on columns that are in the payload but not in the predicates\n",
    "    for table_name, table_payload in query_payload.items():\n",
    "        table = tables[table_name]\n",
    "        if verbose: print(f\"\\nTable --> {table_name}, Payload Columns --> {set(table_predicates)}, table row count --> {table.row_count}\")\n",
    "        \n",
    "        # skip if any of the payload columns for this table are in the predicates\n",
    "        if table_name in query_predicates:\n",
    "            if verbose: print(f\"Payload columns are in the predicates, skipping\")\n",
    "            continue\n",
    "\n",
    "        # check if conditions for cheap full table scan are met\n",
    "        if table.row_count < SMALL_TABLE_IGNORE:\n",
    "            if verbose: print(f\"Full table scan for table: {table_name} is cheap, skipping\")\n",
    "            continue   \n",
    "\n",
    "        # don't need to consider permutations here, just create an index with all payload columns in given order\n",
    "        index_id = get_index_id(table_payload, table_name)\n",
    "        if index_id not in indices:\n",
    "            index_size = get_estimated_index_size(connection, table_name, table_payload)\n",
    "            print(f\"index_id: {index_id}, index columns: {table_payload}, index size: {index_size:.2f} Mb\")\n",
    "            # assign value... (will assign less value to these indices as they are less useful compared to predicate indices)\n",
    "            \n",
    "            indices[index_id] = Index(table_name, index_id, table_payload, index_size)\n",
    "\n",
    "    # indexes with include columns\n",
    "    for table_name, table_predicates in query_predicates.items():\n",
    "        table = tables[table_name]\n",
    "        if verbose: print(f\"\\nTable --> {table_name}, Predicate Columns --> {set(table_predicates)}, table row count --> {table.row_count}\")\n",
    "        \n",
    "        # check if conditions for cheap full table scan are met\n",
    "        if table.row_count < SMALL_TABLE_IGNORE:\n",
    "            if verbose: print(f\"Full table scan for table: {table_name} is cheap, skipping\")\n",
    "            continue  \n",
    "\n",
    "        # identify include columns\n",
    "        include_columns = []\n",
    "        if table_name in query_payload:\n",
    "            include_columns = sorted(list(set(query_payload[table_name]) - set(table_predicates)))\n",
    "\n",
    "        if len(include_columns)>0:    \n",
    "            if verbose: print(f\"Include columns: {include_columns}\")\n",
    "\n",
    "            # generate all possible permutations of predicate columns\n",
    "            table_predicates = list(table_predicates.keys())#[0:6]\n",
    "            #col_permutations = list(itertools.permutations(table_predicates, len(table_predicates))) \n",
    "            col_permutations = list(itertools.permutations(table_predicates, MAX_COLUMNS)) \n",
    "            \n",
    "            if verbose: print(f\"Column permutations: \\n{col_permutations}\")\n",
    "\n",
    "            # assign an id and value to each index/column permutation\n",
    "            for cp in col_permutations:\n",
    "                index_id = get_index_id(cp, table_name, include_columns)\n",
    "                if index_id not in indices:\n",
    "                    index_size = get_estimated_index_size(connection, table_name, list(cp) + include_columns)\n",
    "                    if verbose: print(f\"index_id: {index_id}, index columns: {cp}, include columns: {include_columns}, index size: {index_size:.2f} Mb\")\n",
    "                    # assign value...\n",
    "                    \n",
    "                    # create index object\n",
    "                    indices[index_id] = Index(table_name, index_id, cp, index_size, tuple(include_columns))\n",
    "            \n",
    "    return indices        \n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Given a miniworkload, which is a list of query objects, generate candidate indices\n",
    "\"\"\"\n",
    "def generate_candidate_indices(connection, miniworkload, verbose=False):\n",
    "    print(f\"Gnereting candidate indices for {len(miniworkload)} queries...\")\n",
    "    index_arms = {} \n",
    "    for query in tqdm(miniworkload, desc=\"Processing queries\"):\n",
    "        query_candidate_indices = generate_candidate_indices_from_predicates(connection, query, verbose=verbose)\n",
    "        for index_id, index in query_candidate_indices.items():\n",
    "            if index_id not in index_arms:\n",
    "                # initialization\n",
    "                index.query_template_ids = set()\n",
    "                index.clustered_index_time = 0\n",
    "                index_arms[index_id] = index\n",
    "\n",
    "            # add the maximum table scan time for the table associated with this index and query template\n",
    "            index_arms[index_id].clustered_index_time += max(query.table_scan_times[index.table_name] if query.table_scan_times[index.table_name] else 0)   \n",
    "            index_arms[index_id].query_template_ids.add(query.template_id)\n",
    "\n",
    "    return index_arms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test index generation for miniworkload of first 21 queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|██████████| 21/21 [00:00<00:00, 8198.10it/s]\n"
     ]
    }
   ],
   "source": [
    "connection = start_connection()\n",
    "\n",
    "miniworkload = []\n",
    "for query in workload[0:21]:\n",
    "    # convert to Query object\n",
    "    miniworkload.append(Query(connection, query['template_id'], query['query_string'], query['payload'], query['predicates'], query['order_bys']))\n",
    "\n",
    "# genete candidate indices\n",
    "index_arms = generate_candidate_indices(connection, miniworkload, verbose=False)\n",
    "\n",
    "close_connection(connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Generation of Context Vectors for Each Arm/Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
