{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Online Index Selection Via Combinatorial Contextual Multi Armed Bandits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "import uuid\n",
    "\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import re\n",
    "import json\n",
    "import itertools\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import IPython\n",
    "notebook_path = IPython.get_ipython().starting_dir\n",
    "target_subdirectory_path = os.path.abspath(os.path.join(os.path.dirname(notebook_path), 'database'))\n",
    "sys.path.append(target_subdirectory_path)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100\n"
     ]
    }
   ],
   "source": [
    "# read workload queries from JSON file\n",
    "def read_workload(workload_filepath):\n",
    "    workload = []\n",
    "    with open(workload_filepath) as f:\n",
    "        line = f.readline()\n",
    "        # read the queries from each line\n",
    "        while line:\n",
    "            workload.append(json.loads(line))\n",
    "            line = f.readline()\n",
    "\n",
    "    return workload\n",
    "\n",
    "# Base directory containing the generated queries\n",
    "workload_filepath = '../datagen/TPCH_workloads/TPCH_static_100_workload.json'\n",
    "\n",
    "# Read the workload queries from file\n",
    "workload = read_workload(workload_filepath)\n",
    "print(len(workload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAB index selection algorithm. On each round, do the following:\n",
    "\n",
    "1) Generate candidate arms/indices using mini-workload from previous round\n",
    "2) Generate context vector for each candidate arm\n",
    "3) Select the best super-arm, i.e. configuration/subset of candidate indices\n",
    "4) Materialize the super-arm configuration, then execute new mini-workload for current round\n",
    "\n",
    "\n",
    "We will implement these 4 steps separately in the given order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Generation of Candidate indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "   Index class definition\n",
    "\"\"\"\n",
    "class Index:\n",
    "    def __init__(self, table_name, index_id, index_columns, size, include_columns=(), value=None, payload_only=False):\n",
    "        self.table_name = table_name\n",
    "        self.index_id = index_id\n",
    "        self.index_columns = index_columns\n",
    "        self.size = size\n",
    "        self.include_columns = include_columns\n",
    "        self.value = value\n",
    "        self.query_template_ids = None\n",
    "        self.clustered_index_time = None\n",
    "        self.context_vector_columns = None\n",
    "        self.payload_only = payload_only\n",
    "        self.index_usage_last = 0 \n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Index({self.table_name}, {self.index_id}, {self.index_columns}, {self.include_columns}, {self.size}, {self.value})\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Given a query, generate candidate indices based on the predicates and payload columns in the query.\n",
    "\"\"\"\n",
    "def generate_candidate_indices_from_predicates(connection, query, MAX_COLUMNS=6, SMALL_TABLE_IGNORE=10000, TABLE_MIN_SELECTIVITY=0.2, verbose=False):\n",
    "    # get all tables in the db\n",
    "    tables = get_all_tables(connection)\n",
    "    if verbose:\n",
    "        print(f\"Tables:\")\n",
    "        for key in tables:\n",
    "            print(tables[key])\n",
    "\n",
    "    query_template_id = query.template_id\n",
    "    query_predicates = query.predicates\n",
    "    query_payload = query.payload\n",
    "    \n",
    "    indices = {}\n",
    "\n",
    "    # indexes on predicate columns only\n",
    "    for table_name, table_predicates in query_predicates.items():\n",
    "        table = tables[table_name]\n",
    "        if verbose: print(f\"\\nTable --> {table_name}, Predicate Columns --> {set(table_predicates)}, table row count --> {table.row_count}\")\n",
    "        \n",
    "        # identify include columns\n",
    "        include_columns = []\n",
    "        if table_name in query_payload:\n",
    "            include_columns = list(set(query_payload[table_name]) - set(table_predicates))\n",
    "        \n",
    "        if verbose: \n",
    "            print(f\"Include columns: {include_columns}\")\n",
    "            print(f\"Query selectivity: {query.selectivity[table_name]}\")\n",
    "\n",
    "\n",
    "        # check if conditions for cheap full table scan are met\n",
    "        if table.row_count < SMALL_TABLE_IGNORE or ((query.selectivity[table_name] > TABLE_MIN_SELECTIVITY) and (len(include_columns)>0)):\n",
    "            if verbose: print(f\"Full table scan for table: {table_name} is cheap, skipping\")\n",
    "            continue\n",
    "\n",
    "        # generate all possible permutations of predicate columns, from single column up to MAX_COLUMNS-column indices\n",
    "        table_predicates = list(table_predicates.keys())  #[0:6]\n",
    "        col_permutations = []\n",
    "        for num_columns in range(1, min(MAX_COLUMNS, len(table_predicates)+1)):\n",
    "            col_permutations = col_permutations + list(itertools.permutations(table_predicates, num_columns)) \n",
    "        \n",
    "        if verbose: print(f\"Column permutations: \\n{col_permutations}\")\n",
    "\n",
    "        # assign an id and value to each index/column permutation\n",
    "        for cp in col_permutations:\n",
    "            index_id = get_index_id(cp, table_name)\n",
    "            \n",
    "            if index_id not in indices:\n",
    "                index_size = get_estimated_index_size(connection, table_name, cp)\n",
    "                if verbose:  print(f\"index_id: {index_id}, index columns: {cp}, index size: {index_size:.2f} Mb\")\n",
    "                # assign value...\n",
    "\n",
    "                # create index object\n",
    "                indices[index_id] = Index(table_name, index_id, cp, index_size)\n",
    "\n",
    "    # indexes on columns that are in the payload but not in the predicates\n",
    "    for table_name, table_payload in query_payload.items():\n",
    "        table = tables[table_name]\n",
    "        if verbose: print(f\"\\nTable --> {table_name}, Payload Columns --> {set(table_predicates)}, table row count --> {table.row_count}\")\n",
    "        \n",
    "        # skip if any of the payload columns for this table are in the predicates\n",
    "        if table_name in query_predicates:\n",
    "            if verbose: print(f\"Payload columns are in the predicates, skipping\")\n",
    "            continue\n",
    "\n",
    "        # check if conditions for cheap full table scan are met\n",
    "        if table.row_count < SMALL_TABLE_IGNORE:\n",
    "            if verbose: print(f\"Full table scan for table: {table_name} is cheap, skipping\")\n",
    "            continue   \n",
    "\n",
    "        # don't need to consider permutations here, just create an index with all payload columns in given order\n",
    "        index_id = get_index_id(table_payload, table_name)\n",
    "        if index_id not in indices:\n",
    "            index_size = get_estimated_index_size(connection, table_name, table_payload)\n",
    "            print(f\"index_id: {index_id}, index columns: {table_payload}, index size: {index_size:.2f} Mb\")\n",
    "            # assign value... (will assign less value to these indices as they are less useful compared to predicate indices)\n",
    "            \n",
    "            indices[index_id] = Index(table_name, index_id, table_payload, index_size, payload_only=True)\n",
    "\n",
    "    # indexes with include columns\n",
    "    for table_name, table_predicates in query_predicates.items():\n",
    "        table = tables[table_name]\n",
    "        if verbose: print(f\"\\nTable --> {table_name}, Predicate Columns --> {set(table_predicates)}, table row count --> {table.row_count}\")\n",
    "        \n",
    "        # check if conditions for cheap full table scan are met\n",
    "        if table.row_count < SMALL_TABLE_IGNORE:\n",
    "            if verbose: print(f\"Full table scan for table: {table_name} is cheap, skipping\")\n",
    "            continue  \n",
    "\n",
    "        # identify include columns\n",
    "        include_columns = []\n",
    "        if table_name in query_payload:\n",
    "            include_columns = sorted(list(set(query_payload[table_name]) - set(table_predicates)))\n",
    "\n",
    "        if len(include_columns)>0:    \n",
    "            if verbose: print(f\"Include columns: {include_columns}\")\n",
    "\n",
    "            # generate all possible permutations of predicate columns\n",
    "            table_predicates = list(table_predicates.keys())#[0:6]\n",
    "            #col_permutations = list(itertools.permutations(table_predicates, len(table_predicates))) \n",
    "            col_permutations = list(itertools.permutations(table_predicates, MAX_COLUMNS)) \n",
    "            \n",
    "            if verbose: print(f\"Column permutations: \\n{col_permutations}\")\n",
    "\n",
    "            # assign an id and value to each index/column permutation\n",
    "            for cp in col_permutations:\n",
    "                index_id = get_index_id(cp, table_name, include_columns)\n",
    "                if index_id not in indices:\n",
    "                    index_size = get_estimated_index_size(connection, table_name, list(cp) + include_columns)\n",
    "                    if verbose: print(f\"index_id: {index_id}, index columns: {cp}, include columns: {include_columns}, index size: {index_size:.2f} Mb\")\n",
    "                    # assign value...\n",
    "                    \n",
    "                    # create index object\n",
    "                    indices[index_id] = Index(table_name, index_id, cp, index_size, tuple(include_columns))\n",
    "            \n",
    "    return indices        \n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Given a miniworkload, which is a list of query objects, generate candidate indices\n",
    "\"\"\"\n",
    "def generate_candidate_indices(connection, miniworkload, verbose=False):\n",
    "    print(f\"Gnereting candidate indices for {len(miniworkload)} queries...\")\n",
    "    index_arms = {} \n",
    "    for query in tqdm(miniworkload, desc=\"Processing queries\"):\n",
    "        query_candidate_indices = generate_candidate_indices_from_predicates(connection, query, verbose=verbose)\n",
    "        for index_id, index in query_candidate_indices.items():\n",
    "            if index_id not in index_arms:\n",
    "                # initialization\n",
    "                index.query_template_ids = set()\n",
    "                index.clustered_index_time = 0\n",
    "                index_arms[index_id] = index\n",
    "\n",
    "            # add the maximum table scan time for the table associated with this index and query template\n",
    "            index_arms[index_id].clustered_index_time += max(query.table_scan_times[index.table_name] if query.table_scan_times[index.table_name] else 0)   \n",
    "            index_arms[index_id].query_template_ids.add(query.template_id)\n",
    "\n",
    "    return index_arms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test index generation for miniworkload of first 21 queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gnereting candidate indices for 21 queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|██████████| 21/21 [00:01<00:00, 17.55it/s]\n"
     ]
    }
   ],
   "source": [
    "connection = start_connection()\n",
    "\n",
    "miniworkload = []\n",
    "for query in workload[0:21]:\n",
    "    # convert to Query object\n",
    "    miniworkload.append(Query(connection, query['template_id'], query['query_string'], query['payload'], query['predicates'], query['order_bys']))\n",
    "\n",
    "# genete candidate indices\n",
    "index_arms = generate_candidate_indices(connection, miniworkload, verbose=False)\n",
    "\n",
    "close_connection(connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Generation of Context Vectors for Each Arm/Index\n",
    "\n",
    "The context vector of each index can be defined as a concatenation of two pieces:\n",
    "\n",
    "* Columns Piece:  a vector with length equal to the total number of columns in the database. Each entry in this vector corresponds to one of the columns and contains the value $10^{-j}$ where $j$ is the position of that column in the index, provided that column is in the index, otherwise the value is zero. \n",
    "\n",
    "* Derived Context Piece: a vector of length 2, first component contains time stamp of last round when the index was used and second component is the size of the index relative to the entire database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = start_connection()\n",
    "\n",
    "all_columns, num_columns = get_all_columns(connection)\n",
    "\n",
    "close_connection(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_idx = {}\n",
    "i = 0\n",
    "for table_name, columns in all_columns.items():\n",
    "    for column in columns:\n",
    "        columns_to_idx[column] = i\n",
    "        i += 1\n",
    "\n",
    "idx_to_columns = {v: k for k, v in columns_to_idx.items()}       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate columns piece\n",
    "def generate_context_vector_columns_index(index, columns_to_idx, idx_to_columns):\n",
    "    # return the cached context vector if available\n",
    "    if index.context_vector_columns:\n",
    "        return index.context_vector_columns\n",
    "\n",
    "    context_vector = np.zeros(len(columns_to_idx), dtype=float)\n",
    "    for j, column in enumerate(index.index_columns):\n",
    "        context_vector[columns_to_idx[column]] = 10**(-j)\n",
    "\n",
    "    # cache the context vector\n",
    "    index.encode_context_vector = context_vector    \n",
    "\n",
    "    return context_vector    \n",
    "\n",
    "\n",
    "def generate_context_vector_columns(index_arms, columns_to_idx, idx_to_columns):\n",
    "    # stack up the context vectors for all indices into a single matrix\n",
    "    context_vectors = np.vstack([generate_context_vector_columns_index(index, columns_to_idx, idx_to_columns) for index in index_arms.values()])\n",
    "\n",
    "    return context_vectors\n",
    "\n",
    "\n",
    "# generate derived piece\n",
    "def generate_context_vector_derived(connection, index_arms):\n",
    "    database_size = get_database_size(connection)\n",
    "    derived_context_vectors = np.zeros((len(index_arms), 2), dtype=float)\n",
    "    for i, index in enumerate(index_arms.values()):\n",
    "        derived_context_vectors[i,0] =  index.index_usage_last\n",
    "        derived_context_vectors[i,1] =  index.size/database_size\n",
    "    \n",
    "    return derived_context_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(652, 61)\n",
      "(652, 2)\n"
     ]
    }
   ],
   "source": [
    "connection = start_connection()\n",
    "\n",
    "# test context vector generation\n",
    "context_vectors_columns = generate_context_vector_columns(index_arms, columns_to_idx, idx_to_columns)\n",
    "print(context_vectors_columns.shape)\n",
    "\n",
    "context_vectors_derived = generate_context_vector_derived(connection, index_arms)\n",
    "print(context_vectors_derived.shape)\n",
    "\n",
    "close_connection(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
