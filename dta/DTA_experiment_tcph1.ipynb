{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating index selection recommendations via MS SQL Database Tuning Adviser (DTA)\n",
    "\n",
    "We will use DTA to generate index recommendations on some sample TPC-H OLAP workloads. \n",
    "\n",
    "Given a workload containing N queries, we will split it up into m rounds (for simplicity we will split up evenly into disjoint subsets, we can also do overlapping). \n",
    "\n",
    "* Experiment 1: On each round, we will execute the queries in that round and measure performance.\n",
    "\n",
    "* Experiment 2: On each round, we will first use DTA to obtain recommendations, implement those recommendation (i.e. create/drop indices etc.), then execute the queries in that round and measure performance. To generate recommendations, we will use all queries that have been seen up to and including queries in the current round.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "import uuid\n",
    "\n",
    "import pyodbc\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import xml.etree.ElementTree as ET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    }
   ],
   "source": [
    "def read_sql_files(base_dir, instances_per_template):\n",
    "    queries = []\n",
    "    \n",
    "    # Regex to find erroneous \"where rownum <=\" lines\n",
    "    erroneous_line_pattern = re.compile(r'^\\s*where\\s+rownum\\s*<=\\s*\\d+\\s*;\\s*$', re.IGNORECASE)\n",
    "\n",
    "    # Loop through each query directory\n",
    "    for query_dir in sorted(os.listdir(base_dir)):\n",
    "        query_path = os.path.join(base_dir, query_dir)\n",
    "        \n",
    "        if os.path.isdir(query_path):\n",
    "            # Initialize a counter for the number of instances read from this template\n",
    "            instance_count = 0\n",
    "            \n",
    "            # Loop through each SQL file in the query directory\n",
    "            for sql_file in sorted(os.listdir(query_path)):\n",
    "                if instance_count >= instances_per_template:\n",
    "                    break  # Stop reading more files from this template\n",
    "                \n",
    "                sql_file_path = os.path.join(query_path, sql_file)\n",
    "                \n",
    "                if sql_file_path.endswith('.sql'):\n",
    "                    with open(sql_file_path, 'r') as file:\n",
    "                        lines = file.readlines()\n",
    "                        \n",
    "                        # Filter out the erroneous \"where rownum <=\" lines\n",
    "                        filtered_lines = [line for line in lines if not erroneous_line_pattern.match(line)]\n",
    "                        \n",
    "                        # Extract the query from the filtered lines\n",
    "                        query = ''.join(filtered_lines[3:]).strip()\n",
    "                        \n",
    "                        queries.append(query)\n",
    "                        instance_count += 1  # Increment the counter\n",
    "    \n",
    "    return queries\n",
    "\n",
    "\n",
    "# Base directory containing the generated queries\n",
    "base_dir = '../TPCH_generated_queries'\n",
    "\n",
    "# Read the SQL files and store the queries in a list\n",
    "queries = read_sql_files(base_dir, instances_per_template=5)\n",
    "\n",
    "# shuffle the queries\n",
    "random.shuffle(queries)\n",
    "\n",
    "print(len(queries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a workload file with all the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"workload_filename = 'workload_tpch_20.sql'\\n\\n# Write the queries to file\\nwith open(workload_filename, 'w') as file:\\n    for query in queries:\\n        file.write(query + '\\n\\n')\\n        \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"workload_filename = 'workload_tpch_20.sql'\n",
    "\n",
    "# Write the queries to file\n",
    "with open(workload_filename, 'w') as file:\n",
    "    for query in queries:\n",
    "        file.write(query + '\\n\\n')\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define DTA recommender class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conn_str = (\\n    \"Driver={ODBC Driver 17 for SQL Server};\"\\n    \"Server=172.16.6.196,1433;\"  # Use the IP address and port directly\\n    \"Database=TPCH1;\"  \\n    \"UID=wsl;\" \\n    \"PWD=greatpond501;\"  \\n)\\n\\nconn = pyodbc.connect(conn_str)\\ncursor = conn.cursor()\\n# test the connection\\nprint(cursor.execute(\"SELECT @@version;\"))'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"conn_str = (\n",
    "    \"Driver={ODBC Driver 17 for SQL Server};\"\n",
    "    \"Server=172.16.6.196,1433;\"  # Use the IP address and port directly\n",
    "    \"Database=TPCH1;\"  \n",
    "    \"UID=wsl;\" \n",
    "    \"PWD=greatpond501;\"  \n",
    ")\n",
    "\n",
    "conn = pyodbc.connect(conn_str)\n",
    "cursor = conn.cursor()\n",
    "# test the connection\n",
    "print(cursor.execute(\"SELECT @@version;\"))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Code originally from Malinga Perera's work \"\"\"\n",
    "class QueryPlan:\n",
    "    def __init__(self, xml_string):\n",
    "        self.estimated_rows = 0\n",
    "        self.est_statement_sub_tree_cost = 0\n",
    "        self.elapsed_time = 0\n",
    "        self.cpu_time = 0\n",
    "        self.non_clustered_index_usage = []\n",
    "        self.clustered_index_usage = []\n",
    "\n",
    "        ns = {'sp': 'http://schemas.microsoft.com/sqlserver/2004/07/showplan'}\n",
    "        root = ET.fromstring(xml_string)\n",
    "        stmt_simple = root.find('.//sp:StmtSimple', ns)\n",
    "        if stmt_simple is not None:\n",
    "            self.estimated_rows = float(stmt_simple.attrib.get('StatementEstRows', 0))\n",
    "            self.est_statement_sub_tree_cost = float(stmt_simple.attrib.get('StatementSubTreeCost', 0))\n",
    "\n",
    "        query_stats = root.find('.//sp:QueryTimeStats', ns)\n",
    "        if query_stats is not None:\n",
    "            self.cpu_time = float(query_stats.attrib.get('CpuTime', 0))\n",
    "            self.elapsed_time = float(query_stats.attrib.get('ElapsedTime', 0)) / 1000\n",
    "\n",
    "        rel_ops = root.findall('.//sp:RelOp', ns)\n",
    "        total_po_sub_tree_cost = 0\n",
    "        total_po_actual = 0\n",
    "\n",
    "        for rel_op in rel_ops:\n",
    "            temp_act_elapsed_time = 0\n",
    "            if rel_op.attrib.get('PhysicalOp') in {'Index Seek', 'Index Scan', 'Clustered Index Scan', 'Clustered Index Seek'}:\n",
    "                total_po_sub_tree_cost += float(rel_op.attrib.get('EstimatedTotalSubtreeCost', 0))\n",
    "                runtime_thread_information = rel_op.findall('.//sp:RunTimeCountersPerThread', ns)\n",
    "                for thread_info in runtime_thread_information:\n",
    "                    temp_act_elapsed_time = max(\n",
    "                        int(thread_info.attrib.get('ActualElapsedms', 0)), temp_act_elapsed_time)\n",
    "                total_po_actual += temp_act_elapsed_time / 1000\n",
    "\n",
    "        for rel_op in rel_ops:\n",
    "            rows_read = 0\n",
    "            act_rel_op_elapsed_time = 0\n",
    "            if rel_op.attrib.get('PhysicalOp') in {'Index Seek', 'Index Scan', 'Clustered Index Scan', 'Clustered Index Seek'}:\n",
    "                runtime_thread_information = rel_op.findall('.//sp:RunTimeCountersPerThread', ns)\n",
    "                for thread_info in runtime_thread_information:\n",
    "                    rows_read += int(thread_info.attrib.get('ActualRowsRead', 0))\n",
    "                    act_rel_op_elapsed_time = max(int(thread_info.attrib.get('ActualElapsedms', 0)), act_rel_op_elapsed_time)\n",
    "            act_rel_op_elapsed_time = act_rel_op_elapsed_time / 1000\n",
    "            if rows_read == 0:\n",
    "                rows_read = float(rel_op.attrib.get('EstimatedRowsRead', 0))\n",
    "            rows_output = float(rel_op.attrib.get('EstimateRows', 0))\n",
    "            if rel_op.attrib.get('PhysicalOp') in {'Index Seek', 'Index Scan'}:\n",
    "                po_index_scan = rel_op.find('.//sp:IndexScan', ns)\n",
    "                if po_index_scan is not None:\n",
    "                    po_index = po_index_scan.find('.//sp:Object', ns).attrib.get('Index', '').strip(\"[]\")\n",
    "                    self.non_clustered_index_usage.append(\n",
    "                        (po_index, act_rel_op_elapsed_time, self.cpu_time, self.est_statement_sub_tree_cost, rows_read, rows_output))\n",
    "            elif rel_op.attrib.get('PhysicalOp') in {'Clustered Index Scan', 'Clustered Index Seek'}:\n",
    "                po_index_scan = rel_op.find('.//sp:IndexScan', ns)\n",
    "                if po_index_scan is not None:\n",
    "                    table = po_index_scan.find('.//sp:Object', ns).attrib.get('Table', '').strip(\"[]\")\n",
    "                    self.clustered_index_usage.append(\n",
    "                        (table, act_rel_op_elapsed_time, self.cpu_time, self.est_statement_sub_tree_cost, rows_read, rows_output))\n",
    "                    \n",
    "\n",
    "# more sophisticated query execution metrics        \n",
    "def execute_query(query, connection, cost_type='elapsed_time', verbose=False):\n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "        # clear cache\n",
    "        cursor.execute(\"DBCC DROPCLEANBUFFERS\")\n",
    "        # enable statistics collection\n",
    "        cursor.execute(\"SET STATISTICS XML ON\")\n",
    "        # execute the query\n",
    "        cursor.execute(query)\n",
    "        cursor.nextset()\n",
    "        # fetch execution stats\n",
    "        stat_xml = cursor.fetchone()[0]\n",
    "        cursor.execute(\"SET STATISTICS XML OFF\")\n",
    "        # parse query plan\n",
    "        query_plan = QueryPlan(stat_xml)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"QUERY: \\n{query}\\n\")\n",
    "            print(f\"ELAPSED TIME: \\n{query_plan.elapsed_time}\\n\")\n",
    "            print(f\"CPU TIME: \\n{query_plan.cpu_time}\\n\")\n",
    "            print(f\"SUBTREE COST: \\n{query_plan.est_statement_sub_tree_cost}\\n\")\n",
    "            print(f\"NON CLUSTERED INDEX USAGE: \\n{query_plan.non_clustered_index_usage}\\n\")\n",
    "            print(f\"CLUSTERED INDEX USAGE: \\n{query_plan.clustered_index_usage}\\n\")\n",
    "\n",
    "        # Determine the cost type and return the appropriate metric\n",
    "        if cost_type == 'elapsed_time':\n",
    "            return float(query_plan.elapsed_time), query_plan.non_clustered_index_usage, query_plan.clustered_index_usage\n",
    "        elif cost_type == 'cpu_time':\n",
    "            return float(query_plan.cpu_time), query_plan.non_clustered_index_usage, query_plan.clustered_index_usage\n",
    "        elif cost_type == 'sub_tree_cost':\n",
    "            return float(query_plan.est_statement_sub_tree_cost), query_plan.non_clustered_index_usage, query_plan.clustered_index_usage\n",
    "        else:\n",
    "            return float(query_plan.est_statement_sub_tree_cost), query_plan.non_clustered_index_usage, query_plan.clustered_index_usage\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Exception when executing query: {query}, Error: {e}\")\n",
    "        return-1, [], []\n",
    "    finally:\n",
    "        cursor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTA_recommender:\n",
    "    def __init__(self, queries, invoke_ta_rounds, verbose=False):\n",
    "        self.queries = queries # list of queries to be used as workload\n",
    "        self.invoke_ta_rounds = invoke_ta_rounds # list specifying the rounds to invoke TA\n",
    "        self.verbose = verbose\n",
    "        self.conn_string = (\"Driver={ODBC Driver 17 for SQL Server};\"\n",
    "                            \"Server=172.16.6.196,1433;\"  # Use the IP address and port directly\n",
    "                            \"Database=TPCH1;\"  \n",
    "                            \"UID=wsl;\" \n",
    "                            \"PWD=greatpond501;\")\n",
    "        \n",
    "        self.server = \"172.16.6.196,1433\"\n",
    "        self.database = \"TPCH1\"\n",
    "        self.username = \"wsl\"\n",
    "        self.password = \"greatpond501\"\n",
    "\n",
    "        \n",
    "    def run_dta(self, num_rounds=1, invoke_DTA=True, clear_indexes_start=False, clear_indexes_end=True):\n",
    "\n",
    "        # establish connection to the database\n",
    "        self.conn = pyodbc.connect(self.conn_string)\n",
    "\n",
    "        # clear all non-clustered indexes at the start\n",
    "        if clear_indexes_start:   \n",
    "            self.remove_all_nonclustered_indexes()\n",
    "  \n",
    "        if num_rounds > 0:\n",
    "\n",
    "            # reset workload file\n",
    "            self.workload_file = \"workload_tpch1.sql\"\n",
    "            open(self.workload_file, 'w').close()\n",
    "\n",
    "            num_queries_per_round = len(self.queries) // num_rounds\n",
    "\n",
    "            # iterate over rounds\n",
    "            counter = 0\n",
    "            for i in range(num_rounds):\n",
    "                print(f\"Round {i+1} of {num_rounds}\")\n",
    "                current_round_queries = self.queries[i*num_queries_per_round:(i+1)*num_queries_per_round]\n",
    "                \n",
    "                if invoke_DTA:\n",
    "                    # write the queries for current round to the workload file\n",
    "                    with open(self.workload_file, 'a+') as file:\n",
    "                        for query in current_round_queries:\n",
    "                            # exclude queries with \"view\" in them, otherwise DTA will throw a syntax error\n",
    "                            if \"view\" not in query.lower():\n",
    "                                file.write(query)\n",
    "                                file.write('\\n\\n\\n')\n",
    "                                counter += 1\n",
    "\n",
    "                    print(f\"{counter} queries written to workload file\")\n",
    "\n",
    "                    # invoke DTA if current round is in invoke_ta_rounds\n",
    "                    if i in self.invoke_ta_rounds:\n",
    "                        recommendation_cost_round, recommmendation_output_file = self.get_recommendations()      \n",
    "                        if os.path.isfile(recommmendation_output_file):\n",
    "                            self.implement_recommendations(recommmendation_output_file)\n",
    "\n",
    "                # now execute the workload for current round\n",
    "                execution_cost_round = self.execute_workload(current_round_queries)        \n",
    "\n",
    "            # clear all indexes\n",
    "            if clear_indexes_end: self.remove_all_nonclustered_indexes()\n",
    "            # clear out all the recommendations and session files from directory\n",
    "            for file in os.listdir():\n",
    "                if file.startswith(\"recommendations\") or file.startswith(\"session_output\"):\n",
    "                    os.remove(file)\n",
    "\n",
    "        # close the connection\n",
    "        self.conn.close()\n",
    "\n",
    "        \n",
    "    def get_recommendations(self):\n",
    "        session_name = f\"session_{uuid.uuid4()}\"\n",
    "        max_memory = 4*1024 # MB\n",
    "        max_time = 1 # minutes\n",
    "        recommendation_output_file = f\"recommendations_{session_name}.sql\"\n",
    "        session_output_xml_file = f\"session_output_{session_name}.xml\"        \n",
    "        dta_exe_path = '\"/mnt/c/Program Files (x86)/Microsoft SQL Server Management Studio 20/Common7/DTA.exe\"'\n",
    "        dta_command = f'{dta_exe_path} -S 172.16.6.196 -U wsl -P greatpond501 -D {self.database} -d {self.database} ' \\\n",
    "                    f'-if \"{self.workload_file}\" -s {session_name} ' \\\n",
    "                    f'-of \"{recommendation_output_file}\" ' \\\n",
    "                    f'-ox \"{session_output_xml_file}\" ' \\\n",
    "                    f'-fa NCL_IDX -fp NONE -fk CL_IDX -B {max_memory} -A {max_time} -F'\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "        subprocess.run(dta_command, shell=True)\n",
    "        end_time = datetime.datetime.now()\n",
    "        time_elapsed = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        print(f\"DTA recommendation time --> {time_elapsed} seconds.\")\n",
    "                  \n",
    "        return time_elapsed, recommendation_output_file     \n",
    "\n",
    "\n",
    "    def implement_recommendations(self, recommendation_output_file):\n",
    "        try:\n",
    "            with open(recommendation_output_file, 'r', encoding=\"utf-16\") as file:\n",
    "                query_lines = file.readlines()\n",
    "                sql = ' '.join(query_lines)\n",
    "                sql = sql.replace('go\\n', ';')\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading recommendations file: {e}\")\n",
    "            return 0                    \n",
    "\n",
    "        recommendation_queries = sql.split(';')\n",
    "        #if self.verbose:\n",
    "        #    print(f\"Recommendation queries: \\n{recommendation_queries}\")\n",
    "        \n",
    "        total_index_creation_cost = 0\n",
    "        for query in recommendation_queries[1:]:\n",
    "            if not query.isspace():\n",
    "                if \"create nonclustered index\" in query.lower():\n",
    "                    total_index_creation_cost += self.create_nonclustered_index(query) \n",
    "                elif \"drop index\" in query.lower():\n",
    "                    self.drop_nonclustered_index(query=query)\n",
    "\n",
    "        print(f\"Implemented recommendations.\")\n",
    "        print(f\"Total index creation time --> {total_index_creation_cost} seconds. Total size of configuration --> {self.get_current_pds_size()} MB\")\n",
    "\n",
    "        return total_index_creation_cost\n",
    "\n",
    "\n",
    "    def create_nonclustered_index(self, query):\n",
    "        cursor = self.conn.cursor()\n",
    "        try:\n",
    "            cursor.execute(\"SET STATISTICS XML ON\")\n",
    "            cursor.execute(query)\n",
    "            stat_xml = cursor.fetchone()[0]\n",
    "            cursor.execute(\"SET STATISTICS XML OFF\")\n",
    "            self.conn.commit()    \n",
    "\n",
    "            if self.verbose:\n",
    "                #print(f\"Query: {query}\")\n",
    "                # Extract the index name\n",
    "                index_start = query.upper().find(\"CREATE NONCLUSTERED INDEX\") + len(\"CREATE NONCLUSTERED INDEX\")\n",
    "                index_end = query.upper().find(\"ON\", index_start)\n",
    "                index_name = query[index_start:index_end].strip()\n",
    "\n",
    "                # Extract the table name\n",
    "                table_start = query.upper().find(\"ON\", index_end) + len(\"ON\")\n",
    "                table_end = query.find(\"(\", table_start)\n",
    "                table_name = query[table_start:table_end].strip()\n",
    "\n",
    "                # Extract the indexed columns\n",
    "                columns_start = query.find(\"(\", table_end) + 1\n",
    "                columns_end = query.find(\")\", columns_start)\n",
    "                indexed_columns = [col.split()[0].strip() for col in query[columns_start:columns_end].split(\",\")]\n",
    "\n",
    "                # Extract the included columns\n",
    "                include_start = query.upper().find(\"INCLUDE\", columns_end)\n",
    "                if include_start != -1:\n",
    "                    include_start = query.find(\"(\", include_start) + 1\n",
    "                    include_end = query.find(\")\", include_start)\n",
    "                    included_columns = [col.strip() for col in query[include_start:include_end].split(\",\")]\n",
    "                else:\n",
    "                    included_columns = []\n",
    "                \n",
    "                print(f\"Created index --> {table_name}.{index_name}, Indexed Columns --> {indexed_columns}, Included Columns --> {included_columns}\")\n",
    "\n",
    "                # get index creation time\n",
    "            query_plan = QueryPlan(stat_xml)\n",
    "            elapsed_time = query_plan.elapsed_time\n",
    "            #cpu_time = query_plan.cpu_time\n",
    "\n",
    "        except pyodbc.Error as e:\n",
    "            print(f\"Error creating index {query}: {e}\")\n",
    "            elapsed_time = 0\n",
    "        finally:\n",
    "            cursor.close()    \n",
    "\n",
    "        return elapsed_time\n",
    "\n",
    "\n",
    "    def drop_nonclustered_index(self, schema_name=None, table_name=None, index_name=None, query=None):\n",
    "        cursor = self.conn.cursor()\n",
    "        if query is None:\n",
    "            query = f\"DROP INDEX {schema_name}.{table_name}.{index_name}\"\n",
    "        else:\n",
    "            # extract the schema, table and index names from the query\n",
    "            split = query.split()\n",
    "            index_name = split[2][1:-1]\n",
    "            schema_name = split[4].split('.')[0][1:-1]\n",
    "            table_name = split[4].split('.')[1][1:-1]\n",
    "        try:\n",
    "            cursor.execute(query)\n",
    "            self.conn.commit()\n",
    "            if self.verbose:\n",
    "                print(f\"Dropped index --> [{schema_name}].[{table_name}].[{index_name}]\")\n",
    "        except pyodbc.Error as e:\n",
    "            print(f\"Error dropping index [{schema_name}].[{table_name}].[{index_name}]: {e}\")     \n",
    "        finally:\n",
    "            cursor.close()          \n",
    "\n",
    "\n",
    "    def get_nonclustered_indexes(self):\n",
    "        cursor = self.conn.cursor()\n",
    "        query = \"\"\"\n",
    "                SELECT \n",
    "                s.name AS SchemaName,\n",
    "                t.name AS TableName,\n",
    "                i.name AS IndexName\n",
    "                FROM \n",
    "                    sys.indexes i\n",
    "                JOIN \n",
    "                    sys.tables t ON i.object_id = t.object_id\n",
    "                JOIN \n",
    "                    sys.schemas s ON t.schema_id = s.schema_id\n",
    "                WHERE \n",
    "                    i.type_desc = 'NONCLUSTERED'  -- Only non-clustered indexes\n",
    "                    AND i.is_primary_key = 0  -- Exclude primary key indexes\n",
    "                    AND i.is_unique_constraint = 0  -- Exclude unique constraints\n",
    "                ORDER BY \n",
    "                    s.name, t.name, i.name; \n",
    "                \"\"\"\n",
    "        try:\n",
    "            cursor.execute(query)\n",
    "            indexes = cursor.fetchall() # return list of tuples: (schema_name, table_name, index_name)\n",
    "        except pyodbc.Error as e:\n",
    "            print(f\"Error fetching non-clustered indexes: {e}\")\n",
    "            indexes = []\n",
    "        finally:    \n",
    "            cursor.close()\n",
    "        \n",
    "        return indexes\n",
    "\n",
    "\n",
    "    def remove_all_nonclustered_indexes(self):\n",
    "        # get all non-clustered indexes\n",
    "        indexes = self.get_nonclustered_indexes()\n",
    "        print(f\"All non-clustered indexes --> {indexes}\")\n",
    "        # drop all non-clustered indexes\n",
    "        for (schema_name, table_name, index_name) in indexes:\n",
    "            self.drop_nonclustered_index(schema_name=schema_name, table_name=table_name, index_name=index_name)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"All nonclustered indexes removed.\")\n",
    "\n",
    "\n",
    "    # get size of all PDS in the database\n",
    "    def get_current_pds_size(self):\n",
    "        cursor = self.conn.cursor()\n",
    "        query = '''SELECT (SUM(s.[used_page_count]) * 8)/1024.0 AS size_mb FROM sys.dm_db_partition_stats AS s'''\n",
    "        try:\n",
    "            cursor.execute(query)\n",
    "            pds_size = cursor.fetchone()[0]\n",
    "        except pyodbc.Error as e:\n",
    "            print(f\"Error fetching PDS size: {e}\")\n",
    "            pds_size = 0\n",
    "        finally:\n",
    "            cursor.close()    \n",
    "\n",
    "        return pds_size\n",
    "\n",
    "\n",
    "\n",
    "    def execute_simple(self, query):\n",
    "        cursor = self.conn.cursor()\n",
    "        try:\n",
    "            cursor.execute(query)\n",
    "            self.conn.commit()\n",
    "        except pyodbc.Error as e:\n",
    "            print(f\"Error executing query {query}: {e}\")\n",
    "        finally:\n",
    "            cursor.close()    \n",
    "\n",
    "\n",
    "    def execute_query(self, query, cost_type='elapsed_time'):\n",
    "        cursor = self.conn.cursor()\n",
    "        try:\n",
    "            # clear cache\n",
    "            cursor.execute(\"DBCC DROPCLEANBUFFERS\")\n",
    "            # enable statistics collection\n",
    "            cursor.execute(\"SET STATISTICS XML ON\")\n",
    "            # execute the query\n",
    "            cursor.execute(query)\n",
    "            cursor.nextset()\n",
    "            # fetch execution stats\n",
    "            stat_xml = cursor.fetchone()[0]\n",
    "            cursor.execute(\"SET STATISTICS XML OFF\")\n",
    "            # parse query plan\n",
    "            query_plan = QueryPlan(stat_xml)\n",
    "\n",
    "            \"\"\"if self.verbose:\n",
    "                print(f\"QUERY: \\n{query}\\n\")\n",
    "                print(f\"ELAPSED TIME: \\n{query_plan.elapsed_time}\\n\")\n",
    "                print(f\"CPU TIME: \\n{query_plan.cpu_time}\\n\")\n",
    "                print(f\"SUBTREE COST: \\n{query_plan.est_statement_sub_tree_cost}\\n\")\n",
    "                print(f\"NON CLUSTERED INDEX USAGE: \\n{query_plan.non_clustered_index_usage}\\n\")\n",
    "                print(f\"CLUSTERED INDEX USAGE: \\n{query_plan.clustered_index_usage}\\n\")\n",
    "            \"\"\"\n",
    "        except pyodbc.Error as e:\n",
    "            logging.error(f\"Error executing query: {query}, Error: {e}\")\n",
    "            return 0, [], []\n",
    "        finally:\n",
    "            cursor.close()\n",
    "\n",
    "        # Determine the cost type and return the appropriate metric\n",
    "        if cost_type == 'elapsed_time':\n",
    "            return float(query_plan.elapsed_time), query_plan.non_clustered_index_usage, query_plan.clustered_index_usage\n",
    "        elif cost_type == 'cpu_time':\n",
    "            return float(query_plan.cpu_time), query_plan.non_clustered_index_usage, query_plan.clustered_index_usage\n",
    "        elif cost_type == 'sub_tree_cost':\n",
    "            return float(query_plan.est_statement_sub_tree_cost), query_plan.non_clustered_index_usage, query_plan.clustered_index_usage\n",
    "        else:\n",
    "            return float(query_plan.est_statement_sub_tree_cost), query_plan.non_clustered_index_usage, query_plan.clustered_index_usage\n",
    "\n",
    "\n",
    "    def execute_workload(self, workload):\n",
    "        if self.verbose:\n",
    "            print(f\"Executing workload of {len(workload)} queries\")\n",
    "        total_elapsed_time = 0\n",
    "        # execute the workload\n",
    "        for query in workload:\n",
    "            cost, index_seeks, clustered_index_scans = self.execute_query(query)\n",
    "            total_elapsed_time += cost   \n",
    "        print(f\"Current round workload execution time --> {total_elapsed_time} seconds.\")     \n",
    "\n",
    "        return total_elapsed_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 of 10\n",
      "11 queries written to workload file\n",
      "Microsoft (R) SQL Server dta\n",
      "Version 20.2.30.0\n",
      "Copyright (c) Microsoft. All rights reserved.\n",
      "\n",
      "Tuning session successfully created. Session ID is 41.\n",
      "\n",
      "Time elapsed: 00:00:10            \n",
      "Workload consumed:  100%, Estimated improvement:    0%                         \n"
     ]
    }
   ],
   "source": [
    "# test dta\n",
    "dta_recommender = DTA_recommender(queries, [0, 4, 8], verbose=True)\n",
    "dta_recommender.run_dta(num_rounds=10, invoke_DTA=True, clear_indexes_start=False, clear_indexes_end=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All non-clustered indexes --> []\n",
      "All nonclustered indexes removed.\n",
      "Round 1 of 1\n",
      "Executing workload of 22 queries\n",
      "Current round workload execution time --> 28.278999999999996\n",
      "All non-clustered indexes --> []\n",
      "All nonclustered indexes removed.\n"
     ]
    }
   ],
   "source": [
    "# now run without invoking DTA, the query execution times should be higher\n",
    "dta_recommender.run_dta(num_rounds=1, clear_indexes_start=True, invoke_DTA=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
