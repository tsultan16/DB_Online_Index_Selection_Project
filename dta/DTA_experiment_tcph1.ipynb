{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating index selection recommendations via MS SQL Database Tuning Adviser (DTA)\n",
    "\n",
    "We will use DTA to generate index recommendations on some sample TPC-H OLAP workloads. \n",
    "\n",
    "Given a workload containing N queries, we will split it up into m rounds (for simplicity we will split up evenly into disjoint subsets, we can also do overlapping). \n",
    "\n",
    "* Experiment 1: On each round, we will execute the queries in that round and measure performance.\n",
    "\n",
    "* Experiment 2: On each round, we will first use DTA to obtain recommendations, implement those recommendation (i.e. create/drop indices etc.), then execute the queries in that round and measure performance. To generate recommendations, we will use all queries that have been seen up to and including queries in the current round.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "import uuid\n",
    "\n",
    "import pyodbc\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "def read_sql_files(base_dir, instances_per_template):\n",
    "    queries = []\n",
    "    \n",
    "    # Regex to find erroneous \"where rownum <=\" lines\n",
    "    erroneous_line_pattern = re.compile(r'^\\s*where\\s+rownum\\s*<=\\s*\\d+\\s*;\\s*$', re.IGNORECASE)\n",
    "\n",
    "    # Loop through each query directory\n",
    "    for query_dir in sorted(os.listdir(base_dir)):\n",
    "        query_path = os.path.join(base_dir, query_dir)\n",
    "        \n",
    "        if os.path.isdir(query_path):\n",
    "            # Initialize a counter for the number of instances read from this template\n",
    "            instance_count = 0\n",
    "            \n",
    "            # Loop through each SQL file in the query directory\n",
    "            for sql_file in sorted(os.listdir(query_path)):\n",
    "                if instance_count >= instances_per_template:\n",
    "                    break  # Stop reading more files from this template\n",
    "                \n",
    "                sql_file_path = os.path.join(query_path, sql_file)\n",
    "                \n",
    "                if sql_file_path.endswith('.sql'):\n",
    "                    with open(sql_file_path, 'r') as file:\n",
    "                        lines = file.readlines()\n",
    "                        \n",
    "                        # Filter out the erroneous \"where rownum <=\" lines\n",
    "                        filtered_lines = [line for line in lines if not erroneous_line_pattern.match(line)]\n",
    "                        \n",
    "                        # Extract the query from the filtered lines\n",
    "                        query = ''.join(filtered_lines[3:]).strip()\n",
    "                        \n",
    "                        queries.append(query)\n",
    "                        instance_count += 1  # Increment the counter\n",
    "    \n",
    "    return queries\n",
    "\n",
    "\n",
    "# Base directory containing the generated queries\n",
    "base_dir = '../TPCH_generated_queries'\n",
    "\n",
    "# Read the SQL files and store the queries in a list\n",
    "queries = read_sql_files(base_dir, instances_per_template=1)\n",
    "\n",
    "print(len(queries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a workload file with all the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"workload_filename = 'workload_tpch_20.sql'\n",
    "\n",
    "# Write the queries to file\n",
    "with open(workload_filename, 'w') as file:\n",
    "    for query in queries:\n",
    "        file.write(query + '\\n\\n')\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define DTA recommender class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyodbc.Cursor object at 0x7f144ec934b0>\n"
     ]
    }
   ],
   "source": [
    "conn_str = (\n",
    "    \"Driver={ODBC Driver 17 for SQL Server};\"\n",
    "    \"Server=172.16.6.196,1433;\"  # Use the IP address and port directly\n",
    "    \"Database=TPCH1;\"  \n",
    "    \"UID=wsl;\" \n",
    "    \"PWD=greatpond501;\"  \n",
    ")\n",
    "\n",
    "conn = pyodbc.connect(conn_str)\n",
    "cursor = conn.cursor()\n",
    "# test the connection\n",
    "print(cursor.execute(\"SELECT @@version;\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTA_recommender:\n",
    "    def __init__(self, queries, invoke_ta_rounds, verbose=False):\n",
    "        self.queries = queries # list of queries to be used as workload\n",
    "        self.invoke_ta_rounds = invoke_ta_rounds # list specifying the rounds to invoke TA\n",
    "        self.verbose = verbose\n",
    "        self.conn_string = (\"Driver={ODBC Driver 17 for SQL Server};\"\n",
    "                            \"Server=172.16.6.196,1433;\"  # Use the IP address and port directly\n",
    "                            \"Database=TPCH1;\"  \n",
    "                            \"UID=wsl;\" \n",
    "                            \"PWD=greatpond501;\")\n",
    "        \n",
    "        self.server = \"172.16.6.196,1433\"\n",
    "        self.database = \"TPCH1\"\n",
    "        self.username = \"wsl\"\n",
    "        self.password = \"greatpond501\"\n",
    "\n",
    "        \n",
    "    def run_dta(self, num_rounds=1):\n",
    "\n",
    "        # establish connection to the database\n",
    "        self.conn = pyodbc.connect(self.conn_string)\n",
    "\n",
    "        # reset workload file\n",
    "        self.workload_file = \"workload_tpch1.sql\"\n",
    "        open(self.workload_file, 'w').close()\n",
    "\n",
    "        num_queries_per_round = len(self.queries) // num_rounds\n",
    "\n",
    "        # iterate over rounds\n",
    "        counter = 0\n",
    "        for i in range(num_rounds):\n",
    "            current_round_queries = self.queries[i*num_queries_per_round:(i+1)*num_queries_per_round]\n",
    "            # write the queries for current round to the workload file\n",
    "            with open(self.workload_file, 'a+') as file:\n",
    "                for query in current_round_queries:\n",
    "                    # exclude queries with \"view\" in them, otherwise DTA will throw a syntax error\n",
    "                    if \"view\" not in query.lower():\n",
    "                        file.write(query)\n",
    "                        file.write('\\n\\n\\n')\n",
    "                        counter += 1\n",
    "\n",
    "            print(f\"Round {i+1} -> {counter} queries written to workload file\")\n",
    "\n",
    "            # invoke DTA if current round is in invoke_ta_rounds\n",
    "            if i in self.invoke_ta_rounds:\n",
    "                recommendation_cost_round, recommmendation_output_file = self.get_recommendations()      \n",
    "                if os.path.isfile(recommmendation_output_file):\n",
    "                    self.implement_recommendations(recommmendation_output_file)\n",
    "\n",
    "\n",
    "        # close the connection\n",
    "        self.conn.close()\n",
    "\n",
    "        \n",
    "    def get_recommendations(self):\n",
    "        session_name = f\"session_{uuid.uuid4()}\"\n",
    "        max_memory = 4*1024 # MB\n",
    "        max_time = 1 # minutes\n",
    "        recommendation_output_file = f\"recommendations_{session_name}.sql\"\n",
    "        session_output_xml_file = f\"session_output_{session_name}.xml\"        \n",
    "        dta_exe_path = '\"/mnt/c/Program Files (x86)/Microsoft SQL Server Management Studio 20/Common7/DTA.exe\"'\n",
    "        dta_command = f'{dta_exe_path} -S 172.16.6.196 -U wsl -P greatpond501 -D {self.database} -d {self.database} ' \\\n",
    "                    f'-if \"{self.workload_file}\" -s {session_name} ' \\\n",
    "                    f'-of \"{recommendation_output_file}\" ' \\\n",
    "                    f'-ox \"{session_output_xml_file}\" ' \\\n",
    "                    f'-fa NCL_IDX -fp NONE -fk CL_IDX -B {max_memory} -A {max_time} -F'\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "        subprocess.run(dta_command, shell=True)\n",
    "        end_time = datetime.datetime.now()\n",
    "        time_elapsed = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"DTA took {time_elapsed} seconds. Recommendations:\\n\")\n",
    "            # print the recommendations\n",
    "            with open(recommendation_output_file, 'r', encoding=\"utf-16\") as file:\n",
    "                recommendations = file.readlines()\n",
    "                for recommendation in recommendations:\n",
    "                    print(recommendation)\n",
    "                  \n",
    "        return time_elapsed, recommendation_output_file     \n",
    "\n",
    "\n",
    "\n",
    "    def implement_recommendations(self, recommendation_output_file):\n",
    "        with open(recommendation_output_file, 'r', encoding=\"utf-16\") as file:\n",
    "            query_lines = file.readlines()\n",
    "        sql = ' '.join(query_lines)\n",
    "        sql = sql.replace('go\\n', ';')            \n",
    "\n",
    "\n",
    "        #cursor = self.conn.cursor()\n",
    "        #cursor.execute(sql_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 -> 21 queries written to workload file\n",
      "Microsoft (R) SQL Server dta\n",
      "Version 20.2.30.0\n",
      "Copyright (c) Microsoft. All rights reserved.\n",
      "\n",
      "Tuning session successfully created. Session ID is 16.\n",
      "\n",
      "Time elapsed: 00:00:10            \n",
      "Workload consumed:  100%, Estimated improvement:    0%                         \n"
     ]
    }
   ],
   "source": [
    "# test dta\n",
    "dta_recommender = DTA_recommender(queries, [0], verbose=True)\n",
    "dta_recommender.run_dta(num_rounds=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
