{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating index selection recommendations via MS SQL Database Tuning Adviser (DTA)\n",
    "\n",
    "We will use DTA to generate index recommendations on some sample TPC-H OLAP workloads. \n",
    "\n",
    "Given a workload containing N queries, we will split it up into m rounds (for simplicity we will split up evenly into disjoint subsets, we can also do overlapping). \n",
    "\n",
    "* Experiment 1: On each round, we will execute the queries in that round and measure performance.\n",
    "\n",
    "* Experiment 2: On each round, we will first use DTA to obtain recommendations, implement those recommendation (i.e. create/drop indices etc.), then execute the queries in that round and measure performance. To generate recommendations, we will use all queries that have been seen up to and including queries in the current round.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "import uuid\n",
    "import pyodbc\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440\n"
     ]
    }
   ],
   "source": [
    "def read_sql_files(base_dir, instances_per_template):\n",
    "    queries = []\n",
    "    \n",
    "    # Regex to find erroneous \"where rownum <=\" lines\n",
    "    erroneous_line_pattern = re.compile(r'^\\s*where\\s+rownum\\s*<=\\s*\\d+\\s*;\\s*$', re.IGNORECASE)\n",
    "\n",
    "    # Loop through each query directory\n",
    "    for query_dir in sorted(os.listdir(base_dir)):\n",
    "        query_path = os.path.join(base_dir, query_dir)\n",
    "        \n",
    "        if os.path.isdir(query_path):\n",
    "            # Initialize a counter for the number of instances read from this template\n",
    "            instance_count = 0\n",
    "            \n",
    "            # Loop through each SQL file in the query directory\n",
    "            for sql_file in sorted(os.listdir(query_path)):\n",
    "                if instance_count >= instances_per_template:\n",
    "                    break  # Stop reading more files from this template\n",
    "                \n",
    "                sql_file_path = os.path.join(query_path, sql_file)\n",
    "                \n",
    "                if sql_file_path.endswith('.sql'):\n",
    "                    with open(sql_file_path, 'r') as file:\n",
    "                        lines = file.readlines()\n",
    "                        \n",
    "                        # Filter out the erroneous \"where rownum <=\" lines\n",
    "                        filtered_lines = [line for line in lines if not erroneous_line_pattern.match(line)]\n",
    "                        \n",
    "                        # Extract the query from the filtered lines\n",
    "                        query = ''.join(filtered_lines[3:]).strip()\n",
    "                        \n",
    "                        queries.append(query)\n",
    "                        instance_count += 1  # Increment the counter\n",
    "    \n",
    "    return queries\n",
    "\n",
    "\n",
    "# Base directory containing the generated queries\n",
    "base_dir = '../TPCH_generated_queries'\n",
    "\n",
    "# Read the SQL files and store the queries in a list\n",
    "queries = read_sql_files(base_dir, instances_per_template=20)\n",
    "\n",
    "print(len(queries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a workload file with all the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "workload_filename = 'workload_tpch_20.sql'\n",
    "\n",
    "# Write the queries to file\n",
    "with open(workload_filename, 'w') as file:\n",
    "    for query in queries:\n",
    "        file.write(query + '\\n\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define DTA recommender class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_str = (\n",
    "    \"Driver={ODBC Driver 17 for SQL Server};\"\n",
    "    \"Server=172.16.6.196,1433;\"  # Use the IP address and port directly\n",
    "    \"Database=TPCH1;\"  \n",
    "    \"UID=wsl;\" \n",
    "    \"PWD=greatpond501;\"  \n",
    ")\n",
    "\n",
    "conn = pyodbc.connect(conn_str)\n",
    "cursor = conn.cursor()\n",
    "# test the connection\n",
    "print(cursor.execute(\"SELECT @@version;\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTA_recommender:\n",
    "    def __init__(self, workload_filename, dta_script_path, dta_output_path, dta_log_path):\n",
    "        self.\n",
    "        \n",
    "    def run_dta(self):\n",
    "        # Run the DTA script\n",
    "        subprocess.run(['python', self.dta_script_path, self.workload_filename, self.dta_output_path], check=True)\n",
    "        \n",
    "    def get_recommendations(self):\n",
    "        # Read the recommendations from the DTA output file\n",
    "        recommendations = []\n",
    "        with open(self.dta_output_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                if line.startswith('Recommendation:'):\n",
    "                    query_id = int(line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
