{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating index selection recommendations via MS SQL Database Tuning Adviser (DTA)\n",
    "\n",
    "We will use DTA to generate index recommendations on some sample TPC-H OLAP workloads. \n",
    "\n",
    "Given a workload containing N queries, we will split it up into m rounds (for simplicity we will split up evenly into disjoint subsets, we can also do overlapping). \n",
    "\n",
    "* Experiment 1: On each round, we will execute the queries in that round and measure performance.\n",
    "\n",
    "* Experiment 2: On each round, we will first use DTA to obtain recommendations, implement those recommendation (i.e. create/drop indices etc.), then execute the queries in that round and measure performance. To generate recommendations, we will use all queries that have been seen up to and including queries in the current round.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "import uuid\n",
    "\n",
    "import pyodbc\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import re\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import IPython\n",
    "notebook_path = IPython.get_ipython().starting_dir\n",
    "target_subdirectory_path = os.path.abspath(os.path.join(os.path.dirname(notebook_path), 'database'))\n",
    "sys.path.append(target_subdirectory_path)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100\n"
     ]
    }
   ],
   "source": [
    "# read workload queries from JSON file\n",
    "def read_workload(workload_filepath):\n",
    "    workload = []\n",
    "    with open(workload_filepath) as f:\n",
    "        line = f.readline()\n",
    "        # read the queries from each line\n",
    "        while line:\n",
    "            workload.append(json.loads(line))\n",
    "            line = f.readline()\n",
    "\n",
    "    return workload\n",
    "\n",
    "# Base directory containing the generated queries\n",
    "workload_filepath = '../datagen/TPCH_workloads/TPCH_static_100_workload.json'\n",
    "\n",
    "# Read the workload queries from file\n",
    "workload = read_workload(workload_filepath)\n",
    "print(len(workload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define DTA recommender class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY: \n",
      "select\n",
      "\tl_returnflag,\n",
      "\tl_linestatus,\n",
      "\tsum(l_quantity) as sum_qty,\n",
      "\tsum(l_extendedprice) as sum_base_price,\n",
      "\tsum(l_extendedprice * (1 - l_discount)) as sum_disc_price,\n",
      "\tsum(l_extendedprice * (1 - l_discount) * (1 + l_tax)) as sum_charge,\n",
      "\tavg(l_quantity) as avg_qty,\n",
      "\tavg(l_extendedprice) as avg_price,\n",
      "\tavg(l_discount) as avg_disc,\n",
      "\tcount(*) as count_order\n",
      "from\n",
      "\tlineitem\n",
      "where\n",
      "\tl_shipdate <= DATEADD(dd, -84, CAST('1998-12-01' AS date))\n",
      "group by\n",
      "\tl_returnflag,\n",
      "\tl_linestatus\n",
      "order by\n",
      "\tl_returnflag,\n",
      "\tl_linestatus\n",
      ";\n",
      "\n",
      "ELAPSED TIME: \n",
      "2.014\n",
      "\n",
      "CPU TIME: \n",
      "11727.0\n",
      "\n",
      "SUBTREE COST: \n",
      "78.4947\n",
      "\n",
      "NON CLUSTERED INDEX USAGE: \n",
      "[]\n",
      "\n",
      "CLUSTERED INDEX USAGE: \n",
      "[('lineitem', 1.797, 11727.0, 78.4947, 6001215, 5924530.0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test - execute a query\n",
    "query = workload[0]['query_string']\n",
    "connection = start_connection()\n",
    "execute_query(query, connection, cost_type='elapsed_time', verbose=True)\n",
    "close_connection(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTA_recommender:\n",
    "    def __init__(self, queries, invoke_ta_rounds, verbose=False):\n",
    "        self.queries = queries # list of queries to be used as workload\n",
    "        self.invoke_ta_rounds = invoke_ta_rounds # list specifying the rounds to invoke TA\n",
    "        self.verbose = verbose\n",
    "        self.server=\"172.16.6.196,1433\"\n",
    "        self.database=\"TPCH1\" \n",
    "        self.username=\"wsl\" \n",
    "        self.password=\"greatpond501\"\n",
    "        \n",
    "\n",
    "    def run_dta(self, num_rounds=1, invoke_DTA=True, clear_indexes_start=False, clear_indexes_end=True):\n",
    "\n",
    "        # establish connection to the database\n",
    "        self.conn = start_connection()\n",
    "\n",
    "        # clear all non-clustered indexes at the start\n",
    "        if clear_indexes_start:   \n",
    "            remove_all_nonclustered_indexes(self.conn, self.verbose)\n",
    "  \n",
    "        if num_rounds > 0:\n",
    "\n",
    "            # reset workload file\n",
    "            self.workload_file = \"workload_tpch1.sql\"\n",
    "            open(self.workload_file, 'w').close()\n",
    "\n",
    "            num_queries_per_round = len(self.queries) // num_rounds\n",
    "\n",
    "            # iterate over rounds\n",
    "            counter = 0\n",
    "            for i in range(num_rounds):\n",
    "                print(f\"Round {i+1} of {num_rounds}\")\n",
    "                current_round_queries = self.queries[i*num_queries_per_round:(i+1)*num_queries_per_round]\n",
    "                \n",
    "                if invoke_DTA:\n",
    "                    # write the queries for current round to the workload file\n",
    "                    with open(self.workload_file, 'a+') as file:\n",
    "                        for query in current_round_queries:\n",
    "                            query_string = query['query_string']\n",
    "                            # exclude queries with \"view\" in them, otherwise DTA will throw a syntax error\n",
    "                            if \"view\" not in query_string.lower():\n",
    "                                file.write(query_string)\n",
    "                                file.write('\\n\\n\\n')\n",
    "                                counter += 1\n",
    "\n",
    "                    print(f\"{counter} queries written on workload file\")\n",
    "\n",
    "                    # invoke DTA if current round is in invoke_ta_rounds\n",
    "                    if i in self.invoke_ta_rounds:\n",
    "                        recommendation_cost_round, recommmendation_output_file = self.get_recommendations()      \n",
    "                        if os.path.isfile(recommmendation_output_file):\n",
    "                            self.implement_recommendations(recommmendation_output_file)\n",
    "                            # reset the workload file\n",
    "                            open(self.workload_file, 'w').close()\n",
    "                            counter = 0\n",
    "\n",
    "                # now execute the workload for current round\n",
    "                execution_cost_round = self.execute_workload(current_round_queries)        \n",
    "\n",
    "            # clear all indexes\n",
    "            if clear_indexes_end: remove_all_nonclustered_indexes(self.conn, self.verbose)\n",
    "            # clear out all the recommendations and session files from directory\n",
    "            for file in os.listdir():\n",
    "                if file.startswith(\"recommendations\") or file.startswith(\"session_output\"):\n",
    "                    os.remove(file)\n",
    "\n",
    "        # close the connection\n",
    "        close_connection(self.conn)\n",
    "\n",
    "        \n",
    "    def get_recommendations(self):\n",
    "        session_name = f\"session_{uuid.uuid4()}\"\n",
    "        max_memory = 4*1024 # MB\n",
    "        max_time = 1 # minutes\n",
    "        recommendation_output_file = f\"recommendations_{session_name}.sql\"\n",
    "        session_output_xml_file = f\"session_output_{session_name}.xml\"        \n",
    "        dta_exe_path = '\"/mnt/c/Program Files (x86)/Microsoft SQL Server Management Studio 20/Common7/DTA.exe\"'\n",
    "        dta_command = f'{dta_exe_path} -S 172.16.6.196 -U wsl -P greatpond501 -D {self.database} -d {self.database} ' \\\n",
    "                    f'-if \"{self.workload_file}\" -s {session_name} ' \\\n",
    "                    f'-of \"{recommendation_output_file}\" ' \\\n",
    "                    f'-ox \"{session_output_xml_file}\" ' \\\n",
    "                    f'-fa NCL_IDX -fp NONE -fk CL_IDX -B {max_memory} -A {max_time} -F'\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "        subprocess.run(dta_command, shell=True)\n",
    "        end_time = datetime.datetime.now()\n",
    "        time_elapsed = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        print(f\"DTA recommendation time --> {time_elapsed} seconds.\")\n",
    "                  \n",
    "        return time_elapsed, recommendation_output_file     \n",
    "\n",
    "\n",
    "    def implement_recommendations(self, recommendation_output_file):\n",
    "        if self.verbose: print(\"Implementing recommendations...\")\n",
    "        try:\n",
    "            with open(recommendation_output_file, 'r', encoding=\"utf-16\") as file:\n",
    "                query_lines = file.readlines()\n",
    "                sql = ' '.join(query_lines)\n",
    "                sql = sql.replace('go\\n', ';')\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading recommendations file: {e}\")\n",
    "            return 0                    \n",
    "\n",
    "        recommendation_queries = sql.split(';')\n",
    "        #if self.verbose:\n",
    "        #    print(f\"Recommendation queries: \\n{recommendation_queries}\")\n",
    "        \n",
    "        total_index_creation_cost = 0\n",
    "        for query in recommendation_queries[1:]:\n",
    "            if not query.isspace():\n",
    "                if \"create nonclustered index\" in query.lower():\n",
    "                    total_index_creation_cost += create_nonclustered_index_query(query, self.conn, verbose=self.verbose) \n",
    "                elif \"drop index\" in query.lower():\n",
    "                    drop_nonclustered_index(self.conn, query=query, verbose=self.verbose)\n",
    "\n",
    "        print(f\"Implemented recommendations.\")\n",
    "        print(f\"Total index creation time --> {total_index_creation_cost} seconds. Total size of configuration --> {get_current_pds_size(self.conn)} MB\")\n",
    "\n",
    "        return total_index_creation_cost\n",
    "\n",
    "\n",
    "    def execute_workload(self, workload):\n",
    "        if self.verbose:\n",
    "            print(f\"Executing workload of {len(workload)} queries\")\n",
    "        total_elapsed_time = 0\n",
    "        # execute the workload\n",
    "        for query in workload:\n",
    "            cost, index_seeks, clustered_index_scans = execute_query(query['query_string'], self.conn)\n",
    "            total_elapsed_time += cost   \n",
    "        print(f\"Current round workload execution time --> {total_elapsed_time} seconds.\")     \n",
    "\n",
    "        return total_elapsed_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All non-clustered indexes --> []\n",
      "Round 1 of 5\n",
      "21 queries written on workload file\n",
      "Microsoft (R) SQL Server dta\n",
      "Version 20.2.30.0\n",
      "Copyright (c) Microsoft. All rights reserved.\n",
      "\n",
      "Tuning session successfully created. Session ID is 55.\n",
      "\n",
      "Time elapsed: 00:00:10            \n",
      "Workload consumed:  100%, Estimated improvement:    0%                         \n",
      "Time elapsed: 00:00:40            \n",
      "Workload consumed:  100%, Estimated improvement:    0%                         \n",
      "Time elapsed: 00:00:53            \n",
      "Workload consumed:  100%, Estimated improvement:   75%                         \n",
      "Time elapsed: 00:01:02            \n",
      "Workload consumed:  100%, Estimated improvement:   79%                         \n",
      "Total time used: 00:01:02                                \n",
      "\n",
      "\n",
      "                                                                               \n",
      "                                                                                \n",
      "All events in the workload were not analysed. Check tuning log for more information\n",
      "Tuning process finished.\n",
      "Successfully saved output XML file: \\\\wsl.localhost\\Ubuntu\\home\\tanzid\\Code\\DBMS\\dta\\session_output_session_f5d2faf7-2418-49cb-a54b-d6a96c17d89e.xml.\n",
      "Successfully generated recommendations script: \\\\wsl.localhost\\Ubuntu\\home\\tanzid\\Code\\DBMS\\dta\\recommendations_session_f5d2faf7-2418-49cb-a54b-d6a96c17d89e.sql.\n",
      "DTA recommendation time --> 81.095258 seconds.\n",
      "Implemented recommendations.\n",
      "Total index creation time --> 161.67600000000002 seconds. Total size of configuration --> 4260.148437 MB\n",
      "Current round workload execution time --> 13.568999999999997 seconds.\n",
      "Round 2 of 5\n",
      "21 queries written on workload file\n",
      "Current round workload execution time --> 13.291 seconds.\n",
      "Round 3 of 5\n",
      "42 queries written on workload file\n",
      "Microsoft (R) SQL Server dta\n",
      "Version 20.2.30.0\n",
      "Copyright (c) Microsoft. All rights reserved.\n",
      "\n",
      "Tuning session successfully created. Session ID is 56.\n",
      "\n",
      "Time elapsed: 00:00:10            \n",
      "Workload consumed:  100%, Estimated improvement:    0%                         \n"
     ]
    }
   ],
   "source": [
    "# test dta\n",
    "dta_recommender = DTA_recommender(workload[:105], [0, 2], verbose=False)\n",
    "dta_recommender.run_dta(num_rounds=5, invoke_DTA=True, clear_indexes_start=True, clear_indexes_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All non-clustered indexes --> []\n",
      "All nonclustered indexes removed.\n",
      "Round 1 of 5\n",
      "Executing workload of 21 queries\n",
      "Current round workload execution time --> 24.636 seconds.\n",
      "Round 2 of 5\n",
      "Executing workload of 21 queries\n",
      "Current round workload execution time --> 26.837000000000007 seconds.\n",
      "Round 3 of 5\n",
      "Executing workload of 21 queries\n",
      "Current round workload execution time --> 25.334 seconds.\n",
      "Round 4 of 5\n",
      "Executing workload of 21 queries\n",
      "Current round workload execution time --> 25.929999999999996 seconds.\n",
      "Round 5 of 5\n",
      "Executing workload of 21 queries\n",
      "Current round workload execution time --> 25.415 seconds.\n",
      "All non-clustered indexes --> []\n",
      "All nonclustered indexes removed.\n"
     ]
    }
   ],
   "source": [
    "# now run without invoking DTA, the query execution times should be higher\n",
    "dta_recommender.run_dta(num_rounds=5, clear_indexes_start=True, invoke_DTA=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
