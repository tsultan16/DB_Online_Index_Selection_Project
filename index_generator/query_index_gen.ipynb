{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-clustered Index configuration generator\n",
    "\n",
    "Given a query, we will use it's properties (predicates, payload) to generate a list of candidate index configuration that could benefit the execution of that query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "import uuid\n",
    "\n",
    "import pyodbc\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import re\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import itertools\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import IPython\n",
    "notebook_path = IPython.get_ipython().starting_dir\n",
    "target_subdirectory_path = os.path.abspath(os.path.join(os.path.dirname(notebook_path), 'database'))\n",
    "sys.path.append(target_subdirectory_path)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100\n"
     ]
    }
   ],
   "source": [
    "# read workload queries from JSON file\n",
    "def read_workload(workload_filepath):\n",
    "    workload = []\n",
    "    with open(workload_filepath) as f:\n",
    "        line = f.readline()\n",
    "        # read the queries from each line\n",
    "        while line:\n",
    "            workload.append(json.loads(line))\n",
    "            line = f.readline()\n",
    "\n",
    "    return workload\n",
    "\n",
    "# Base directory containing the generated queries\n",
    "workload_filepath = '../datagen/TPCH_workloads/TPCH_static_100_workload.json'\n",
    "\n",
    "# Read the workload queries from file\n",
    "workload = read_workload(workload_filepath)\n",
    "print(len(workload))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To generate candidate index configurations that may benefit a given query, can do the following:\n",
    "\n",
    "* Look at each table in that query (If the table is too small, then full table scan is cheap so don't need to index. Also, if a table has high \"selectivity\" and also contains INCLUDE columns, then most likely a large proportion of it's rows will be returned, so again full table scan will be cheap so don't need to index)\n",
    "* For each of these tables, look at the corresponding predicate columns (these are usually columns under the WHERE clause)    \n",
    "* Identify the INCLUDE columns, which are columns that are in the payload (payload columns are usually under the SELECT clause) but are not predicate columns, i.e. columns which are needed in the query result but are not used for filtering\n",
    "* Then generate multicolumn indexes without include columns by enumerating all permutations of the predicate columns, ranging from single-column permutations up to 6-column permutations (indexes on more than 6 columns becomes impractical) \n",
    "* Similarly, we generate multicolumn indexes by considering columns that are only in the payload but not in any predicate. Here, we don't need to consider all different column combinations, we can just make a single index for all payload columns for a given table in whatever order, this will mainly just serve as a covering index \n",
    "* Finally, we create indexes on tables with both predicate and payload columns. Here we consider indexes on all permutations of the predicate columns as index columns along with the include columns.\n",
    "* For each index, we also estimate it's value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index:\n",
    "    def __init__(self, table_name, index_id, index_columns, size, include_columns=(), value=None):\n",
    "        self.table_name = table_name\n",
    "        self.index_id = index_id\n",
    "        self.index_columns = index_columns\n",
    "        self.size = size\n",
    "        self.include_columns = include_columns\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Index({self.table_name}, {self.index_id}, {self.index_columns}, {self.include_columns}, {self.size}, {self.value})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick a test query and generate all it's candidadte indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables:\n",
      "Table: customer, Row Count: 150000, PK Columns: ['c_custkey']\n",
      "Table: orders, Row Count: 1500000, PK Columns: ['o_orderkey']\n",
      "Table: lineitem, Row Count: 6001215, PK Columns: ['l_linenumber', 'l_orderkey']\n",
      "Table: part, Row Count: 200000, PK Columns: ['p_partkey']\n",
      "Table: supplier, Row Count: 10000, PK Columns: ['s_suppkey']\n",
      "Table: partsupp, Row Count: 800000, PK Columns: ['ps_partkey', 'ps_suppkey']\n",
      "Table: nation, Row Count: 25, PK Columns: ['n_nationkey']\n",
      "Table: region, Row Count: 5, PK Columns: ['r_regionkey']\n",
      "\n",
      "All columns: (defaultdict(<class 'list'>, {'customer': ['c_acctbal', 'c_address', 'c_comment', 'c_custkey', 'c_mktsegment', 'c_name', 'c_nationkey', 'c_phone'], 'orders': ['o_clerk', 'o_comment', 'o_custkey', 'o_orderdate', 'o_orderkey', 'o_orderpriority', 'o_orderstatus', 'o_shippriority', 'o_totalprice'], 'lineitem': ['l_comment', 'l_commitdate', 'l_discount', 'l_extendedprice', 'l_linenumber', 'l_linestatus', 'l_orderkey', 'l_partkey', 'l_quantity', 'l_receiptdate', 'l_returnflag', 'l_shipdate', 'l_shipinstruct', 'l_shipmode', 'l_suppkey', 'l_tax'], 'part': ['p_brand', 'p_comment', 'p_container', 'p_mfgr', 'p_name', 'p_partkey', 'p_retailprice', 'p_size', 'p_type'], 'supplier': ['s_acctbal', 's_address', 's_comment', 's_name', 's_nationkey', 's_phone', 's_suppkey'], 'partsupp': ['ps_availqty', 'ps_comment', 'ps_partkey', 'ps_suppkey', 'ps_supplycost'], 'nation': ['n_comment', 'n_name', 'n_nationkey', 'n_regionkey'], 'region': ['r_comment', 'r_name', 'r_regionkey']}), 61)\n",
      "\n",
      "\n",
      "\n",
      "Payload: {'supplier': ['s_acctbal', 's_name', 's_address', 's_phone', 's_comment'], 'nation': ['n_name'], 'part': ['p_partkey', 'p_mfgr'], 'partsupp': ['ps_supplycost']}\n",
      "\n",
      "\n",
      "Table --> part, Predicate Columns --> {'p_size', 'p_type'}, table row count --> 200000\n",
      "Include columns: ['p_mfgr', 'p_partkey']\n",
      "Query selectivity: 0.0092183\n",
      "Column permutations: \n",
      "[('p_size', 'p_type'), ('p_type', 'p_size')]\n",
      "index_id: IX_part_p_size_p_type, index columns: ('p_size', 'p_type'), index size: 7.44 Mb\n",
      "index_id: IX_part_p_type_p_size, index columns: ('p_type', 'p_size'), index size: 7.44 Mb\n",
      "\n",
      "Table --> partsupp, Predicate Columns --> {'ps_suppkey', 'ps_partkey'}, table row count --> 800000\n",
      "Include columns: ['ps_supplycost']\n",
      "Query selectivity: 1.250025e-06\n",
      "Column permutations: \n",
      "[('ps_suppkey', 'ps_partkey'), ('ps_partkey', 'ps_suppkey')]\n",
      "index_id: IX_partsupp_ps_suppkey_ps_partkey, index columns: ('ps_suppkey', 'ps_partkey'), index size: 11.44 Mb\n",
      "index_id: IX_partsupp_ps_partkey_ps_suppkey, index columns: ('ps_partkey', 'ps_suppkey'), index size: 11.44 Mb\n",
      "\n",
      "Table --> supplier, Predicate Columns --> {'s_nationkey'}, table row count --> 10000\n",
      "Include columns: ['s_comment', 's_phone', 's_name', 's_address', 's_acctbal']\n",
      "Query selectivity: 0.0001\n",
      "Column permutations: \n",
      "[('s_nationkey',)]\n",
      "index_id: IX_supplier_s_nationkey, index columns: ('s_nationkey',), index size: 0.14 Mb\n",
      "\n",
      "Table --> nation, Predicate Columns --> {'n_regionkey'}, table row count --> 25\n",
      "Include columns: ['n_name']\n",
      "Query selectivity: 0.04\n",
      "Full table scan for table: nation is cheap, skipping\n",
      "\n",
      "Table --> region, Predicate Columns --> {'r_name'}, table row count --> 5\n",
      "Include columns: []\n",
      "Query selectivity: 0.2\n",
      "Full table scan for table: region is cheap, skipping\n",
      "\n",
      "Table --> supplier, Payload Columns --> {'r_name'}, table row count --> 10000\n",
      "Payload columns are in the predicates, skipping\n",
      "\n",
      "Table --> nation, Payload Columns --> {'r_name'}, table row count --> 25\n",
      "Payload columns are in the predicates, skipping\n",
      "\n",
      "Table --> part, Payload Columns --> {'r_name'}, table row count --> 200000\n",
      "Payload columns are in the predicates, skipping\n",
      "\n",
      "Table --> partsupp, Payload Columns --> {'r_name'}, table row count --> 800000\n",
      "Payload columns are in the predicates, skipping\n",
      "\n",
      "Table --> part, Predicate Columns --> {'p_size', 'p_type'}, table row count --> 200000\n",
      "Include columns: ['p_mfgr', 'p_partkey']\n",
      "Column permutations: \n",
      "[('p_size', 'p_type'), ('p_type', 'p_size')]\n",
      "index_id: IXN_part_p_size_p_type_p_mf_p_pa, index columns: ('p_size', 'p_type'), include columns: ['p_mfgr', 'p_partkey'], index size: 12.21 Mb\n",
      "index_id: IXN_part_p_type_p_size_p_mf_p_pa, index columns: ('p_type', 'p_size'), include columns: ['p_mfgr', 'p_partkey'], index size: 12.21 Mb\n",
      "\n",
      "Table --> partsupp, Predicate Columns --> {'ps_suppkey', 'ps_partkey'}, table row count --> 800000\n",
      "Include columns: ['ps_supplycost']\n",
      "Column permutations: \n",
      "[('ps_suppkey', 'ps_partkey'), ('ps_partkey', 'ps_suppkey')]\n",
      "index_id: IXN_partsupp_ps_suppkey_ps_partkey_ps_s, index columns: ('ps_suppkey', 'ps_partkey'), include columns: ['ps_supplycost'], index size: 18.31 Mb\n",
      "index_id: IXN_partsupp_ps_partkey_ps_suppkey_ps_s, index columns: ('ps_partkey', 'ps_suppkey'), include columns: ['ps_supplycost'], index size: 18.31 Mb\n",
      "\n",
      "Table --> supplier, Predicate Columns --> {'s_nationkey'}, table row count --> 10000\n",
      "Include columns: ['s_acctbal', 's_address', 's_comment', 's_name', 's_phone']\n",
      "Column permutations: \n",
      "[('s_nationkey',)]\n",
      "index_id: IXN_supplier_s_nationkey_s_ac_s_ad_s_co_s_na_s_ph, index columns: ('s_nationkey',), include columns: ['s_acctbal', 's_address', 's_comment', 's_name', 's_phone'], index size: 1.51 Mb\n",
      "\n",
      "Table --> nation, Predicate Columns --> {'n_regionkey'}, table row count --> 25\n",
      "Full table scan for table: nation is cheap, skipping\n",
      "\n",
      "Table --> region, Predicate Columns --> {'r_name'}, table row count --> 5\n",
      "Full table scan for table: region is cheap, skipping\n"
     ]
    }
   ],
   "source": [
    "# constants\n",
    "SMALL_TABLE_IGNORE = 10000\n",
    "TABLE_MIN_SELECTIVITY = 0.2\n",
    "\n",
    "connection = start_connection() \n",
    "tables = get_all_tables(connection)\n",
    "all_columns = get_all_columns(connection)\n",
    "\n",
    "# get all tables in db\n",
    "print(f\"Tables:\")\n",
    "for key in tables:\n",
    "    print(tables[key])\n",
    "\n",
    "print(f\"\\nAll columns: {all_columns}\\n\")    \n",
    "\n",
    "# pick a query from the workload, get it's predicates and payload\n",
    "i = 1\n",
    "query = workload[i]\n",
    "# convert to proper query object\n",
    "query = Query(connection, query['template_id'], query['query_string'], query['payload'], query['predicates'], query['order_bys'])\n",
    "\n",
    "query_template_id = query.template_id\n",
    "query_predicates = query.predicates\n",
    "query_payload = query.payload\n",
    "print()\n",
    "#print(f\"Query: {query.query_string}\")\n",
    "print()\n",
    "print(f\"Payload: {query_payload}\")\n",
    "print()\n",
    "\n",
    "indices = []\n",
    "\n",
    "# indexes on predicate columns only\n",
    "for table_name, table_predicates in query_predicates.items():\n",
    "    table = tables[table_name]\n",
    "    print(f\"\\nTable --> {table_name}, Predicate Columns --> {set(table_predicates)}, table row count --> {table.row_count}\")\n",
    "    \n",
    "    # identify include columns\n",
    "    include_columns = []\n",
    "    if table_name in query_payload:\n",
    "        include_columns = list(set(query_payload[table_name]) - set(table_predicates))\n",
    "\n",
    "    print(f\"Include columns: {include_columns}\")\n",
    "    print(f\"Query selectivity: {query.selectivity[table_name]}\")\n",
    "\n",
    "\n",
    "    # check if conditions for cheap full table scan are met\n",
    "    if table.row_count < SMALL_TABLE_IGNORE or ((query.selectivity[table_name] > TABLE_MIN_SELECTIVITY) and (len(include_columns)>0)):\n",
    "        print(f\"Full table scan for table: {table_name} is cheap, skipping\")\n",
    "        continue\n",
    "\n",
    "    # generate all possible permutations of predicate columns, from single column up to 6-column indices\n",
    "    table_predicates = list(table_predicates.keys())[0:6]\n",
    "    col_permutations = []\n",
    "    for num_columns in range(1, min(6, len(table_predicates)+1)):\n",
    "        col_permutations = list(itertools.permutations(table_predicates, num_columns)) \n",
    "    \n",
    "    print(f\"Column permutations: \\n{col_permutations}\")\n",
    "\n",
    "    # assign an id and value to each index/column permutation\n",
    "    for cp in col_permutations:\n",
    "        index_id = get_index_id(cp, table_name)\n",
    "        index_size = get_estimated_index_size(connection, table_name, cp)\n",
    "        print(f\"index_id: {index_id}, index columns: {cp}, index size: {index_size:.2f} Mb\")\n",
    "        # assign value...\n",
    "        # create index object\n",
    "        index = Index(table_name, index_id, cp, index_size)\n",
    "        indices.append(index)\n",
    "\n",
    "\n",
    "\n",
    "# indexes on columns that are in the payload but not in the predicates\n",
    "for table_name, table_payload in query_payload.items():\n",
    "    table = tables[table_name]\n",
    "    print(f\"\\nTable --> {table_name}, Payload Columns --> {set(table_predicates)}, table row count --> {table.row_count}\")\n",
    "    \n",
    "    # skip if any of the payload columns for this table are in the predicates\n",
    "    if table_name in query_predicates:\n",
    "        print(f\"Payload columns are in the predicates, skipping\")\n",
    "        continue\n",
    "\n",
    "    # check if conditions for cheap full table scan are met\n",
    "    if table.row_count < SMALL_TABLE_IGNORE:\n",
    "        print(f\"Full table scan for table: {table_name} is cheap, skipping\")\n",
    "        continue   \n",
    "\n",
    "    # don't need to consider permutations here, just create an index with all payload columns in given order\n",
    "    index_id = get_index_id(table_payload, table_name)\n",
    "    index_size = get_estimated_index_size(connection, table_name, table_payload)\n",
    "    print(f\"index_id: {index_id}, index columns: {table_payload}, index size: {index_size:.2f} Mb\")\n",
    "    # assign value... (will assign less value to these indices as they are less useful compared to predicate indices)\n",
    "    indices.append(Index(table_name, index_id, table_payload, index_size))\n",
    "\n",
    "# indexes with include columns\n",
    "for table_name, table_predicates in query_predicates.items():\n",
    "    table = tables[table_name]\n",
    "    print(f\"\\nTable --> {table_name}, Predicate Columns --> {set(table_predicates)}, table row count --> {table.row_count}\")\n",
    "    \n",
    "    # check if conditions for cheap full table scan are met\n",
    "    if table.row_count < SMALL_TABLE_IGNORE:\n",
    "        print(f\"Full table scan for table: {table_name} is cheap, skipping\")\n",
    "        continue  \n",
    "\n",
    "    # identify include columns\n",
    "    include_columns = []\n",
    "    if table_name in query_payload:\n",
    "        include_columns = sorted(list(set(query_payload[table_name]) - set(table_predicates)))\n",
    "\n",
    "    if len(include_columns)>0:    \n",
    "        print(f\"Include columns: {include_columns}\")\n",
    "\n",
    "        # generate all possible permutations of predicate columns, from single column up to 6-column indices\n",
    "        table_predicates = list(table_predicates.keys())[0:6]\n",
    "        col_permutations = list(itertools.permutations(table_predicates, len(table_predicates))) \n",
    "        \n",
    "        print(f\"Column permutations: \\n{col_permutations}\")\n",
    "\n",
    "        # assign an id and value to each index/column permutation\n",
    "        for cp in col_permutations:\n",
    "            index_id = get_index_id(cp, table_name, include_columns)\n",
    "            index_size = get_estimated_index_size(connection, table_name, list(cp) + include_columns)\n",
    "            print(f\"index_id: {index_id}, index columns: {cp}, include columns: {include_columns}, index size: {index_size:.2f} Mb\")\n",
    "            # assign value...\n",
    "            # create index object\n",
    "            index = Index(table_name, index_id, cp, index_size, tuple(include_columns))\n",
    "            indices.append(index)\n",
    "    \n",
    "\n",
    "close_connection(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates candidate indices using predicates and payload of a given query (should be a Query object)\n",
    "def generate_indices(connection, query, SMALL_TABLE_IGNORE=10000, TABLE_MIN_SELECTIVITY=0.2, verbose=False):\n",
    "    # get all tables in the db\n",
    "    tables = get_all_tables(connection)\n",
    "    if verbose:\n",
    "        print(f\"Tables:\")\n",
    "        for key in tables:\n",
    "            print(tables[key])\n",
    "\n",
    "    query_template_id = query.template_id\n",
    "    query_predicates = query.predicates\n",
    "    query_payload = query.payload\n",
    "    \n",
    "    indices = {}\n",
    "\n",
    "    # indexes on predicate columns only\n",
    "    for table_name, table_predicates in query_predicates.items():\n",
    "        table = tables[table_name]\n",
    "        if verbose: print(f\"\\nTable --> {table_name}, Predicate Columns --> {set(table_predicates)}, table row count --> {table.row_count}\")\n",
    "        \n",
    "        # identify include columns\n",
    "        include_columns = []\n",
    "        if table_name in query_payload:\n",
    "            include_columns = list(set(query_payload[table_name]) - set(table_predicates))\n",
    "        \n",
    "        if verbose: \n",
    "            print(f\"Include columns: {include_columns}\")\n",
    "            print(f\"Query selectivity: {query.selectivity[table_name]}\")\n",
    "\n",
    "\n",
    "        # check if conditions for cheap full table scan are met\n",
    "        if table.row_count < SMALL_TABLE_IGNORE or ((query.selectivity[table_name] > TABLE_MIN_SELECTIVITY) and (len(include_columns)>0)):\n",
    "            if verbose: print(f\"Full table scan for table: {table_name} is cheap, skipping\")\n",
    "            continue\n",
    "\n",
    "        # generate all possible permutations of predicate columns, from single column up to 6-column indices\n",
    "        table_predicates = list(table_predicates.keys())[0:6]\n",
    "        col_permutations = []\n",
    "        for num_columns in range(1, min(6, len(table_predicates)+1)):\n",
    "            col_permutations = list(itertools.permutations(table_predicates, num_columns)) \n",
    "        \n",
    "        if verbose: print(f\"Column permutations: \\n{col_permutations}\")\n",
    "\n",
    "        # assign an id and value to each index/column permutation\n",
    "        for cp in col_permutations:\n",
    "            index_id = get_index_id(cp, table_name)\n",
    "            \n",
    "            if index_id not in indices:\n",
    "                index_size = get_estimated_index_size(connection, table_name, cp)\n",
    "                if verbose:  print(f\"index_id: {index_id}, index columns: {cp}, index size: {index_size:.2f} Mb\")\n",
    "                # assign value...\n",
    "\n",
    "                # create index object\n",
    "                indices[index_id] = Index(table_name, index_id, cp, index_size)\n",
    "\n",
    "    # indexes on columns that are in the payload but not in the predicates\n",
    "    for table_name, table_payload in query_payload.items():\n",
    "        table = tables[table_name]\n",
    "        if verbose: print(f\"\\nTable --> {table_name}, Payload Columns --> {set(table_predicates)}, table row count --> {table.row_count}\")\n",
    "        \n",
    "        # skip if any of the payload columns for this table are in the predicates\n",
    "        if table_name in query_predicates:\n",
    "            if verbose: print(f\"Payload columns are in the predicates, skipping\")\n",
    "            continue\n",
    "\n",
    "        # check if conditions for cheap full table scan are met\n",
    "        if table.row_count < SMALL_TABLE_IGNORE:\n",
    "            if verbose: print(f\"Full table scan for table: {table_name} is cheap, skipping\")\n",
    "            continue   \n",
    "\n",
    "        # don't need to consider permutations here, just create an index with all payload columns in given order\n",
    "        index_id = get_index_id(table_payload, table_name)\n",
    "        if index_id not in indices:\n",
    "            index_size = get_estimated_index_size(connection, table_name, table_payload)\n",
    "            print(f\"index_id: {index_id}, index columns: {table_payload}, index size: {index_size:.2f} Mb\")\n",
    "            # assign value... (will assign less value to these indices as they are less useful compared to predicate indices)\n",
    "            \n",
    "            indices[index_id] = Index(table_name, index_id, table_payload, index_size)\n",
    "\n",
    "    # indexes with include columns\n",
    "    for table_name, table_predicates in query_predicates.items():\n",
    "        table = tables[table_name]\n",
    "        if verbose: print(f\"\\nTable --> {table_name}, Predicate Columns --> {set(table_predicates)}, table row count --> {table.row_count}\")\n",
    "        \n",
    "        # check if conditions for cheap full table scan are met\n",
    "        if table.row_count < SMALL_TABLE_IGNORE:\n",
    "            if verbose: print(f\"Full table scan for table: {table_name} is cheap, skipping\")\n",
    "            continue  \n",
    "\n",
    "        # identify include columns\n",
    "        include_columns = []\n",
    "        if table_name in query_payload:\n",
    "            include_columns = sorted(list(set(query_payload[table_name]) - set(table_predicates)))\n",
    "\n",
    "        if len(include_columns)>0:    \n",
    "            if verbose: print(f\"Include columns: {include_columns}\")\n",
    "\n",
    "            # generate all possible permutations of predicate columns, from single column up to 6-column indices\n",
    "            table_predicates = list(table_predicates.keys())[0:6]\n",
    "            col_permutations = list(itertools.permutations(table_predicates, len(table_predicates))) \n",
    "            \n",
    "            if verbose: print(f\"Column permutations: \\n{col_permutations}\")\n",
    "\n",
    "            # assign an id and value to each index/column permutation\n",
    "            for cp in col_permutations:\n",
    "                index_id = get_index_id(cp, table_name, include_columns)\n",
    "                if index_id not in indices:\n",
    "                    index_size = get_estimated_index_size(connection, table_name, list(cp) + include_columns)\n",
    "                    if verbose: print(f\"index_id: {index_id}, index columns: {cp}, include columns: {include_columns}, index size: {index_size:.2f} Mb\")\n",
    "                    # assign value...\n",
    "                    \n",
    "                    # create index object\n",
    "                    indices[index_id] = Index(table_name, index_id, cp, index_size, tuple(include_columns))\n",
    "            \n",
    "    return indices        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = start_connection() \n",
    "\n",
    "# generate candidate indices for all queries in first round (i.e. first 21 queries in TPC-H static workload)\n",
    "candidate_indices = {}\n",
    "for query in workload[0:21]:\n",
    "    # convert to Query object\n",
    "    query = Query(connection, query['template_id'], query['query_string'], query['payload'], query['predicates'], query['order_bys'])\n",
    "    indices = generate_indices(connection, query, verbose=False)\n",
    "    for index_id, index in indices.items():\n",
    "        if index_id not in candidate_indices:\n",
    "            candidate_indices[index_id] = index\n",
    "\n",
    "close_connection(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of candidate indices: 394\n",
      "Total size of all candidate indices: 63134.69 Mb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 9/394 [01:19<54:50,  8.55s/it]  "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"Total number of candidate indices: {len(candidate_indices)}\")\n",
    "print(f\"Total size of all candidate indices: {sum([index.size for index in candidate_indices.values()]):.2f} Mb\")\n",
    "\n",
    "connection = start_connection() \n",
    "\n",
    "# for each candidate index, calculate the cost of the query with and without the index, i.e. the estimated benefit of the index\n",
    "# assuming the starting configuration is no indices\n",
    "candidate_indices_values = defaultdict(float)\n",
    "for index_id in tqdm(candidate_indices.keys()):\n",
    "    for query in workload[0:21]:\n",
    "        indexes_added = [candidate_indices[index_id]]\n",
    "        indexes_removed = [] \n",
    "        total_orig_cost, total_hyp_cost, hyp_index_creation_cost = hyp_configuration_cost_estimate(connection, indexes_added, indexes_removed, [query['query_string']], verbose=False)\n",
    "        # compute the value, defined as the estimated time saved by using the index\n",
    "        value = total_orig_cost - total_hyp_cost\n",
    "        # add up the values for all the queries \n",
    "        candidate_indices_values[index_id] += value\n",
    "\n",
    "close_connection(connection)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate indices:\n",
      "Index(part, IX_part_p_size_p_type, ('p_size', 'p_type'), (), 7.43865966796875, None)\n",
      "Index(part, IX_part_p_type_p_size, ('p_type', 'p_size'), (), 7.43865966796875, None)\n",
      "Index(partsupp, IX_partsupp_ps_suppkey_ps_partkey, ('ps_suppkey', 'ps_partkey'), (), 11.444091796875, None)\n",
      "Index(partsupp, IX_partsupp_ps_partkey_ps_suppkey, ('ps_partkey', 'ps_suppkey'), (), 11.444091796875, None)\n",
      "Index(supplier, IX_supplier_s_nationkey, ('s_nationkey',), (), 0.1430511474609375, None)\n",
      "Index(part, IXN_part_p_size_p_type_p_mf_p_pa, ('p_size', 'p_type'), ('p_mfgr', 'p_partkey'), 12.20703125, None)\n",
      "Index(part, IXN_part_p_type_p_size_p_mf_p_pa, ('p_type', 'p_size'), ('p_mfgr', 'p_partkey'), 12.20703125, None)\n",
      "Index(partsupp, IXN_partsupp_ps_suppkey_ps_partkey_ps_s, ('ps_suppkey', 'ps_partkey'), ('ps_supplycost',), 18.310546875, None)\n",
      "Index(partsupp, IXN_partsupp_ps_partkey_ps_suppkey_ps_s, ('ps_partkey', 'ps_suppkey'), ('ps_supplycost',), 18.310546875, None)\n",
      "Index(supplier, IXN_supplier_s_nationkey_s_ac_s_ad_s_co_s_na_s_ph, ('s_nationkey',), ('s_acctbal', 's_address', 's_comment', 's_name', 's_phone'), 1.506805419921875, None)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Candidate indices:\")\n",
    "for index in indices:\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test index creations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Existing indexes: []\n",
      "\n",
      "Created index --> [dbo].[part].[IX_part_p_size_p_type], Indexed Columns --> ('p_size', 'p_type'), Included Columns --> (), index creation time: 0.658 seconds\n",
      "Created index --> [dbo].[part].[IX_part_p_type_p_size], Indexed Columns --> ('p_type', 'p_size'), Included Columns --> (), index creation time: 0.682 seconds\n",
      "Created index --> [dbo].[partsupp].[IX_partsupp_ps_suppkey_ps_partkey], Indexed Columns --> ('ps_suppkey', 'ps_partkey'), Included Columns --> (), index creation time: 0.846 seconds\n",
      "Created index --> [dbo].[partsupp].[IX_partsupp_ps_partkey_ps_suppkey], Indexed Columns --> ('ps_partkey', 'ps_suppkey'), Included Columns --> (), index creation time: 0.455 seconds\n",
      "\n",
      "Total index creation time: 2.64 seconds, Total Size: 37.7655029296875 Mb\n"
     ]
    }
   ],
   "source": [
    "connection = start_connection() \n",
    "\n",
    "# get list of all indexes currently in the database\n",
    "existing_indexes = get_nonclustered_indexes(connection)\n",
    "existing_indexes = [x[2] for x in existing_indexes]\n",
    "print(f\"\\nExisting indexes: {existing_indexes}\\n\")\n",
    "\n",
    "# create all candidadte indexes for the test query\n",
    "index_creation_time = 0\n",
    "index_size = 0\n",
    "for index in indices[:4]:\n",
    "    if index.index_id not in existing_indexes:\n",
    "        index_creation_time += create_nonclustered_index_object(connection, index, verbose=True)\n",
    "        index_size += index.size\n",
    "    else:\n",
    "        print(f\"Index {index.index_id} already exists, skipping\")\n",
    "\n",
    "print(f\"\\nTotal index creation time: {index_creation_time:.2f} seconds, Total Size: {index_size} Mb\")\n",
    "\n",
    "close_connection(connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Index dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Existing indexes: ['IX_part_p_size_p_type', 'IX_part_p_type_p_size', 'IX_partsupp_ps_partkey_ps_suppkey', 'IX_partsupp_ps_suppkey_ps_partkey']\n",
      "\n",
      "Dropped index --> [dbo].[part].[IX_part_p_size_p_type]\n",
      "Dropped index --> [dbo].[part].[IX_part_p_type_p_size]\n",
      "Dropped index --> [dbo].[partsupp].[IX_partsupp_ps_suppkey_ps_partkey]\n",
      "Dropped index --> [dbo].[partsupp].[IX_partsupp_ps_partkey_ps_suppkey]\n",
      "Index IX_supplier_s_nationkey does not exists, skipping\n",
      "Index IXN_part_p_size_p_type_p_mf_p_pa does not exists, skipping\n",
      "Index IXN_part_p_type_p_size_p_mf_p_pa does not exists, skipping\n",
      "Index IXN_partsupp_ps_suppkey_ps_partkey_ps_s does not exists, skipping\n",
      "Index IXN_partsupp_ps_partkey_ps_suppkey_ps_s does not exists, skipping\n",
      "Index IXN_supplier_s_nationkey_s_ac_s_ad_s_co_s_na_s_ph does not exists, skipping\n"
     ]
    }
   ],
   "source": [
    "connection = start_connection() \n",
    "\n",
    "# get list of all indexes currently in the database\n",
    "existing_indexes = get_nonclustered_indexes(connection)\n",
    "existing_indexes = [x[2] for x in existing_indexes]\n",
    "print(f\"\\nExisting indexes: {existing_indexes}\\n\")\n",
    "\n",
    "# create all candidadte indexes for the test query\n",
    "for index in indices:\n",
    "    if index.index_id in existing_indexes:\n",
    "        drop_noncluster_index_object(connection, index, verbose=True)\n",
    "    else:\n",
    "        print(f\"Index {index.index_id} does not exists, skipping\")\n",
    "\n",
    "close_connection(connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test hypothetical query execution cost with indices vs without"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created hypothetical index --> [part].[IX_part_p_size_p_type]\n",
      "Created hypothetical index --> [part].[IX_part_p_type_p_size]\n",
      "Created hypothetical index --> [partsupp].[IX_partsupp_ps_suppkey_ps_partkey]\n",
      "Created hypothetical index --> [partsupp].[IX_partsupp_ps_partkey_ps_suppkey]\n",
      "Enabling hypothetical index: (5, 997578592, 7)\n",
      "Enabling hypothetical index: (5, 997578592, 8)\n",
      "Enabling hypothetical index: (5, 1061578820, 4)\n",
      "Enabling hypothetical index: (5, 1061578820, 5)\n",
      "Dropped hypothetical index: dbo.part.IX_part_p_size_p_type\n",
      "Dropped hypothetical index: dbo.part.IX_part_p_type_p_size\n",
      "Dropped hypothetical index: dbo.partsupp.IX_partsupp_ps_suppkey_ps_partkey\n",
      "Dropped hypothetical index: dbo.partsupp.IX_partsupp_ps_partkey_ps_suppkey\n",
      "(16.6491, 12.6178, None)\n"
     ]
    }
   ],
   "source": [
    "connection = start_connection() \n",
    "\n",
    "indexes_added = indices[:4]\n",
    "indexes_removed = [] #indices[4:]\n",
    "print(hyp_configuration_cost_estimate(connection, indexes_added, indexes_removed,  [query.query_string], verbose=True))\n",
    "\n",
    "close_connection(connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Materialse the hypothetical configuration and measure actual query cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Existing indexes: ['IX_part_p_size_p_type', 'IX_part_p_type_p_size', 'IX_partsupp_ps_partkey_ps_suppkey', 'IX_partsupp_ps_suppkey_ps_partkey']\n",
      "\n",
      "Index IX_part_p_size_p_type already exists, skipping\n",
      "Index IX_part_p_type_p_size already exists, skipping\n",
      "Index IX_partsupp_ps_suppkey_ps_partkey already exists, skipping\n",
      "Index IX_partsupp_ps_partkey_ps_suppkey already exists, skipping\n",
      "\n",
      "Total index creation time: 0.00 seconds, Total Size: 0 Mb\n",
      "0.215\n",
      "16.3005\n"
     ]
    }
   ],
   "source": [
    "connection = start_connection() \n",
    "\n",
    "config_indexes = indices[:4]\n",
    "\n",
    "\n",
    "# get list of all indexes currently in the database\n",
    "existing_indexes = get_nonclustered_indexes(connection)\n",
    "existing_indexes = [x[2] for x in existing_indexes]\n",
    "print(f\"\\nExisting indexes: {existing_indexes}\\n\")\n",
    "\n",
    "# create all candidadte indexes for the test query\n",
    "index_creation_time = 0\n",
    "index_size = 0\n",
    "for index in config_indexes:\n",
    "    if index.index_id not in existing_indexes:\n",
    "        index_creation_time += create_nonclustered_index_object(connection, index, verbose=True)\n",
    "        index_size += index.size\n",
    "    else:\n",
    "        print(f\"Index {index.index_id} already exists, skipping\")\n",
    "\n",
    "print(f\"\\nTotal index creation time: {index_creation_time:.2f} seconds, Total Size: {index_size} Mb\")\n",
    "\n",
    "\n",
    "# execute the query and get the execution time\n",
    "print(execute_query(query.query_string, connection, cost_type='elapsed_time')[0])\n",
    "print(estimate_query_cost(connection, query.query_string)[0])\n",
    "\n",
    "\n",
    "close_connection(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "select\n",
      "\ts_acctbal,\n",
      "\ts_name,\n",
      "\tn_name,\n",
      "\tp_partkey,\n",
      "\tp_mfgr,\n",
      "\ts_address,\n",
      "\ts_phone,\n",
      "\ts_comment\n",
      "from\n",
      "\tpart,\n",
      "\tsupplier,\n",
      "\tpartsupp,\n",
      "\tnation,\n",
      "\tregion\n",
      "where\n",
      "\tp_partkey = ps_partkey\n",
      "\tand s_suppkey = ps_suppkey\n",
      "\tand p_size = 26\n",
      "\tand p_type like '%NICKEL'\n",
      "\tand s_nationkey = n_nationkey\n",
      "\tand n_regionkey = r_regionkey\n",
      "\tand r_name = 'EUROPE'\n",
      "\tand ps_supplycost = (\n",
      "\t\tselect\n",
      "\t\t\tmin(ps_supplycost)\n",
      "\t\tfrom\n",
      "\t\t\tpartsupp,\n",
      "\t\t\tsupplier,\n",
      "\t\t\tnation,\n",
      "\t\t\tregion\n",
      "\t\twhere\n",
      "\t\t\tp_partkey = ps_partkey\n",
      "\t\t\tand s_suppkey = ps_suppkey\n",
      "\t\t\tand s_nationkey = n_nationkey\n",
      "\t\t\tand n_regionkey = r_regionkey\n",
      "\t\t\tand r_name = 'EUROPE'\n",
      "\t)\n",
      "order by\n",
      "\ts_acctbal desc,\n",
      "\tn_name,\n",
      "\ts_name,\n",
      "\tp_partkey\n",
      "\n",
      ";\n"
     ]
    }
   ],
   "source": [
    "print(query.query_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
