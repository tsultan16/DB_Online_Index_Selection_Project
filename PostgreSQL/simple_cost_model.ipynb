{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Cost-Model for predicting query execution cost (assume disk IO dominates) and access paths selected \n",
    "\n",
    "\n",
    "Main steps in the cost model algorithm:\n",
    "\n",
    "1) Identify predicates (quatily and range) and payloads\n",
    "2) Enumerate all possible access paths (sequential scans, index scans, bitmap index scan + bitmap heap scan)\n",
    "3) Estimate selectivity for each predicate (i.e. what fraction of data needs to be accessed from a table)\n",
    "4) Estimate cardinality of each access path (using total number of rows and selectivity information)\n",
    "5) Estimate disk IO cost for each access path (for index scans, we use cardinality estimate to figure out how many pages need to be fetchs)\n",
    "6) Compare the estimated costs and choose best access paths for covering the query\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Some useful notes on access paths:\n",
    "* sequential scans --> better for low selectivity or if no suitable index available or if the table is really small\n",
    "* index scan --> better for high selectivity and simpler predicates\n",
    "* index only scan --> better for high selectivity and index is also a covering index and simpler predicates\n",
    "* bitmap index scan + bitmap heap scan --> better for medium selectivity and complex predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import numpy as np\n",
    "from pg_utils import *\n",
    "from ssb_qgen_class import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Table Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'customer': 300000, 'dwdate': 2556, 'lineorder': 59986216, 'part': 800000, 'supplier': 20000}\n"
     ]
    }
   ],
   "source": [
    "def get_page_size():\n",
    "    return 8192  # 8 KB\n",
    "\n",
    "def get_table_stats(table_name):\n",
    "\n",
    "    conn = create_connection()\n",
    "\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    try:\n",
    "        # Execute the query to get the estimated number of rows in the table\n",
    "        cur.execute(f\"\"\"\n",
    "                    SELECT reltuples::bigint AS estimated_rows\n",
    "                    FROM pg_class\n",
    "                    WHERE relname = '{table_name}';\n",
    "                    \"\"\")\n",
    "        estimated_rows = cur.fetchone()[0]\n",
    "    except:\n",
    "        print(f\"Error: Could not get the estimated number of rows in the '{table_name}' table.\")\n",
    "        estimated_rows = None\n",
    "\n",
    "    try:\n",
    "        # Query to get column statistics\n",
    "        cur.execute(f\"SELECT * FROM pg_stats WHERE tablename = '{table_name}';\")\n",
    "        column_stats = cur.fetchall()\n",
    "\n",
    "        # Define the column names based on the pg_stats view\n",
    "        column_names = [\n",
    "                        \"schemaname\", \"tablename\", \"attname\", \"inherited\", \"null_frac\",\n",
    "                        \"avg_width\", \"n_distinct\", \"most_common_vals\", \"most_common_freqs\",\n",
    "                        \"histogram_bounds\", \"correlation\", \"most_common_elems\",\n",
    "                        \"most_common_elem_freqs\", \"elem_count_histogram\"\n",
    "                    ]\n",
    "\n",
    "        # Organize the results into a dictionary\n",
    "        stats_dict = {}\n",
    "        for row in column_stats:\n",
    "            column_name = row[2]  # 'attname' is the third column in the result\n",
    "            stats_dict[column_name] = {column_names[i]: row[i] for i in range(len(column_names))}\n",
    "    except:\n",
    "        print(f\"Error: Could not get the statistics for the '{table_name}' table\")\n",
    "        stats_dict = None\n",
    "\n",
    "    # Close the cursor and connection\n",
    "    cur.close()\n",
    "    close_connection(conn)\n",
    "\n",
    "    return stats_dict, estimated_rows\n",
    "\n",
    "\n",
    "# Get the statistics for all tables in the SSB database\n",
    "table_names = [\"customer\", \"dwdate\", \"lineorder\", \"part\", \"supplier\"]\n",
    "stats = {}\n",
    "estimated_rows = {}\n",
    "for table_name in table_names:\n",
    "    stats[table_name], estimated_rows[table_name] = get_table_stats(table_name)\n",
    "\n",
    "\n",
    "# Print the organized statistics dictionary\n",
    "#for key, value in stats_dict.items():\n",
    "#    print(f\"{key}\")\n",
    "#    for k, v in value.items():\n",
    "#        print(f\"    {k}: {v}\")\n",
    "\n",
    "#print(f\"\\nEstimated number of rows in the 'customer' table: {estimated_rows}\")\n",
    "print(estimated_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c_custkey': 'numeric',\n",
       " 'c_name': 'char',\n",
       " 'c_address': 'char',\n",
       " 'c_city': 'char',\n",
       " 'c_nation': 'char',\n",
       " 'c_region': 'char',\n",
       " 'c_phone': 'char',\n",
       " 'c_mktsegment': 'char',\n",
       " 'd_datekey': 'char',\n",
       " 'd_date': 'char',\n",
       " 'd_dayofweek': 'char',\n",
       " 'd_month': 'char',\n",
       " 'd_year': 'numeric',\n",
       " 'd_yearmonthnum': 'numeric',\n",
       " 'd_yearmonth': 'char',\n",
       " 'd_daynuminweek': 'numeric',\n",
       " 'd_daynuminmonth': 'numeric',\n",
       " 'd_daynuminyear': 'numeric',\n",
       " 'd_monthnuminyear': 'numeric',\n",
       " 'd_weeknuminyear': 'numeric',\n",
       " 'd_sellingseason': 'char',\n",
       " 'd_lastdayinweekfl': 'numeric',\n",
       " 'd_lastdayinmonthfl': 'numeric',\n",
       " 'd_holidayfl': 'numeric',\n",
       " 'd_weekdayfl': 'numeric',\n",
       " 'lo_orderkey': 'numeric',\n",
       " 'lo_linenumber': 'numeric',\n",
       " 'lo_custkey': 'numeric',\n",
       " 'lo_partkey': 'numeric',\n",
       " 'lo_suppkey': 'numeric',\n",
       " 'lo_orderdate': 'char',\n",
       " 'lo_orderpriority': 'char',\n",
       " 'lo_shippriority': 'char',\n",
       " 'lo_quantity': 'numeric',\n",
       " 'lo_extendedprice': 'numeric',\n",
       " 'lo_ordtotalprice': 'numeric',\n",
       " 'lo_discount': 'numeric',\n",
       " 'lo_revenue': 'numeric',\n",
       " 'lo_supplycost': 'numeric',\n",
       " 'lo_tax': 'numeric',\n",
       " 'lo_commitdate': 'char',\n",
       " 'lo_shipmode': 'char',\n",
       " 'p_partkey': 'numeric',\n",
       " 'p_name': 'char',\n",
       " 'p_mfgr': 'char',\n",
       " 'p_category': 'char',\n",
       " 'p_brand': 'char',\n",
       " 'p_color': 'char',\n",
       " 'p_type': 'char',\n",
       " 'p_size': 'numeric',\n",
       " 'p_container': 'char',\n",
       " 's_suppkey': 'numeric',\n",
       " 's_name': 'char',\n",
       " 's_address': 'char',\n",
       " 's_city': 'char',\n",
       " 's_nation': 'char',\n",
       " 's_region': 'char',\n",
       " 's_phone': 'char'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssb_tables, pk_columns = get_ssb_schema()\n",
    "\n",
    "# create a dictionary and specify whether each attribute in each table is numeric or char\n",
    "data_type_dict = {}\n",
    "for table_name in [\"customer\", \"dwdate\", \"lineorder\", \"part\", \"supplier\"]:\n",
    "    for column_name, column_type in ssb_tables[table_name]:\n",
    "        if (\"INT\" in column_type) or (\"DECIMAL\" in column_type) or (\"BIT\" in column_type):\n",
    "            data_type_dict[column_name] = \"numeric\"\n",
    "        else:\n",
    "            data_type_dict[column_name] = \"char\"\n",
    "    \n",
    "data_type_dict    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating Selectivity for a value range on a particular column, i.e. what fraction of the data (i.e. tuples) fall in the given range, using the Table Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_selectivity_one_sided_range(attribute, boundary_value, operator, stats_dict, total_rows):\n",
    "    data_type = data_type_dict[attribute]\n",
    "    # Get the column statistics\n",
    "    stats = stats_dict[attribute]\n",
    "    histogram_bounds = stats['histogram_bounds']\n",
    "    n_distinct = stats['n_distinct']\n",
    "    most_common_vals = stats['most_common_vals']\n",
    "    most_common_freqs = stats['most_common_freqs']\n",
    "\n",
    "    # Convert most_common_values string to list of correct data type\n",
    "    if most_common_vals:\n",
    "        if data_type == 'numeric':\n",
    "            most_common_vals = [float(x) for x in most_common_vals.strip('{}').split(',')]\n",
    "        elif data_type == 'char':\n",
    "            most_common_vals = [x for x in most_common_vals.strip('{}').split(',')]\n",
    "        else:\n",
    "            raise ValueError(\"Data type not supported, needs to be either numeric or char\")\n",
    "\n",
    "    # Convert negative n_distinct to an absolute count\n",
    "    if n_distinct < 0:\n",
    "        n_distinct = -n_distinct * total_rows\n",
    "\n",
    "    selectivity = 0.0\n",
    "\n",
    "    # Check for overlap with most common values\n",
    "    if most_common_vals:\n",
    "        for val, freq in zip(most_common_vals, most_common_freqs):\n",
    "            if (operator == '>' and val > boundary_value) or (operator == '<' and val < boundary_value):\n",
    "                selectivity += freq\n",
    "\n",
    "    if histogram_bounds is not None:\n",
    "        if data_type == 'numeric':\n",
    "            histogram_bounds = [float(x) for x in histogram_bounds.strip('{}').split(',')]\n",
    "        elif data_type == 'char':\n",
    "            histogram_bounds = [x for x in histogram_bounds.strip('{}').split(',')]\n",
    "        else:\n",
    "            raise ValueError(\"Data type not supported, needs to be either numeric or char\")\n",
    "\n",
    "        total_bins = len(histogram_bounds) - 1\n",
    "\n",
    "        # Iterate over bins, find overlapping bins\n",
    "        for i in range(total_bins):\n",
    "            bin_lower_bound = histogram_bounds[i]\n",
    "            bin_upper_bound = histogram_bounds[i + 1]\n",
    "\n",
    "            # Check for range overlap\n",
    "            if (operator == '>' and boundary_value < bin_upper_bound) or (operator == '<' and boundary_value > bin_lower_bound):\n",
    "                # Calculate the overlap fraction within this bin\n",
    "                if operator == '>':\n",
    "                    overlap_min = max(boundary_value, bin_lower_bound)\n",
    "                    overlap_fraction = (bin_upper_bound - overlap_min) / (bin_upper_bound - bin_lower_bound)\n",
    "                else:  # operator == '<'\n",
    "                    overlap_max = min(boundary_value, bin_upper_bound)\n",
    "                    overlap_fraction = (overlap_max - bin_lower_bound) / (bin_upper_bound - bin_lower_bound)\n",
    "\n",
    "                # Accumulate to the total selectivity\n",
    "                selectivity += overlap_fraction * (1.0 / total_bins)\n",
    "\n",
    "    if selectivity == 0.0:\n",
    "        # If no overlap with most common values or histogram bins, assume uniform distribution and estimate selectivity\n",
    "        selectivity = 1.0 / n_distinct\n",
    "\n",
    "    return selectivity\n",
    "\n",
    "\n",
    "def estimate_selectivity_range(attribute, value_range, stats_dict, total_rows):\n",
    "    data_type = data_type_dict[attribute]\n",
    "    # get the column statistics\n",
    "    stats = stats_dict[attribute]\n",
    "    # get the histogram bounds\n",
    "    histogram_bounds = stats['histogram_bounds']\n",
    "    n_distinct = stats['n_distinct']\n",
    "    most_common_vals = stats['most_common_vals']\n",
    "    most_common_freqs = stats['most_common_freqs']\n",
    "\n",
    "    # convert most_common_values string to list of correct data type\n",
    "    if most_common_vals:\n",
    "        if data_type == 'numeric':\n",
    "            most_common_vals = [float(x) for x in most_common_vals.strip('{}').split(',')]\n",
    "        elif data_type == 'char':\n",
    "            most_common_vals = [x for x in most_common_vals.strip('{}').split(',')]    \n",
    "        else:\n",
    "            raise ValueError(\"Data type not supported, needs ot be either numeric or char\")\n",
    "\n",
    "    # Convert negative n_distinct to an absolute count\n",
    "    if n_distinct < 0:\n",
    "        n_distinct = -n_distinct * total_rows\n",
    "\n",
    "    min_value = value_range[0]\n",
    "    max_value = value_range[1]\n",
    "    selectivity = 0.0\n",
    "\n",
    "    # check for overlap with most common values\n",
    "    if most_common_vals:\n",
    "        for val, freq in zip(most_common_vals, most_common_freqs):\n",
    "            if min_value <= val <= max_value:\n",
    "                selectivity += freq    \n",
    "\n",
    "    if histogram_bounds is not None:\n",
    "        if data_type == 'numeric':\n",
    "            histogram_bounds = [float(x) for x in histogram_bounds.strip('{}').split(',')] # convert to list of integers\n",
    "        elif data_type == 'char':\n",
    "            histogram_bounds = [x for x in histogram_bounds.strip('{}').split(',')]\n",
    "        else:\n",
    "            raise ValueError(\"Data type not supported, needs ot be either numeric or char\")    \n",
    "\n",
    "        total_bins = len(histogram_bounds) - 1\n",
    "\n",
    "        # iterate over bins, find overlapping bins\n",
    "        for i in range(total_bins):\n",
    "            bin_lower_bound = histogram_bounds[i]\n",
    "            bin_upper_bound = histogram_bounds[i+1]\n",
    "\n",
    "            # check for range overlap\n",
    "            if min_value < bin_lower_bound or max_value > bin_upper_bound:\n",
    "                # does not overlap\n",
    "                continue    \n",
    "\n",
    "            # calculate the overlap fraction within this bin\n",
    "            overlap_min = max(min_value, bin_lower_bound)\n",
    "            overlap_max = min(max_value, bin_upper_bound)\n",
    "            overlap_fraction = (overlap_max - overlap_min) / (bin_upper_bound - bin_lower_bound)\n",
    "\n",
    "            #print(f\"Overlap fraction for bin {i}: {overlap_fraction}\")\n",
    "            #print(f\"Bin bounds: {bin_lower_bound}, {bin_upper_bound}\")\n",
    "\n",
    "            # accumulate to the total selectivity\n",
    "            # Assume each bin represents an equal fraction of the total rows\n",
    "            selectivity += overlap_fraction * (1.0 / total_bins)\n",
    "\n",
    "    if selectivity == 0.0:\n",
    "        # if no overlap with most common values or histogram bins, assume uniform distribution and estimate selectivity\n",
    "        selectivity = 1.0 / n_distinct       \n",
    "\n",
    "    return selectivity\n",
    "\n",
    "\n",
    "def estimate_selectivity_eq(attribute, value, stats_dict):\n",
    "    data_type = data_type_dict[attribute]\n",
    "    # get the column statistics\n",
    "    stats = stats_dict[attribute]\n",
    "    # get the histogram bounds\n",
    "    histogram_bounds = stats['histogram_bounds']\n",
    "    n_distinct = stats['n_distinct']\n",
    "    most_common_vals = stats['most_common_vals']\n",
    "    most_common_freqs = stats['most_common_freqs']\n",
    "\n",
    "    # convert most_common_values string to list of correct data type\n",
    "    if most_common_vals:\n",
    "        if data_type == 'numeric':\n",
    "            most_common_vals = [float(x) for x in most_common_vals.strip('{}').split(',')]\n",
    "        elif data_type == 'char':\n",
    "            most_common_vals = [x for x in most_common_vals.strip('{}').split(',')]    \n",
    "        else:\n",
    "            raise ValueError(\"Data type not supported, needs ot be either numeric or char\")\n",
    "\n",
    "    # first check if the value is in the most common values\n",
    "    if most_common_vals and value in most_common_vals:\n",
    "        selectivity = most_common_freqs[most_common_vals.index(value)] \n",
    "        return selectivity\n",
    "\n",
    "    # if not a common value, estimate using n_distinct\n",
    "    if n_distinct < 0:\n",
    "        n_distinct = -n_distinct\n",
    "\n",
    "    selectivity = 1.0 / n_distinct    \n",
    "\n",
    "    if histogram_bounds is not None:\n",
    "        if data_type == 'numeric':\n",
    "            histogram_bounds = [float(x) for x in histogram_bounds.strip('{}').split(',')] # convert to list of integers\n",
    "        elif data_type == 'char':\n",
    "            histogram_bounds = [x for x in histogram_bounds.strip('{}').split(',')]\n",
    "        else:\n",
    "            raise ValueError(\"Data type not supported, needs ot be either numeric or char\")    \n",
    "\n",
    "        total_bins = len(histogram_bounds) - 1\n",
    "\n",
    "        # iterate over bins, find bin that contains the value\n",
    "        for i in range(total_bins):\n",
    "            bin_lower_bound = histogram_bounds[i]\n",
    "            bin_upper_bound = histogram_bounds[i+1]\n",
    "\n",
    "            # check for range overlap\n",
    "            if bin_lower_bound <= value <= bin_upper_bound:\n",
    "                bin_width = bin_upper_bound - bin_lower_bound\n",
    "                if bin_width > 0:\n",
    "                    # assume uniform distribution within this bin and calculate selectivity\n",
    "                    uniform_selectivity = 1.0 / (bin_width*total_bins)\n",
    "                    selectivity = min(selectivity, uniform_selectivity)\n",
    "                break    \n",
    "\n",
    "    return selectivity\n",
    "\n",
    "\n",
    "def estimate_selectivity_or(attribute, value, stats_dict):\n",
    "    combined_selectivity = 0.0\n",
    "    individual_selectivities = []\n",
    "\n",
    "    # for each value in the IN list, estimate the selectivity separately\n",
    "    for val in value:\n",
    "        individual_selectivities.append(estimate_selectivity_eq(attribute, val, stats_dict))\n",
    "\n",
    "    # compute combined selectivities using inclusion-exclusion principle and assuming independence\n",
    "    for selectivity in individual_selectivities:\n",
    "        combined_selectivity += selectivity \n",
    "\n",
    "    overlap_adjustment = 1.0\n",
    "    for selectivity in individual_selectivities:\n",
    "        overlap_adjustment *= (1.0 - selectivity)\n",
    "\n",
    "    combined_selectivity -= overlap_adjustment   \n",
    "\n",
    "    # make sure the combined selectivity is between 0 and 1\n",
    "    combined_selectivity = max(0.0, min(combined_selectivity, 1.0))\n",
    "\n",
    "    return combined_selectivity \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated selectivity for range (1992, 1994): 0.42879498\n",
      "Estimated selectivity for value 1992: 0.14319248\n",
      "Estimated selectivity for range ('Monday', 'Wednesday'): 0.8571987299999999\n",
      "Estimated selectivity for value Monday: 0.14280125\n"
     ]
    }
   ],
   "source": [
    "# test the selectivity estimation functions on a numeric column\n",
    "attribute = 'd_year'\n",
    "stats_dict = stats['dwdate']\n",
    "\n",
    "# test range selectivity estimation\n",
    "value_range = (1992, 1994)\n",
    "selectivity = estimate_selectivity_range(attribute, value_range, stats_dict, estimated_rows)\n",
    "print(f\"Estimated selectivity for range {value_range}: {selectivity}\")\n",
    "\n",
    "# test equality selectivity estimation\n",
    "value = 1992\n",
    "selectivity = estimate_selectivity_eq(attribute, value, stats_dict)\n",
    "print(f\"Estimated selectivity for value {value}: {selectivity}\")\n",
    "\n",
    "# now, let's try a char column\n",
    "attribute = 'd_dayofweek'\n",
    "\n",
    "# test range selectivity estimation\n",
    "value_range = ('Monday', 'Wednesday')\n",
    "selectivity = estimate_selectivity_range(attribute, value_range, stats_dict, estimated_rows)\n",
    "print(f\"Estimated selectivity for range {value_range}: {selectivity}\")\n",
    "\n",
    "# test equality selectivity estimation\n",
    "value = 'Monday'\n",
    "selectivity = estimate_selectivity_eq(attribute, value, stats_dict)\n",
    "print(f\"Estimated selectivity for value {value}: {selectivity}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Case: consider three possible access paths: Sequential Scan, Index Scan and Index Only Scan\n",
    "\n",
    "TODO: Add Bitmap Index Scan + Bitmap Heap Scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Extract query predicates and payload, the payload is a list of column names (across possibly multiple tables) and the predicate is a list of dictionaries, each dict contains the column name, operator (either equality or range) and either a range tuple or single value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "qgen = QGEN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "template id: 1, query: \n",
      "                SELECT SUM(lo_extendedprice * lo_discount) AS revenue\n",
      "                FROM lineorder, dwdate\n",
      "                WHERE lo_orderdate = d_datekey\n",
      "                AND d_year = 1998\n",
      "                AND lo_discount BETWEEN 3 AND 5 \n",
      "                AND lo_quantity < 25;\n",
      "            , payload: {'lineorder': ['lo_extendedprice', 'lo_discount']}, predicates: {'lineorder': ['lo_orderdate', 'lo_discount', 'lo_quantity'], 'dwdate': ['d_datekey', 'd_year']}, order by: {}, group by: {}\n",
      "{'lineorder': [{'column': 'lo_orderdate', 'operator': 'eq', 'value': 'd_datekey', 'join': True}, {'column': 'lo_discount', 'operator': 'range', 'value': (3, 5), 'join': False}, {'column': 'lo_quantity', 'operator': '<', 'value': 25, 'join': False}], 'dwdate': [{'column': 'd_year', 'operator': 'eq', 'value': 1998, 'join': False}]}\n"
     ]
    }
   ],
   "source": [
    "example_query = qgen.generate_query(1)\n",
    "print(example_query)\n",
    "print(example_query.predicate_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables and columns: {'lineorder': ['lo_extendedprice', 'lo_discount', 'lo_quantity', 'lo_orderdate'], 'dwdate': ['d_year', 'd_datekey']}\n",
      "Payload: {'lineorder': ['lo_extendedprice', 'lo_discount']}\n",
      "Predicates:\n",
      "\n",
      "lineorder\n",
      "\t{'column': 'lo_orderdate', 'operator': 'eq', 'value': 'd_datekey', 'join': True}\n",
      "\t{'column': 'lo_discount', 'operator': 'range', 'value': (3, 5), 'join': False}\n",
      "\t{'column': 'lo_quantity', 'operator': '<', 'value': 25, 'join': False}\n",
      "\n",
      "dwdate\n",
      "\t{'column': 'd_year', 'operator': 'eq', 'value': 1998, 'join': False}\n"
     ]
    }
   ],
   "source": [
    "# extract tables and associated columns\n",
    "tables = {}\n",
    "#tables['lineorder'] = ['lo_linenumber', 'lo_quantity', 'lo_orderdate']\n",
    "for table_name in example_query.payload:\n",
    "    tables[table_name] = example_query.payload[table_name]\n",
    "\n",
    "for table_name in example_query.predicates:\n",
    "    if table_name not in tables:\n",
    "        tables[table_name] = []\n",
    "    tables[table_name] = list(set(tables[table_name] + example_query.predicates[table_name]))\n",
    "\n",
    "# extract the payload\n",
    "payload = example_query.payload\n",
    "\n",
    "# extract the predicates\n",
    "predicates = example_query.predicate_dict\n",
    "\n",
    "print(f\"Tables and columns: {tables}\")   \n",
    "print(f\"Payload: {payload}\")\n",
    "print(f\"Predicates:\")\n",
    "for table_name, predicate_list in predicates.items():\n",
    "    print(f\"\\n{table_name}\")\n",
    "    for predicate in predicate_list:\n",
    "        print(f\"\\t{predicate}\")\n",
    "\n",
    "# create some index objects\n",
    "index_1 = Index('lineorder', 'IX_lineorder_lo_orderdate_lo_suppkey', index_columns=['lo_orderdate', 'lo_suppkey'])\n",
    "index_2 = Index('lineorder', 'IX_lineorder_lo_quantity_lo_o', index_columns=['lo_quantity'], include_columns=['lo_orderdate'])\n",
    "index_3 = Index('part', 'IX_part_p_mfgr', index_columns=['p_mfgr'])\n",
    "index_4 = Index('customer', 'IX_customer_c_region', index_columns=['c_region'])\n",
    "index_5 = Index('supplier', 'IX_supplier_s_region', index_columns=['s_region'])\n",
    "index_6 = Index('dwdate', 'IX_dwdate_d_datekey', index_columns=['d_datekey'])\n",
    "indexes = {index.index_id: index for index in [index_1, index_2, index_3, index_4, index_5, index_6]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Enumerate the possible access path for each table involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicates: {'lineorder': [{'column': 'lo_orderdate', 'operator': 'eq', 'value': 'd_datekey', 'join': True}, {'column': 'lo_discount', 'operator': 'range', 'value': (3, 5), 'join': False}, {'column': 'lo_quantity', 'operator': '<', 'value': 25, 'join': False}], 'dwdate': [{'column': 'd_year', 'operator': 'eq', 'value': 1998, 'join': False}]}\n",
      "Join predicates: {'dwdate': [{'column': 'd_datekey', 'operator': 'eq', 'value': 'lo_orderdate', 'join': True}]}\n",
      "\n",
      "Table predicate columns for lineorder: ['lo_orderdate', 'lo_discount', 'lo_quantity']\n",
      "Relevant predicate columns for lineorder: {'lo_orderdate', 'lo_discount', 'lo_quantity'}\n",
      "Payload columns for lineorder: ['lo_extendedprice', 'lo_discount']\n",
      "Checking index:  IX_lineorder_lo_orderdate_lo_suppkey\n",
      "Index scan possible!\n",
      "Checking index:  IX_lineorder_lo_quantity_lo_o\n",
      "Index scan possible!\n",
      "\n",
      "Table predicate columns for dwdate: ['d_year', 'd_datekey']\n",
      "Relevant predicate columns for dwdate: {'d_datekey', 'd_year'}\n",
      "Payload columns for dwdate: ['lo_extendedprice', 'lo_discount']\n",
      "Checking index:  IX_dwdate_d_datekey\n",
      "Index scan possible!\n",
      "\n",
      "Access paths: \n",
      "Table: lineorder\n",
      "    {'scan_type': 'Sequential Scan'}\n",
      "    {'scan_type': 'Index Scan', 'index_id': 'IX_lineorder_lo_orderdate_lo_suppkey'}\n",
      "    {'scan_type': 'Index Scan', 'index_id': 'IX_lineorder_lo_quantity_lo_o'}\n",
      "Table: dwdate\n",
      "    {'scan_type': 'Sequential Scan'}\n",
      "    {'scan_type': 'Index Scan', 'index_id': 'IX_dwdate_d_datekey'}\n"
     ]
    }
   ],
   "source": [
    "# extract join predicate columns\n",
    "join_predicates = {}\n",
    "join_predicates_temp = {}\n",
    "for table_name in predicates:\n",
    "    table_preicates = predicates[table_name]\n",
    "    for pred in table_preicates:\n",
    "        if pred['join'] == True:\n",
    "            if table_name not in join_predicates_temp:\n",
    "                join_predicates_temp[table_name] = []\n",
    "            join_predicates_temp[table_name].append(pred['column'])\n",
    "            # add the other table's column to the join predicate list\n",
    "            other_table_column = pred['value']\n",
    "            # search for the table name containing the other column\n",
    "            for other_table_name in tables:\n",
    "                if other_table_column in tables[other_table_name]:\n",
    "                    if other_table_name not in join_predicates_temp:\n",
    "                        join_predicates_temp[other_table_name] = []\n",
    "                    join_predicates_temp[other_table_name].append(other_table_column)\n",
    "                    join_pred = pred.copy()\n",
    "                    join_pred['column'] = other_table_column\n",
    "                    join_pred['value'] = pred['column']\n",
    "                    if other_table_name not in join_predicates:\n",
    "                        join_predicates[other_table_name] = []\n",
    "                    join_predicates[other_table_name].append(join_pred)\n",
    "                    break\n",
    "\n",
    "print(f\"Predicates: {predicates}\")\n",
    "print(f\"Join predicates: {join_predicates}\")\n",
    "\n",
    "# add join predicates to the main predicate dictionary\n",
    "for table_name in join_predicates:\n",
    "    if table_name not in predicates:\n",
    "        predicates[table_name] = []\n",
    "    # Ensure unique dictionaries in the list\n",
    "    existing_predicates = {frozenset(pred.items()): pred for pred in predicates[table_name]}\n",
    "    for pred in join_predicates[table_name]:\n",
    "        pred_key = frozenset(pred.items())\n",
    "        if pred_key not in existing_predicates:\n",
    "            existing_predicates[pred_key] = pred\n",
    "    predicates[table_name] = list(existing_predicates.values())\n",
    "\n",
    "\n",
    "access_paths = {}\n",
    "for table_name in tables:\n",
    "    if table_name in predicates:\n",
    "        #table_predicate_cols = [pred['column'] for pred in predicates[table_name] if pred['join'] == False]\n",
    "        table_predicate_cols = [pred['column'] for pred in predicates[table_name]]\n",
    "    if table_name in payload:\n",
    "        table_payload_cols = [col for col in payload[table_name] if col in tables[table_name]]   \n",
    "    if table_name in join_predicates_temp:\n",
    "        join_predicate_cols = join_predicates_temp[table_name]\n",
    "    \n",
    "    relevant_predicate_cols = set(table_predicate_cols).union(join_predicate_cols)\n",
    "    table_access_paths = [{'scan_type': 'Sequential Scan'}]\n",
    "    print(f\"\\nTable predicate columns for {table_name}: {table_predicate_cols}\")\n",
    "    print(f\"Relevant predicate columns for {table_name}: {relevant_predicate_cols}\")\n",
    "    print(f\"Payload columns for {table_name}: {table_payload_cols}\")\n",
    "    for index in indexes.values():\n",
    "        if index.table_name == table_name:\n",
    "            print(\"Checking index: \", index.index_id)\n",
    "            # Check if index scan is possible\n",
    "            if set(index.index_columns).intersection(relevant_predicate_cols):\n",
    "                table_access_paths.append({'scan_type': 'Index Scan', 'index_id': index.index_id})\n",
    "                print(\"Index scan possible!\")\n",
    "            # Check if index only scan is possible\n",
    "            if set(index.index_columns).issuperset(relevant_predicate_cols) and set(\n",
    "                list(index.index_columns) + list(index.include_columns)).issuperset(table_payload_cols):\n",
    "                table_access_paths.append({'scan_type': 'Index Only Scan', 'index_id': index.index_id})\n",
    "                print(\"Index only scan possible!\")\n",
    "\n",
    "    access_paths[table_name] = table_access_paths\n",
    "\n",
    "\n",
    "print(f\"\\nAccess paths: \")\n",
    "for table, paths in access_paths.items():\n",
    "    print(f\"Table: {table}\")\n",
    "    for path in paths:\n",
    "        print(f\"    {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate selectivity of the predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_selectivity(attribute, operator, value, stats_dict, total_rows):\n",
    "    if operator == 'eq':\n",
    "        return estimate_selectivity_eq(attribute, value, stats_dict)\n",
    "    elif operator == 'range':\n",
    "        return estimate_selectivity_range(attribute, value, stats_dict, total_rows)\n",
    "    elif operator == '<' or operator == '>':\n",
    "        return estimate_selectivity_one_sided_range(attribute, value, operator, stats_dict, total_rows)\n",
    "    elif operator == 'or':\n",
    "        return estimate_selectivity_or(attribute, value, stats_dict)    \n",
    "    else:\n",
    "        raise ValueError(f\"Operator '{operator}' not supported, needs to be either 'eq', 'range', or 'or'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated selectivity for predicate {'column': 'lo_discount', 'operator': 'range', 'value': (3, 5), 'join': False}: 0.273600006\n",
      "Estimated selectivity for predicate {'column': 'lo_quantity', 'operator': '<', 'value': 25, 'join': False}: 0.4762666669999999\n",
      "Estimated selectivity for predicate {'column': 'd_year', 'operator': 'eq', 'value': 1998, 'join': False}: 0.14241001\n"
     ]
    }
   ],
   "source": [
    "for table_name, table_preds in predicates.items():\n",
    "    table_stats_dict = stats[table_name]   \n",
    "    table_estimated_rows = estimated_rows[table_name]\n",
    "    for pred in table_preds:\n",
    "        if pred['join'] == False:\n",
    "            selectivity = estimate_selectivity(pred['column'], pred['operator'], pred['value'], table_stats_dict, table_estimated_rows)\n",
    "            print(f\"Estimated selectivity for predicate {pred}: {selectivity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lineorder': [{'column': 'lo_orderdate',\n",
       "   'operator': 'eq',\n",
       "   'value': 'd_datekey',\n",
       "   'join': True},\n",
       "  {'column': 'lo_discount',\n",
       "   'operator': 'range',\n",
       "   'value': (3, 5),\n",
       "   'join': False},\n",
       "  {'column': 'lo_quantity', 'operator': '<', 'value': 25, 'join': False}],\n",
       " 'dwdate': [{'column': 'd_year',\n",
       "   'operator': 'eq',\n",
       "   'value': 1998,\n",
       "   'join': False},\n",
       "  {'column': 'd_datekey',\n",
       "   'operator': 'eq',\n",
       "   'value': 'lo_orderdate',\n",
       "   'join': True}]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each table, estimate selectivity and disk IO cost of all access path for that table and select cheapest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_row_overhead(num_nullable_columns=0):\n",
    "    # Tuple header size\n",
    "    tuple_header_size = 23  # bytes\n",
    "    # Null bitmap size (1 byte for every 8 nullable columns)\n",
    "    null_bitmap_size = (num_nullable_columns + 7) // 8\n",
    "    # Total overhead\n",
    "    total_overhead = tuple_header_size + null_bitmap_size\n",
    "\n",
    "    return total_overhead\n",
    "\n",
    "\n",
    "def table_avg_rows_per_page(table_stats_dict):\n",
    "    # add up the average width of all columns to get the average width of a row\n",
    "    avg_row_size = 0\n",
    "    avg_row_size = sum(column_stats['avg_width'] for column_stats in table_stats_dict.values())\n",
    "    # add the row overhead\n",
    "    avg_row_size += calculate_row_overhead()\n",
    "    # calculate the average number of rows that can fit in a page\n",
    "    avg_rows_per_page = int(get_page_size() / avg_row_size)\n",
    "\n",
    "    return avg_rows_per_page\n",
    "\n",
    "\n",
    "def index_average_rows_per_page(index, table_stats_dict):\n",
    "    columns = list(index.index_columns) + list(index.include_columns)   \n",
    "    # add up the average width of all columns to get the average width of a row\n",
    "    avg_row_size = sum(table_stats_dict[column]['avg_width'] for column in columns)\n",
    "    # add the row overhead\n",
    "    index_row_overhead = 16  # assume 16 bytes \n",
    "    avg_row_size += index_row_overhead\n",
    "    # calculate the average number of rows that can fit in a page\n",
    "    # (assuming the index is a B+ tree, so only the leaf nodes contain the actual data)\n",
    "    avg_rows_per_page = int(get_page_size() / avg_row_size)\n",
    "       \n",
    "    return avg_rows_per_page\n",
    "\n",
    "\n",
    "def estimate_index_scan_cost(index, table_stats_dict, table_predicates, total_rows, cost_multiplier=4.0, index_only_scan=False, verbose=False):\n",
    "    # check if leading index column is in the predicates\n",
    "    leading_index_column = index.index_columns[0]\n",
    "    #print(f\"\\t\\t\\tTable predicates: {table_predicates}, Leading index column: {leading_index_column}\")\n",
    "    predicate_columns = [pred['column'] for pred in table_predicates]\n",
    "    \n",
    "    if leading_index_column not in predicate_columns:\n",
    "        # assign high cost to prevent using this index, sequential scan will be cheaper\n",
    "        return float('inf')\n",
    "    \n",
    "    # calculate the combined selectivity for this index (assuming attribute independence/no correlations of predicates)\n",
    "    leading_column_selectivity = 1.0\n",
    "    combined_selectivity = 1.0\n",
    "    for pred in table_predicates:\n",
    "        if pred['column'] in index.index_columns and pred['join'] == False:\n",
    "            selectivity = estimate_selectivity(pred['column'], pred['operator'], pred['value'], table_stats_dict, total_rows)\n",
    "            if verbose: print(f\"\\t\\tSelectivity for predicate {pred}: {selectivity}\")\n",
    "            combined_selectivity *= selectivity\n",
    "            if pred['column'] == leading_index_column:\n",
    "                leading_column_selectivity = selectivity\n",
    "\n",
    "    # estimate cardinality of the index scan\n",
    "    index_cardinality = leading_column_selectivity * total_rows\n",
    "    # estimate the number of pages that need to be accessed\n",
    "    avg_rows_per_page = index_average_rows_per_page(index, table_stats_dict)\n",
    "    index_pages = int(index_cardinality / avg_rows_per_page)\n",
    "    \n",
    "    table_pages = 0\n",
    "    if not index_only_scan: \n",
    "        table_cardinality = combined_selectivity * total_rows\n",
    "        # for index scan, we need to access the table as well\n",
    "        index_average_rows_per_page_table = table_avg_rows_per_page(table_stats_dict)\n",
    "        table_pages = int(table_cardinality / index_average_rows_per_page_table)\n",
    "    \n",
    "    # return total cost as the sum of index and table pages\n",
    "    index_scan_cost = (index_pages + table_pages) * cost_multiplier\n",
    "    if verbose: \n",
    "        print(f\"\\tLeading column selectivity: {leading_column_selectivity}, Combined selectivity: {combined_selectivity}\")\n",
    "        print(f\"\\tEstimated number of pages for index scan: {index_pages}, Table pages: {table_pages}\")\n",
    "        print(f\"\\tIndex scan cost: {index_scan_cost}\")\n",
    "\n",
    "    return index_scan_cost\n",
    "\n",
    "\n",
    "def estimate_sequentail_scan_cost(table_stats_dict, total_rows, cost_multiplier=1.0, verbose=False):\n",
    "    # estimate cardinality of the scan\n",
    "    scan_cardinality = total_rows\n",
    "    # estimate the number of pages that need to be accessed\n",
    "    avg_rows_per_page = table_avg_rows_per_page(table_stats_dict)\n",
    "    scan_pages = int(scan_cardinality / avg_rows_per_page)\n",
    "    # estimate the total cost as the number of pages that need to be accessed\n",
    "    sequential_scan_cost = scan_pages * cost_multiplier\n",
    "\n",
    "    if verbose: \n",
    "        print(f\"\\tEstimated number of pages for sequential scan: {scan_pages}\")\n",
    "        print(f\"\\tSequential scan cost: {sequential_scan_cost}\")\n",
    "\n",
    "    return sequential_scan_cost\n",
    "\n",
    "\n",
    "def find_cheapest_paths(access_paths, predicates, join_predicates, stats, estimated_rows, verbose=False):\n",
    "    cheapest_table_access_path = {}    \n",
    "    if verbose: print(f\"Finding cheapest access paths for tables: {access_paths.keys()}\")\n",
    "    # enumerate over tables that need to be accessed\n",
    "    for table_name in access_paths:\n",
    "        if verbose: print(f\"\\nTable: {table_name}\")\n",
    "        # enumerate over access paths for this table\n",
    "        cheapest_cost = float('inf')\n",
    "        for path in access_paths[table_name]:\n",
    "            print(f\"\\tComputing cost for access path: {path}\")\n",
    "            # compute the cost of this access path\n",
    "            # (for now, assume cost is proportional to the cardinality of the data that needs to be accessed)\n",
    "            if path['scan_type'] == 'Sequential Scan':\n",
    "                cost = estimate_sequentail_scan_cost(stats[table_name], estimated_rows[table_name], verbose=verbose)\n",
    "            elif path['scan_type'] == 'Index Scan':\n",
    "                index_id = path['index_id']\n",
    "                index = indexes[index_id]\n",
    "                cost = estimate_index_scan_cost(index, stats[table_name], predicates[table_name], estimated_rows[table_name], verbose=verbose)    \n",
    "            elif path['scan_type'] == 'Index Only Scan':\n",
    "                index_id = path['index_id']\n",
    "                index = indexes[index_id]\n",
    "                cost = estimate_index_scan_cost(index, stats[table_name], predicates[table_name], estimated_rows[table_name], index_only_scan=True, verbose=verbose)\n",
    "            else:\n",
    "                raise ValueError(\"Scan type not supported\")            \n",
    "\n",
    "            if verbose: print(f\"\\tAccess path: {path}, Cost: {cost}\\n\")\n",
    "            if cost < cheapest_cost:\n",
    "                cheapest_cost = cost\n",
    "                cheapest_access_path = path\n",
    "        cheapest_table_access_path[table_name] = cheapest_access_path\n",
    "        if verbose: print(f\"\\tCheapest access path: {cheapest_access_path}, Cost: {cheapest_cost}\")\n",
    "    return cheapest_table_access_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding cheapest access paths for tables: dict_keys(['lineorder', 'dwdate'])\n",
      "\n",
      "Table: lineorder\n",
      "\tComputing cost for access path: {'scan_type': 'Sequential Scan'}\n",
      "\tEstimated number of pages for sequential scan: 759319\n",
      "\tSequential scan cost: 759319.0\n",
      "\tAccess path: {'scan_type': 'Sequential Scan'}, Cost: 759319.0\n",
      "\n",
      "\tComputing cost for access path: {'scan_type': 'Index Scan', 'index_id': 'IX_lineorder_lo_orderdate_lo_suppkey'}\n",
      "\tLeading column selectivity: 1.0, Combined selectivity: 1.0\n",
      "\tEstimated number of pages for index scan: 175912, Table pages: 759319\n",
      "\tIndex scan cost: 3740924.0\n",
      "\tAccess path: {'scan_type': 'Index Scan', 'index_id': 'IX_lineorder_lo_orderdate_lo_suppkey'}, Cost: 3740924.0\n",
      "\n",
      "\tComputing cost for access path: {'scan_type': 'Index Scan', 'index_id': 'IX_lineorder_lo_quantity_lo_o'}\n",
      "\t\tSelectivity for predicate {'column': 'lo_quantity', 'operator': '<', 'value': 25, 'join': False}: 0.4762666669999999\n",
      "\tLeading column selectivity: 0.4762666669999999, Combined selectivity: 0.4762666669999999\n",
      "\tEstimated number of pages for index scan: 83781, Table pages: 361638\n",
      "\tIndex scan cost: 1781676.0\n",
      "\tAccess path: {'scan_type': 'Index Scan', 'index_id': 'IX_lineorder_lo_quantity_lo_o'}, Cost: 1781676.0\n",
      "\n",
      "\tCheapest access path: {'scan_type': 'Sequential Scan'}, Cost: 759319.0\n",
      "\n",
      "Table: dwdate\n",
      "\tComputing cost for access path: {'scan_type': 'Sequential Scan'}\n",
      "\tEstimated number of pages for sequential scan: 38\n",
      "\tSequential scan cost: 38.0\n",
      "\tAccess path: {'scan_type': 'Sequential Scan'}, Cost: 38.0\n",
      "\n",
      "\tComputing cost for access path: {'scan_type': 'Index Scan', 'index_id': 'IX_dwdate_d_datekey'}\n",
      "\tLeading column selectivity: 1.0, Combined selectivity: 1.0\n",
      "\tEstimated number of pages for index scan: 6, Table pages: 38\n",
      "\tIndex scan cost: 176.0\n",
      "\tAccess path: {'scan_type': 'Index Scan', 'index_id': 'IX_dwdate_d_datekey'}, Cost: 176.0\n",
      "\n",
      "\tCheapest access path: {'scan_type': 'Sequential Scan'}, Cost: 38.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lineorder': {'scan_type': 'Sequential Scan'},\n",
       " 'dwdate': {'scan_type': 'Sequential Scan'}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_cheapest_paths(access_paths, predicates, join_predicates, stats, estimated_rows, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Simple Cost Model with Postgres EXPLAIN (+ HypoPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"Plan\": {\n",
      "      \"Node Type\": \"Aggregate\",\n",
      "      \"Strategy\": \"Plain\",\n",
      "      \"Partial Mode\": \"Simple\",\n",
      "      \"Parallel Aware\": false,\n",
      "      \"Async Capable\": false,\n",
      "      \"Startup Cost\": 1972556.68,\n",
      "      \"Total Cost\": 1972556.69,\n",
      "      \"Plan Rows\": 1,\n",
      "      \"Plan Width\": 32,\n",
      "      \"Plans\": [\n",
      "        {\n",
      "          \"Node Type\": \"Hash Join\",\n",
      "          \"Parent Relationship\": \"Outer\",\n",
      "          \"Parallel Aware\": false,\n",
      "          \"Async Capable\": false,\n",
      "          \"Join Type\": \"Inner\",\n",
      "          \"Startup Cost\": 79.5,\n",
      "          \"Total Cost\": 1966990.87,\n",
      "          \"Plan Rows\": 1113162,\n",
      "          \"Plan Width\": 10,\n",
      "          \"Inner Unique\": true,\n",
      "          \"Hash Cond\": \"(lineorder.lo_orderdate = dwdate.d_datekey)\",\n",
      "          \"Plans\": [\n",
      "            {\n",
      "              \"Node Type\": \"Seq Scan\",\n",
      "              \"Parent Relationship\": \"Outer\",\n",
      "              \"Parallel Aware\": false,\n",
      "              \"Async Capable\": false,\n",
      "              \"Relation Name\": \"lineorder\",\n",
      "              \"Alias\": \"lineorder\",\n",
      "              \"Startup Cost\": 0.0,\n",
      "              \"Total Cost\": 1946358.78,\n",
      "              \"Plan Rows\": 7816598,\n",
      "              \"Plan Width\": 14,\n",
      "              \"Filter\": \"((lo_discount >= '3'::numeric) AND (lo_discount <= '5'::numeric) AND (lo_quantity < 25))\"\n",
      "            },\n",
      "            {\n",
      "              \"Node Type\": \"Hash\",\n",
      "              \"Parent Relationship\": \"Inner\",\n",
      "              \"Parallel Aware\": false,\n",
      "              \"Async Capable\": false,\n",
      "              \"Startup Cost\": 74.95,\n",
      "              \"Total Cost\": 74.95,\n",
      "              \"Plan Rows\": 364,\n",
      "              \"Plan Width\": 4,\n",
      "              \"Plans\": [\n",
      "                {\n",
      "                  \"Node Type\": \"Seq Scan\",\n",
      "                  \"Parent Relationship\": \"Outer\",\n",
      "                  \"Parallel Aware\": false,\n",
      "                  \"Async Capable\": false,\n",
      "                  \"Relation Name\": \"dwdate\",\n",
      "                  \"Alias\": \"dwdate\",\n",
      "                  \"Startup Cost\": 0.0,\n",
      "                  \"Total Cost\": 74.95,\n",
      "                  \"Plan Rows\": 364,\n",
      "                  \"Plan Width\": 4,\n",
      "                  \"Filter\": \"(d_year = 1998)\"\n",
      "                }\n",
      "              ]\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "]\n",
      "Estimated cost without hypothetical indexes: 1972556.69\n",
      "Scan costs: {'Seq Scan': 1946433.73}\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"Plan\": {\n",
      "      \"Node Type\": \"Aggregate\",\n",
      "      \"Strategy\": \"Plain\",\n",
      "      \"Partial Mode\": \"Simple\",\n",
      "      \"Parallel Aware\": false,\n",
      "      \"Async Capable\": false,\n",
      "      \"Startup Cost\": 1972556.68,\n",
      "      \"Total Cost\": 1972556.69,\n",
      "      \"Plan Rows\": 1,\n",
      "      \"Plan Width\": 32,\n",
      "      \"Plans\": [\n",
      "        {\n",
      "          \"Node Type\": \"Hash Join\",\n",
      "          \"Parent Relationship\": \"Outer\",\n",
      "          \"Parallel Aware\": false,\n",
      "          \"Async Capable\": false,\n",
      "          \"Join Type\": \"Inner\",\n",
      "          \"Startup Cost\": 79.5,\n",
      "          \"Total Cost\": 1966990.87,\n",
      "          \"Plan Rows\": 1113162,\n",
      "          \"Plan Width\": 10,\n",
      "          \"Inner Unique\": true,\n",
      "          \"Hash Cond\": \"(lineorder.lo_orderdate = dwdate.d_datekey)\",\n",
      "          \"Plans\": [\n",
      "            {\n",
      "              \"Node Type\": \"Seq Scan\",\n",
      "              \"Parent Relationship\": \"Outer\",\n",
      "              \"Parallel Aware\": false,\n",
      "              \"Async Capable\": false,\n",
      "              \"Relation Name\": \"lineorder\",\n",
      "              \"Alias\": \"lineorder\",\n",
      "              \"Startup Cost\": 0.0,\n",
      "              \"Total Cost\": 1946358.78,\n",
      "              \"Plan Rows\": 7816598,\n",
      "              \"Plan Width\": 14,\n",
      "              \"Filter\": \"((lo_discount >= '3'::numeric) AND (lo_discount <= '5'::numeric) AND (lo_quantity < 25))\"\n",
      "            },\n",
      "            {\n",
      "              \"Node Type\": \"Hash\",\n",
      "              \"Parent Relationship\": \"Inner\",\n",
      "              \"Parallel Aware\": false,\n",
      "              \"Async Capable\": false,\n",
      "              \"Startup Cost\": 74.95,\n",
      "              \"Total Cost\": 74.95,\n",
      "              \"Plan Rows\": 364,\n",
      "              \"Plan Width\": 4,\n",
      "              \"Plans\": [\n",
      "                {\n",
      "                  \"Node Type\": \"Seq Scan\",\n",
      "                  \"Parent Relationship\": \"Outer\",\n",
      "                  \"Parallel Aware\": false,\n",
      "                  \"Async Capable\": false,\n",
      "                  \"Relation Name\": \"dwdate\",\n",
      "                  \"Alias\": \"dwdate\",\n",
      "                  \"Startup Cost\": 0.0,\n",
      "                  \"Total Cost\": 74.95,\n",
      "                  \"Plan Rows\": 364,\n",
      "                  \"Plan Width\": 4,\n",
      "                  \"Filter\": \"(d_year = 1998)\"\n",
      "                }\n",
      "              ]\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "]\n",
      "No index scans were explicitly noted in the query plan.\n",
      "\n",
      "Estimated cost with hypothetical indexes: 1972556.69\n",
      "Speedup: 1.0000\n",
      "\n",
      "Indexes used in the query plan:\n"
     ]
    }
   ],
   "source": [
    "candidate_indexes = list(indexes.values())\n",
    "\n",
    "conn = create_connection()\n",
    "\n",
    "# drop all existing secondary indexes \n",
    "drop_all_indexes(conn)\n",
    "\n",
    "cost_wo_indexes, scan_costs = get_query_cost_estimate(conn, example_query.query_string, show_plan=True)\n",
    "print(f\"Estimated cost without hypothetical indexes: {cost_wo_indexes}\")\n",
    "print(f\"Scan costs: {scan_costs}\")\n",
    "\n",
    "# create hypothetical indexes for candidate indexes\n",
    "\n",
    "hypothetical_indexes = {}\n",
    "indexes = bulk_create_hypothetical_indexes(conn, candidate_indexes, return_size=True)\n",
    "for i in range(len(indexes)):\n",
    "    hypothetical_indexes[indexes[i][0]] = (candidate_indexes[i], indexes[i][1]) \n",
    "    index_oid, index_size_mb = indexes[i]\n",
    "    #print(f\"Index {candidate_indexes[i].index_id} created with oid {index_oid} and size {index_size_mb} MB\")\n",
    "\n",
    "# get the cost of the query with the hypothetical indexes\n",
    "print()\n",
    "cost_w_indexes, indexes_used = get_query_cost_estimate_hypo_indexes(conn, example_query.query_string, show_plan=True)\n",
    "print(f\"\\nEstimated cost with hypothetical indexes: {cost_w_indexes}\")\n",
    "print(f\"Speedup: {cost_wo_indexes/cost_w_indexes:.4f}\")\n",
    "\n",
    "print(\"\\nIndexes used in the query plan:\")\n",
    "for oid, scan_type, scan_cost in indexes_used:\n",
    "    print(hypothetical_indexes[oid][0], \", Scan type: \", scan_type, \", Scan cost: \", scan_cost)\n",
    "\n",
    "bulk_drop_hypothetical_indexes(conn)\n",
    "\n",
    "close_connection(conn)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clone_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
